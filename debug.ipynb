{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# define customized functions with customized gradients\n",
    "class STEFunction_relay(torch.autograd.Function):\n",
    "    \"\"\" define straight through estimator with overrided gradient (gate) \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_x, mask, threshold = 0.005):\n",
    "        ctx.save_for_backward(input_x)\n",
    "        mask_higher = (input_x > threshold).float()\n",
    "        mask_lower = (input_x < (-1) * threshold).float()\n",
    "        mask > 0\n",
    "        return (input_x > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_x, = ctx.saved_tensors\n",
    "        return torch.mul(F.softplus(input_x), grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "now_state = 1\n",
    "in_val = -0.015\n",
    "threshold = 0.01\n",
    "def Hysteresis(now_state, in_val, threshold):\n",
    "    if now_state == 1:\n",
    "        if in_val < (-1) * threshold:\n",
    "            now_state = 0\n",
    "    else:\n",
    "        if in_val > threshold:\n",
    "            now_state = 1\n",
    "    return now_state\n",
    "now_state = Hysteresis(now_state, in_val, threshold) \n",
    "print(now_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "now_state = 1.0\n",
    "in_val_list = [-0.011, 0.005, 0.001, 0.011, -0.009, -0.011, 0.0, 0.011]\n",
    "threshold = 0.01\n",
    "def Hysteresis(now_state, in_val, threshold):\n",
    "    now_state = now_state * float(in_val > (-1) * threshold) + (1 - now_state) * float(in_val > threshold)\n",
    "    return now_state\n",
    "for in_val in in_val_list:\n",
    "    now_state = Hysteresis(now_state, in_val, threshold) \n",
    "    print(now_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "now_state = 0\n",
    "in_val = -0.0101\n",
    "threshold = 0.01\n",
    "now_state = now_state * float(in_val > (-1) * threshold) + (1 - now_state) * float(in_val > threshold)\n",
    "print(now_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3044, -1.3046,  0.2962,  1.8289, -1.2884,  0.0434,  0.2522, -1.1956,\n",
      "        -0.3554, -0.8371], requires_grad=True)\n",
      "tensor([ 0.2044, -1.2046,  0.1962,  1.7289, -1.1884, -0.0566,  0.1522, -1.0956,\n",
      "        -0.2554, -0.7371], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "class MultiInputRelayFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input1, input2):\n",
    "        ctx.save_for_backward(input1, input2)\n",
    "        return input1, input2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output1, grad_output2):\n",
    "        input1, input2 = ctx.saved_tensors\n",
    "        if not input1.requires_grad:\n",
    "            grad_input1 = None\n",
    "        else:\n",
    "            grad_input1 = grad_output1.clone()\n",
    "\n",
    "        if not input2.requires_grad:\n",
    "            grad_input2 = None\n",
    "        else:\n",
    "            grad_input2 = grad_output2.clone()\n",
    "\n",
    "        return grad_input1, grad_input2\n",
    "\n",
    "x1 = torch.randn(10, requires_grad=True)\n",
    "x2 = torch.randn(10)\n",
    "print(x1)\n",
    "# Use the MultiInputRelayFunction to calculate the output\n",
    "output = MultiInputRelayFunction.apply(x1, x2)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = (output[0] ** 2).sum()\n",
    "\n",
    "# Backprop to calculate gradients\n",
    "loss.backward()\n",
    "\n",
    "# Update x1 using an optimizer\n",
    "optimizer = torch.optim.Adam([x1], lr=0.1)\n",
    "optimizer.step()\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7832, 0.8368, 0.3353, 0.2943, 0.4404, 0.9394, 0.7239, 0.0102, 0.3909,\n",
      "        0.6217], requires_grad=True)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([-0.2168, -0.1632, -0.6647, -0.7057, -0.5596, -0.0606, -0.2761, -0.9898,\n",
      "        -0.6091, -0.3783], requires_grad=True)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0., 0., 1., 1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# define customized functions with customized gradients\n",
    "class STEFunction_relay(torch.autograd.Function):\n",
    "    \"\"\" define straight through estimator with overrided gradient (gate) \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mask_old):\n",
    "        ctx.save_for_backward(input)\n",
    "        return mask_old\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return torch.mul(F.softplus(input), grad_output), None\n",
    "\n",
    "x1 = torch.randn(10, requires_grad=True)\n",
    "nn.init.uniform_(x1, a = 0, b = 1)\n",
    "mask_old = (x1 > 0).float()\n",
    "print(x1)\n",
    "print(mask_old)\n",
    "mask_old.requires_grad = False\n",
    "x1_out = STEFunction_relay.apply(x1, mask_old)\n",
    "loss = torch.sum(x1_out)\n",
    "threshold = 0.5\n",
    "\n",
    "loss.backward()\n",
    "optimizer = torch.optim.Adam([x1], lr=1)\n",
    "optimizer.step()\n",
    "print(x1)\n",
    "print(mask_old)\n",
    "mask_new = mask_old * (x1 > (-1) * threshold).float() + (1 - mask_old) * (x1 > threshold).float()\n",
    "print(mask_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0808,  0.2326, -0.0325],\n",
      "        [ 0.4869, -0.0527, -0.3543],\n",
      "        [ 0.2577, -0.1948, -0.0866]], requires_grad=True)\n",
      "tensor([[0.8084, 0.4958, 0.9062],\n",
      "        [0.9594, 0.9866, 0.9635],\n",
      "        [0.8957, 0.1373, 0.4079]])\n",
      "Parameter containing:\n",
      "tensor([[0.8084, 0.4958, 0.9062],\n",
      "        [0.9594, 0.9866, 0.9635],\n",
      "        [0.8957, 0.1373, 0.4079]], requires_grad=True)\n",
      "tensor([[0.8566, 0.1845, 0.7607],\n",
      "        [0.7572, 0.2837, 0.1152],\n",
      "        [0.5088, 0.6485, 0.9269]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with random values\n",
    "tensor = torch.rand(3, 3)\n",
    "\n",
    "# Define a model with a single linear layer\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(3, 3)\n",
    "\n",
    "# Initialize the model\n",
    "model = Model()\n",
    "\n",
    "# Copy the values from the tensor into the model's parameters\n",
    "print(model.linear.weight)\n",
    "print(tensor)\n",
    "model.linear.weight.data.copy_(tensor)\n",
    "print(model.linear.weight)\n",
    "\n",
    "### Other method to copy parameter from one to another\n",
    "param1 = torch.nn.Parameter(torch.rand(3, 3))\n",
    "param2 = torch.nn.Parameter(torch.rand(3, 3))\n",
    "param2.data.copy_(param1.data)\n",
    "print(param2.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value\n",
      "Value\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self._attr1 = Attr1()\n",
    "    def set_attr1(self, value):\n",
    "        self._attr1.attr2 = value\n",
    "\n",
    "class Attr1:\n",
    "    def __init__(self):\n",
    "        self.attr2 = None\n",
    "\n",
    "obj = MyClass()\n",
    "obj.set_attr1(\"Value\")\n",
    "print(obj._attr1.attr2)\n",
    "# setattr(eval('obj._attr1'), 'attr2', 'a')\n",
    "print(getattr(obj, '_attr1').attr2)\n",
    "getattr(obj, '_attr1').attr2 = 'a'\n",
    "print(obj._attr1.attr2)\n",
    "obj._attr1.attr2 = 'a'\n",
    "print(obj._attr1.attr2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09ed53c31c4084b0592fb3928828ca5258593b9e37489b03328497df4e897e81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
