01/19 03:39:17 AM | 
01/19 03:39:17 AM | Parameters:
01/19 03:39:17 AM | NUM_MASK=1
01/19 03:39:17 AM | ACT_TYPE=ReLU_masked_spgrad_relay
01/19 03:39:17 AM | ALPHA_LR=0.0002
01/19 03:39:17 AM | ALPHA_WEIGHT_DECAY=0.001
01/19 03:39:17 AM | ARCH=resnet18
01/19 03:39:17 AM | BATCH_SIZE=128
01/19 03:39:17 AM | CHECKPOINT_PATH=None
01/19 03:39:17 AM | DATA_PATH=./data/
01/19 03:39:17 AM | DATASET=cifar10
01/19 03:39:17 AM | DISTIL=False
01/19 03:39:17 AM | DROPOUT=0
01/19 03:39:17 AM | EPOCHS=100
01/19 03:39:17 AM | EVALUATE=None
01/19 03:39:17 AM | EXT=baseline
01/19 03:39:17 AM | GPUS=[0]
01/19 03:39:17 AM | LAMDA=10.0
01/19 03:39:17 AM | MASK_DROPOUT=0.01
01/19 03:39:17 AM | MASK_EPOCHS=0
01/19 03:39:17 AM | NUM_CLASSES=10
01/19 03:39:17 AM | OPTIM=cosine
01/19 03:39:17 AM | PATH=train_cifar_relay/resnet18__cifar10_relay_0.003/cosine_mask_dropout_0.01sp0.5lr0.01ep100_baseline
01/19 03:39:17 AM | PLOT_PATH=train_cifar_relay/resnet18__cifar10_relay_0.003/cosine_mask_dropout_0.01sp0.5lr0.01ep100_baseline/plots
01/19 03:39:17 AM | PRECISION=full
01/19 03:39:17 AM | PRETRAINED_PATH=./train_cifar_relay/resnet18__cifar10_relay_0.003/cosine_mask_dropout_0.01sp0.5wm_lr0.02mep100_baseline/checkpoint_mask_train.pth.tar
01/19 03:39:17 AM | PRINT_FREQ=100
01/19 03:39:17 AM | SCALE_X=0.5
01/19 03:39:17 AM | SCALE_X2=0.1
01/19 03:39:17 AM | SEED=2
01/19 03:39:17 AM | SPARSITY=0.5
01/19 03:39:17 AM | START_EPOCH=0
01/19 03:39:17 AM | TEACHER_ARCH=
01/19 03:39:17 AM | TEACHER_PATH=None
01/19 03:39:17 AM | THRESHOLD=0.003
01/19 03:39:17 AM | W_DECAY_EPOCH=20
01/19 03:39:17 AM | W_GRAD_CLIP=5.0
01/19 03:39:17 AM | W_LR=0.01
01/19 03:39:17 AM | W_LR_MIN=1e-05
01/19 03:39:17 AM | W_MASK_LR=0.02
01/19 03:39:17 AM | W_MOMENTUM=0.9
01/19 03:39:17 AM | W_WEIGHT_DECAY=0.001
01/19 03:39:17 AM | WORKERS=4
01/19 03:39:17 AM | X_SIZE=[1, 3, 32, 32]
01/19 03:39:17 AM | 
01/19 03:39:17 AM | Logger is set - training start
01/19 03:39:17 AM | 
01/19 03:39:17 AM | Parameters:
01/19 03:39:17 AM | NUM_MASK=1
01/19 03:39:17 AM | ACT_TYPE=ReLU_masked_spgrad_relay
01/19 03:39:17 AM | ALPHA_LR=0.0002
01/19 03:39:17 AM | ALPHA_WEIGHT_DECAY=0.001
01/19 03:39:17 AM | ARCH=resnet18
01/19 03:39:17 AM | BATCH_SIZE=128
01/19 03:39:17 AM | CHECKPOINT_PATH=None
01/19 03:39:17 AM | DATA_PATH=./data/
01/19 03:39:17 AM | DATASET=cifar10
01/19 03:39:17 AM | DISTIL=False
01/19 03:39:17 AM | DROPOUT=0
01/19 03:39:17 AM | EPOCHS=100
01/19 03:39:17 AM | EVALUATE=None
01/19 03:39:17 AM | EXT=baseline
01/19 03:39:17 AM | GPUS=[1]
01/19 03:39:17 AM | LAMDA=10.0
01/19 03:39:17 AM | MASK_DROPOUT=0.01
01/19 03:39:17 AM | MASK_EPOCHS=0
01/19 03:39:17 AM | NUM_CLASSES=10
01/19 03:39:17 AM | OPTIM=cosine
01/19 03:39:17 AM | PATH=train_cifar_relay/resnet18__cifar10_relay_0.002/cosine_mask_dropout_0.01sp0.9lr0.01ep100_baseline
01/19 03:39:17 AM | PLOT_PATH=train_cifar_relay/resnet18__cifar10_relay_0.002/cosine_mask_dropout_0.01sp0.9lr0.01ep100_baseline/plots
01/19 03:39:17 AM | PRECISION=full
01/19 03:39:17 AM | PRETRAINED_PATH=./train_cifar_relay/resnet18__cifar10_relay_0.002/cosine_mask_dropout_0.01sp0.9wm_lr0.02mep100_baseline/checkpoint_mask_train.pth.tar
01/19 03:39:17 AM | PRINT_FREQ=100
01/19 03:39:17 AM | SCALE_X=0.5
01/19 03:39:17 AM | SCALE_X2=0.1
01/19 03:39:17 AM | SEED=2
01/19 03:39:17 AM | SPARSITY=0.9
01/19 03:39:17 AM | START_EPOCH=0
01/19 03:39:17 AM | TEACHER_ARCH=
01/19 03:39:17 AM | TEACHER_PATH=None
01/19 03:39:17 AM | THRESHOLD=0.002
01/19 03:39:17 AM | W_DECAY_EPOCH=20
01/19 03:39:17 AM | W_GRAD_CLIP=5.0
01/19 03:39:17 AM | W_LR=0.01
01/19 03:39:17 AM | W_LR_MIN=1e-05
01/19 03:39:17 AM | W_MASK_LR=0.02
01/19 03:39:17 AM | W_MOMENTUM=0.9
01/19 03:39:17 AM | W_WEIGHT_DECAY=0.001
01/19 03:39:17 AM | WORKERS=4
01/19 03:39:17 AM | X_SIZE=[1, 3, 32, 32]
01/19 03:39:17 AM | 
01/19 03:39:17 AM | Logger is set - training start
01/19 03:39:17 AM | 
01/19 03:39:17 AM | 
01/19 03:39:17 AM | Parameters:
01/19 03:39:17 AM | Parameters:
01/19 03:39:17 AM | NUM_MASK=1
01/19 03:39:17 AM | NUM_MASK=1
01/19 03:39:17 AM | ACT_TYPE=ReLU_masked_spgrad_relay
01/19 03:39:17 AM | ACT_TYPE=ReLU_masked_spgrad_relay
01/19 03:39:17 AM | ALPHA_LR=0.0002
01/19 03:39:17 AM | ALPHA_LR=0.0002
01/19 03:39:17 AM | ALPHA_WEIGHT_DECAY=0.001
01/19 03:39:17 AM | ARCH=resnet18
01/19 03:39:17 AM | ALPHA_WEIGHT_DECAY=0.001
01/19 03:39:17 AM | BATCH_SIZE=128
01/19 03:39:17 AM | ARCH=resnet18
01/19 03:39:17 AM | CHECKPOINT_PATH=None
01/19 03:39:17 AM | BATCH_SIZE=128
01/19 03:39:17 AM | DATA_PATH=./data/
01/19 03:39:17 AM | CHECKPOINT_PATH=None
01/19 03:39:17 AM | DATASET=cifar10
01/19 03:39:17 AM | DATA_PATH=./data/
01/19 03:39:17 AM | DISTIL=False
01/19 03:39:17 AM | DATASET=cifar10
01/19 03:39:17 AM | DROPOUT=0
01/19 03:39:17 AM | DISTIL=False
01/19 03:39:17 AM | EPOCHS=100
01/19 03:39:17 AM | DROPOUT=0
01/19 03:39:17 AM | EVALUATE=None
01/19 03:39:17 AM | EPOCHS=100
01/19 03:39:17 AM | EXT=baseline
01/19 03:39:17 AM | EVALUATE=None
01/19 03:39:17 AM | EXT=baseline
01/19 03:39:17 AM | GPUS=[1]
01/19 03:39:17 AM | GPUS=[0]
01/19 03:39:17 AM | LAMDA=10.0
01/19 03:39:17 AM | LAMDA=10.0
01/19 03:39:17 AM | MASK_DROPOUT=0.01
01/19 03:39:17 AM | MASK_DROPOUT=0.01
01/19 03:39:17 AM | MASK_EPOCHS=0
01/19 03:39:17 AM | MASK_EPOCHS=0
01/19 03:39:17 AM | NUM_CLASSES=10
01/19 03:39:17 AM | NUM_CLASSES=10
01/19 03:39:17 AM | OPTIM=cosine
01/19 03:39:17 AM | OPTIM=cosine
01/19 03:39:17 AM | PATH=train_cifar_relay/resnet18__cifar10_relay_0.003/cosine_mask_dropout_0.01sp0.9lr0.01ep100_baseline
01/19 03:39:17 AM | PATH=train_cifar_relay/resnet18__cifar10_relay_0.002/cosine_mask_dropout_0.01sp0.5lr0.01ep100_baseline
01/19 03:39:17 AM | PLOT_PATH=train_cifar_relay/resnet18__cifar10_relay_0.003/cosine_mask_dropout_0.01sp0.9lr0.01ep100_baseline/plots
01/19 03:39:17 AM | PLOT_PATH=train_cifar_relay/resnet18__cifar10_relay_0.002/cosine_mask_dropout_0.01sp0.5lr0.01ep100_baseline/plots
01/19 03:39:17 AM | PRECISION=full
01/19 03:39:17 AM | PRECISION=full
01/19 03:39:17 AM | PRETRAINED_PATH=./train_cifar_relay/resnet18__cifar10_relay_0.003/cosine_mask_dropout_0.01sp0.9wm_lr0.02mep100_baseline/checkpoint_mask_train.pth.tar
01/19 03:39:17 AM | PRETRAINED_PATH=./train_cifar_relay/resnet18__cifar10_relay_0.002/cosine_mask_dropout_0.01sp0.5wm_lr0.02mep100_baseline/checkpoint_mask_train.pth.tar
01/19 03:39:17 AM | PRINT_FREQ=100
01/19 03:39:17 AM | PRINT_FREQ=100
01/19 03:39:17 AM | SCALE_X=0.5
01/19 03:39:17 AM | SCALE_X=0.5
01/19 03:39:17 AM | SCALE_X2=0.1
01/19 03:39:17 AM | SCALE_X2=0.1
01/19 03:39:17 AM | SEED=2
01/19 03:39:17 AM | SEED=2
01/19 03:39:17 AM | SPARSITY=0.9
01/19 03:39:17 AM | SPARSITY=0.5
01/19 03:39:17 AM | START_EPOCH=0
01/19 03:39:17 AM | START_EPOCH=0
01/19 03:39:17 AM | TEACHER_ARCH=
01/19 03:39:17 AM | TEACHER_ARCH=
01/19 03:39:17 AM | TEACHER_PATH=None
01/19 03:39:17 AM | TEACHER_PATH=None
01/19 03:39:17 AM | THRESHOLD=0.003
01/19 03:39:17 AM | THRESHOLD=0.002
01/19 03:39:17 AM | W_DECAY_EPOCH=20
01/19 03:39:17 AM | W_DECAY_EPOCH=20
01/19 03:39:17 AM | W_GRAD_CLIP=5.0
01/19 03:39:17 AM | W_GRAD_CLIP=5.0
01/19 03:39:17 AM | W_LR=0.01
01/19 03:39:17 AM | W_LR=0.01
01/19 03:39:17 AM | W_LR_MIN=1e-05
01/19 03:39:17 AM | W_LR_MIN=1e-05
01/19 03:39:17 AM | W_MASK_LR=0.02
01/19 03:39:17 AM | W_MASK_LR=0.02
01/19 03:39:17 AM | W_MOMENTUM=0.9
01/19 03:39:17 AM | W_MOMENTUM=0.9
01/19 03:39:17 AM | W_WEIGHT_DECAY=0.001
01/19 03:39:17 AM | W_WEIGHT_DECAY=0.001
01/19 03:39:17 AM | WORKERS=4
01/19 03:39:17 AM | WORKERS=4
01/19 03:39:17 AM | X_SIZE=[1, 3, 32, 32]
01/19 03:39:17 AM | X_SIZE=[1, 3, 32, 32]
01/19 03:39:17 AM | 
01/19 03:39:17 AM | 
01/19 03:39:17 AM | Logger is set - training start
01/19 03:39:17 AM | Logger is set - training start
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:39:35 AM | Train: [ 1/100] Step 000/390 Loss 0.132 Prec@(1,5) (94.5%, 100.0%)
01/19 03:39:40 AM | Train: [ 1/100] Step 100/390 Loss 0.254 Prec@(1,5) (91.8%, 99.8%)
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:39:45 AM | Train: [ 1/100] Step 000/390 Loss 0.154 Prec@(1,5) (95.3%, 100.0%)
01/19 03:39:45 AM | Train: [ 1/100] Step 200/390 Loss 0.243 Prec@(1,5) (91.8%, 99.9%)
01/19 03:39:47 AM | Train: [ 1/100] Step 000/390 Loss 0.229 Prec@(1,5) (89.1%, 100.0%)
01/19 03:39:47 AM | Train: [ 1/100] Step 000/390 Loss 0.026 Prec@(1,5) (99.2%, 100.0%)
01/19 03:39:51 AM | Train: [ 1/100] Step 300/390 Loss 0.243 Prec@(1,5) (91.7%, 99.9%)
01/19 03:39:51 AM | Train: [ 1/100] Step 100/390 Loss 0.470 Prec@(1,5) (83.7%, 99.4%)
01/19 03:39:53 AM | Train: [ 1/100] Step 100/390 Loss 0.480 Prec@(1,5) (83.5%, 99.3%)
01/19 03:39:53 AM | Train: [ 1/100] Step 100/390 Loss 0.241 Prec@(1,5) (92.1%, 99.8%)
01/19 03:39:56 AM | Train: [ 1/100] Step 390/390 Loss 0.243 Prec@(1,5) (91.6%, 99.9%)
01/19 03:39:56 AM | Train: [ 1/100] Final Prec@1 91.6400%
01/19 03:39:57 AM | Valid: [ 1/100] Step 000/078 Loss 0.360 Prec@(1,5) (90.6%, 97.7%)
01/19 03:39:57 AM | Train: [ 1/100] Step 200/390 Loss 0.462 Prec@(1,5) (83.8%, 99.5%)
01/19 03:39:58 AM | Valid: [ 1/100] Step 078/078 Loss 0.422 Prec@(1,5) (86.9%, 99.5%)
01/19 03:39:58 AM | Valid: [ 1/100] Final Prec@1 86.8600%
01/19 03:39:59 AM | Train: [ 1/100] Step 200/390 Loss 0.244 Prec@(1,5) (91.7%, 99.8%)
01/19 03:39:59 AM | Current best Prec@1 = 86.8600%
01/19 03:39:59 AM | Perform validation on training dataset. 
01/19 03:39:59 AM | Train: [ 1/100] Step 200/390 Loss 0.469 Prec@(1,5) (83.8%, 99.3%)
01/19 03:39:59 AM | Valid on training dataset: [ 1/100] Step 000/390 Loss 0.269 Prec@(1,5) (87.5%, 100.0%)
01/19 03:40:02 AM | Valid on training dataset: [ 1/100] Step 100/390 Loss 0.232 Prec@(1,5) (91.7%, 99.9%)
01/19 03:40:03 AM | Train: [ 1/100] Step 300/390 Loss 0.458 Prec@(1,5) (83.8%, 99.5%)
01/19 03:40:04 AM | Valid on training dataset: [ 1/100] Step 200/390 Loss 0.233 Prec@(1,5) (91.5%, 99.9%)
01/19 03:40:04 AM | Train: [ 1/100] Step 300/390 Loss 0.247 Prec@(1,5) (91.5%, 99.9%)
01/19 03:40:05 AM | Train: [ 1/100] Step 300/390 Loss 0.466 Prec@(1,5) (83.8%, 99.4%)
01/19 03:40:06 AM | Valid on training dataset: [ 1/100] Step 300/390 Loss 0.232 Prec@(1,5) (91.6%, 99.9%)
01/19 03:40:08 AM | Valid on training dataset: [ 1/100] Step 390/390 Loss 0.234 Prec@(1,5) (91.5%, 99.9%)
01/19 03:40:08 AM | Train: [ 1/100] Step 390/390 Loss 0.455 Prec@(1,5) (84.0%, 99.5%)
01/19 03:40:08 AM | Valid on training dataset: [ 1/100] Final Prec@1 91.5220%
01/19 03:40:08 AM | Final train Prec@1 = 91.5220%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:40:09 AM | Train: [ 1/100] Final Prec@1 84.0040%
01/19 03:40:09 AM | Valid: [ 1/100] Step 000/078 Loss 0.416 Prec@(1,5) (84.4%, 99.2%)
01/19 03:40:09 AM | Train: [ 2/100] Step 000/390 Loss 0.218 Prec@(1,5) (91.4%, 100.0%)
01/19 03:40:10 AM | Train: [ 1/100] Step 390/390 Loss 0.247 Prec@(1,5) (91.5%, 99.9%)
01/19 03:40:10 AM | Train: [ 1/100] Final Prec@1 91.4560%
01/19 03:40:11 AM | Valid: [ 1/100] Step 000/078 Loss 0.305 Prec@(1,5) (91.4%, 99.2%)
01/19 03:40:11 AM | Valid: [ 1/100] Step 078/078 Loss 0.533 Prec@(1,5) (81.9%, 99.1%)
01/19 03:40:11 AM | Valid: [ 1/100] Final Prec@1 81.9200%
01/19 03:40:11 AM | Train: [ 1/100] Step 390/390 Loss 0.461 Prec@(1,5) (83.9%, 99.4%)
01/19 03:40:11 AM | Current best Prec@1 = 81.9200%
01/19 03:40:11 AM | Perform validation on training dataset. 
01/19 03:40:11 AM | Train: [ 1/100] Final Prec@1 83.9460%
01/19 03:40:12 AM | Valid: [ 1/100] Step 000/078 Loss 0.411 Prec@(1,5) (89.1%, 98.4%)
01/19 03:40:12 AM | Valid on training dataset: [ 1/100] Step 000/390 Loss 0.427 Prec@(1,5) (82.8%, 100.0%)
01/19 03:40:13 AM | Valid: [ 1/100] Step 078/078 Loss 0.389 Prec@(1,5) (87.8%, 99.6%)
01/19 03:40:13 AM | Valid: [ 1/100] Final Prec@1 87.8100%
01/19 03:40:13 AM | Current best Prec@1 = 87.8100%
01/19 03:40:13 AM | Perform validation on training dataset. 
01/19 03:40:14 AM | Valid on training dataset: [ 1/100] Step 000/390 Loss 0.264 Prec@(1,5) (89.8%, 99.2%)
01/19 03:40:14 AM | Valid: [ 1/100] Step 078/078 Loss 0.508 Prec@(1,5) (82.8%, 99.2%)
01/19 03:40:14 AM | Valid: [ 1/100] Final Prec@1 82.7800%
01/19 03:40:15 AM | Current best Prec@1 = 82.7800%
01/19 03:40:15 AM | Perform validation on training dataset. 
01/19 03:40:15 AM | Valid on training dataset: [ 1/100] Step 100/390 Loss 0.427 Prec@(1,5) (84.9%, 99.6%)
01/19 03:40:15 AM | Train: [ 2/100] Step 100/390 Loss 0.220 Prec@(1,5) (92.1%, 99.9%)
01/19 03:40:15 AM | Valid on training dataset: [ 1/100] Step 000/390 Loss 0.517 Prec@(1,5) (78.1%, 100.0%)
01/19 03:40:17 AM | Valid on training dataset: [ 1/100] Step 100/390 Loss 0.207 Prec@(1,5) (92.5%, 100.0%)
01/19 03:40:17 AM | Valid on training dataset: [ 1/100] Step 200/390 Loss 0.425 Prec@(1,5) (84.9%, 99.6%)
01/19 03:40:18 AM | Valid on training dataset: [ 1/100] Step 100/390 Loss 0.408 Prec@(1,5) (85.3%, 99.5%)
01/19 03:40:19 AM | Valid on training dataset: [ 1/100] Step 200/390 Loss 0.208 Prec@(1,5) (92.4%, 100.0%)
01/19 03:40:20 AM | Valid on training dataset: [ 1/100] Step 300/390 Loss 0.426 Prec@(1,5) (85.0%, 99.5%)
01/19 03:40:21 AM | Valid on training dataset: [ 1/100] Step 200/390 Loss 0.410 Prec@(1,5) (85.3%, 99.5%)
01/19 03:40:22 AM | Train: [ 2/100] Step 200/390 Loss 0.216 Prec@(1,5) (92.2%, 99.9%)
01/19 03:40:22 AM | Valid on training dataset: [ 1/100] Step 300/390 Loss 0.207 Prec@(1,5) (92.5%, 99.9%)
01/19 03:40:23 AM | Valid on training dataset: [ 1/100] Step 390/390 Loss 0.432 Prec@(1,5) (84.8%, 99.6%)
01/19 03:40:23 AM | Valid on training dataset: [ 1/100] Final Prec@1 84.8000%
01/19 03:40:23 AM | Final train Prec@1 = 84.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:40:24 AM | Train: [ 2/100] Step 000/390 Loss 0.374 Prec@(1,5) (87.5%, 100.0%)
01/19 03:40:24 AM | Valid on training dataset: [ 1/100] Step 300/390 Loss 0.410 Prec@(1,5) (85.4%, 99.5%)
01/19 03:40:25 AM | Valid on training dataset: [ 1/100] Step 390/390 Loss 0.209 Prec@(1,5) (92.4%, 99.9%)
01/19 03:40:25 AM | Valid on training dataset: [ 1/100] Final Prec@1 92.4280%
01/19 03:40:25 AM | Final train Prec@1 = 92.4280%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:40:26 AM | Train: [ 2/100] Step 000/390 Loss 0.123 Prec@(1,5) (94.5%, 100.0%)
01/19 03:40:26 AM | Valid on training dataset: [ 1/100] Step 390/390 Loss 0.412 Prec@(1,5) (85.3%, 99.5%)
01/19 03:40:26 AM | Valid on training dataset: [ 1/100] Final Prec@1 85.3300%
01/19 03:40:26 AM | Final train Prec@1 = 85.3300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:40:27 AM | Train: [ 2/100] Step 000/390 Loss 0.431 Prec@(1,5) (84.4%, 100.0%)
01/19 03:40:28 AM | Train: [ 2/100] Step 300/390 Loss 0.222 Prec@(1,5) (92.1%, 99.9%)
01/19 03:40:30 AM | Train: [ 2/100] Step 100/390 Loss 0.421 Prec@(1,5) (85.3%, 99.5%)
01/19 03:40:32 AM | Train: [ 2/100] Step 100/390 Loss 0.221 Prec@(1,5) (92.1%, 99.8%)
01/19 03:40:33 AM | Train: [ 2/100] Step 390/390 Loss 0.227 Prec@(1,5) (91.9%, 99.9%)
01/19 03:40:33 AM | Train: [ 2/100] Step 100/390 Loss 0.405 Prec@(1,5) (85.8%, 99.5%)
01/19 03:40:33 AM | Train: [ 2/100] Final Prec@1 91.8600%
01/19 03:40:34 AM | Valid: [ 2/100] Step 000/078 Loss 0.300 Prec@(1,5) (89.8%, 99.2%)
01/19 03:40:36 AM | Valid: [ 2/100] Step 078/078 Loss 0.400 Prec@(1,5) (87.0%, 99.5%)
01/19 03:40:36 AM | Valid: [ 2/100] Final Prec@1 86.9800%
01/19 03:40:36 AM | Current best Prec@1 = 86.9800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:40:36 AM | Train: [ 2/100] Step 200/390 Loss 0.423 Prec@(1,5) (85.1%, 99.5%)
01/19 03:40:37 AM | Train: [ 3/100] Step 000/390 Loss 0.181 Prec@(1,5) (95.3%, 100.0%)
01/19 03:40:38 AM | Train: [ 2/100] Step 200/390 Loss 0.219 Prec@(1,5) (92.2%, 99.9%)
01/19 03:40:40 AM | Train: [ 2/100] Step 200/390 Loss 0.416 Prec@(1,5) (85.4%, 99.5%)
01/19 03:40:43 AM | Train: [ 2/100] Step 300/390 Loss 0.429 Prec@(1,5) (84.9%, 99.5%)
01/19 03:40:43 AM | Train: [ 3/100] Step 100/390 Loss 0.200 Prec@(1,5) (93.2%, 99.9%)
01/19 03:40:44 AM | Train: [ 2/100] Step 300/390 Loss 0.219 Prec@(1,5) (92.3%, 99.9%)
01/19 03:40:46 AM | Train: [ 2/100] Step 300/390 Loss 0.424 Prec@(1,5) (85.2%, 99.4%)
01/19 03:40:49 AM | Train: [ 2/100] Step 390/390 Loss 0.428 Prec@(1,5) (84.9%, 99.5%)
01/19 03:40:49 AM | Train: [ 2/100] Final Prec@1 84.8680%
01/19 03:40:49 AM | Valid: [ 2/100] Step 000/078 Loss 0.462 Prec@(1,5) (85.2%, 98.4%)
01/19 03:40:49 AM | Train: [ 2/100] Step 390/390 Loss 0.220 Prec@(1,5) (92.2%, 99.9%)
01/19 03:40:50 AM | Train: [ 2/100] Final Prec@1 92.2320%
01/19 03:40:50 AM | Train: [ 3/100] Step 200/390 Loss 0.206 Prec@(1,5) (92.9%, 99.9%)
01/19 03:40:50 AM | Valid: [ 2/100] Step 000/078 Loss 0.337 Prec@(1,5) (89.1%, 99.2%)
01/19 03:40:51 AM | Train: [ 2/100] Step 390/390 Loss 0.423 Prec@(1,5) (85.2%, 99.4%)
01/19 03:40:51 AM | Train: [ 2/100] Final Prec@1 85.2280%
01/19 03:40:51 AM | Valid: [ 2/100] Step 078/078 Loss 0.527 Prec@(1,5) (82.4%, 99.2%)
01/19 03:40:51 AM | Valid: [ 2/100] Final Prec@1 82.4300%
01/19 03:40:51 AM | Current best Prec@1 = 82.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:40:52 AM | Valid: [ 2/100] Step 000/078 Loss 0.431 Prec@(1,5) (82.8%, 100.0%)
01/19 03:40:52 AM | Valid: [ 2/100] Step 078/078 Loss 0.440 Prec@(1,5) (86.4%, 99.5%)
01/19 03:40:52 AM | Train: [ 3/100] Step 000/390 Loss 0.489 Prec@(1,5) (82.0%, 100.0%)
01/19 03:40:52 AM | Valid: [ 2/100] Final Prec@1 86.4200%
01/19 03:40:52 AM | Current best Prec@1 = 87.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:40:53 AM | Train: [ 3/100] Step 000/390 Loss 0.242 Prec@(1,5) (92.2%, 100.0%)
01/19 03:40:53 AM | Valid: [ 2/100] Step 078/078 Loss 0.513 Prec@(1,5) (82.5%, 99.2%)
01/19 03:40:54 AM | Valid: [ 2/100] Final Prec@1 82.5500%
01/19 03:40:54 AM | Current best Prec@1 = 82.7800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:40:54 AM | Train: [ 3/100] Step 000/390 Loss 0.405 Prec@(1,5) (87.5%, 99.2%)
01/19 03:40:56 AM | Train: [ 3/100] Step 300/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:40:58 AM | Train: [ 3/100] Step 100/390 Loss 0.411 Prec@(1,5) (85.8%, 99.5%)
01/19 03:41:00 AM | Train: [ 3/100] Step 100/390 Loss 0.196 Prec@(1,5) (93.1%, 99.9%)
01/19 03:41:01 AM | Train: [ 3/100] Step 100/390 Loss 0.394 Prec@(1,5) (86.2%, 99.6%)
01/19 03:41:01 AM | Train: [ 3/100] Step 390/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:41:02 AM | Train: [ 3/100] Final Prec@1 92.6900%
01/19 03:41:02 AM | Valid: [ 3/100] Step 000/078 Loss 0.324 Prec@(1,5) (93.0%, 98.4%)
01/19 03:41:04 AM | Valid: [ 3/100] Step 078/078 Loss 0.366 Prec@(1,5) (88.4%, 99.5%)
01/19 03:41:04 AM | Valid: [ 3/100] Final Prec@1 88.3700%
01/19 03:41:04 AM | Current best Prec@1 = 88.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:41:05 AM | Train: [ 3/100] Step 200/390 Loss 0.419 Prec@(1,5) (85.5%, 99.5%)
01/19 03:41:05 AM | Train: [ 4/100] Step 000/390 Loss 0.193 Prec@(1,5) (93.8%, 100.0%)
01/19 03:41:06 AM | Train: [ 3/100] Step 200/390 Loss 0.211 Prec@(1,5) (92.7%, 99.9%)
01/19 03:41:07 AM | Train: [ 3/100] Step 200/390 Loss 0.409 Prec@(1,5) (85.7%, 99.4%)
01/19 03:41:11 AM | Train: [ 3/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.4%, 99.5%)
01/19 03:41:11 AM | Train: [ 4/100] Step 100/390 Loss 0.215 Prec@(1,5) (92.3%, 99.9%)
01/19 03:41:12 AM | Train: [ 3/100] Step 300/390 Loss 0.212 Prec@(1,5) (92.6%, 99.9%)
01/19 03:41:13 AM | Train: [ 3/100] Step 300/390 Loss 0.416 Prec@(1,5) (85.4%, 99.4%)
01/19 03:41:16 AM | Train: [ 3/100] Step 390/390 Loss 0.422 Prec@(1,5) (85.3%, 99.5%)
01/19 03:41:17 AM | Train: [ 3/100] Final Prec@1 85.2980%
01/19 03:41:17 AM | Valid: [ 3/100] Step 000/078 Loss 0.479 Prec@(1,5) (82.8%, 99.2%)
01/19 03:41:18 AM | Train: [ 3/100] Step 390/390 Loss 0.214 Prec@(1,5) (92.5%, 99.9%)
01/19 03:41:18 AM | Train: [ 4/100] Step 200/390 Loss 0.211 Prec@(1,5) (92.6%, 99.9%)
01/19 03:41:18 AM | Train: [ 3/100] Final Prec@1 92.4800%
01/19 03:41:18 AM | Valid: [ 3/100] Step 000/078 Loss 0.320 Prec@(1,5) (89.1%, 98.4%)
01/19 03:41:19 AM | Train: [ 3/100] Step 390/390 Loss 0.416 Prec@(1,5) (85.5%, 99.4%)
01/19 03:41:19 AM | Train: [ 3/100] Final Prec@1 85.4520%
01/19 03:41:19 AM | Valid: [ 3/100] Step 078/078 Loss 0.469 Prec@(1,5) (84.0%, 99.4%)
01/19 03:41:19 AM | Valid: [ 3/100] Final Prec@1 84.0500%
01/19 03:41:20 AM | Current best Prec@1 = 84.0500%
01/19 03:41:20 AM | Valid: [ 3/100] Step 000/078 Loss 0.444 Prec@(1,5) (87.5%, 99.2%)
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:41:20 AM | Valid: [ 3/100] Step 078/078 Loss 0.396 Prec@(1,5) (87.2%, 99.6%)
01/19 03:41:20 AM | Train: [ 4/100] Step 000/390 Loss 0.394 Prec@(1,5) (86.7%, 99.2%)
01/19 03:41:21 AM | Valid: [ 3/100] Final Prec@1 87.2400%
01/19 03:41:21 AM | Current best Prec@1 = 87.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:41:21 AM | Valid: [ 3/100] Step 078/078 Loss 0.526 Prec@(1,5) (83.0%, 99.1%)
01/19 03:41:21 AM | Valid: [ 3/100] Final Prec@1 83.0200%
01/19 03:41:21 AM | Train: [ 4/100] Step 000/390 Loss 0.160 Prec@(1,5) (95.3%, 100.0%)
01/19 03:41:21 AM | Current best Prec@1 = 83.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:41:22 AM | Train: [ 4/100] Step 000/390 Loss 0.451 Prec@(1,5) (85.9%, 100.0%)
01/19 03:41:24 AM | Train: [ 4/100] Step 300/390 Loss 0.215 Prec@(1,5) (92.5%, 99.9%)
01/19 03:41:27 AM | Train: [ 4/100] Step 100/390 Loss 0.418 Prec@(1,5) (85.3%, 99.5%)
01/19 03:41:28 AM | Train: [ 4/100] Step 100/390 Loss 0.211 Prec@(1,5) (92.5%, 99.9%)
01/19 03:41:28 AM | Train: [ 4/100] Step 100/390 Loss 0.424 Prec@(1,5) (85.4%, 99.4%)
01/19 03:41:29 AM | Train: [ 4/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.4%, 99.9%)
01/19 03:41:30 AM | Train: [ 4/100] Final Prec@1 92.3960%
01/19 03:41:30 AM | Valid: [ 4/100] Step 000/078 Loss 0.187 Prec@(1,5) (91.4%, 100.0%)
01/19 03:41:32 AM | Valid: [ 4/100] Step 078/078 Loss 0.366 Prec@(1,5) (88.3%, 99.6%)
01/19 03:41:32 AM | Valid: [ 4/100] Final Prec@1 88.2900%
01/19 03:41:32 AM | Current best Prec@1 = 88.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:41:33 AM | Train: [ 5/100] Step 000/390 Loss 0.157 Prec@(1,5) (94.5%, 100.0%)
01/19 03:41:33 AM | Train: [ 4/100] Step 200/390 Loss 0.419 Prec@(1,5) (85.2%, 99.5%)
01/19 03:41:34 AM | Train: [ 4/100] Step 200/390 Loss 0.210 Prec@(1,5) (92.6%, 99.9%)
01/19 03:41:34 AM | Train: [ 4/100] Step 200/390 Loss 0.421 Prec@(1,5) (85.6%, 99.4%)
01/19 03:41:39 AM | Train: [ 5/100] Step 100/390 Loss 0.196 Prec@(1,5) (92.9%, 99.9%)
01/19 03:41:40 AM | Train: [ 4/100] Step 300/390 Loss 0.419 Prec@(1,5) (85.2%, 99.5%)
01/19 03:41:40 AM | Train: [ 4/100] Step 300/390 Loss 0.210 Prec@(1,5) (92.6%, 99.9%)
01/19 03:41:41 AM | Train: [ 4/100] Step 300/390 Loss 0.414 Prec@(1,5) (85.7%, 99.5%)
01/19 03:41:46 AM | Train: [ 4/100] Step 390/390 Loss 0.417 Prec@(1,5) (85.2%, 99.5%)
01/19 03:41:46 AM | Train: [ 5/100] Step 200/390 Loss 0.207 Prec@(1,5) (92.5%, 99.9%)
01/19 03:41:46 AM | Train: [ 4/100] Final Prec@1 85.2320%
01/19 03:41:46 AM | Valid: [ 4/100] Step 000/078 Loss 0.386 Prec@(1,5) (85.9%, 99.2%)
01/19 03:41:46 AM | Train: [ 4/100] Step 390/390 Loss 0.212 Prec@(1,5) (92.5%, 99.9%)
01/19 03:41:46 AM | Train: [ 4/100] Step 390/390 Loss 0.414 Prec@(1,5) (85.6%, 99.5%)
01/19 03:41:47 AM | Train: [ 4/100] Final Prec@1 92.5180%
01/19 03:41:47 AM | Train: [ 4/100] Final Prec@1 85.6320%
01/19 03:41:47 AM | Valid: [ 4/100] Step 000/078 Loss 0.244 Prec@(1,5) (89.1%, 100.0%)
01/19 03:41:47 AM | Valid: [ 4/100] Step 000/078 Loss 0.406 Prec@(1,5) (85.9%, 99.2%)
01/19 03:41:48 AM | Valid: [ 4/100] Step 078/078 Loss 0.473 Prec@(1,5) (84.1%, 99.3%)
01/19 03:41:48 AM | Valid: [ 4/100] Final Prec@1 84.1000%
01/19 03:41:49 AM | Current best Prec@1 = 84.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:41:49 AM | Valid: [ 4/100] Step 078/078 Loss 0.478 Prec@(1,5) (83.9%, 99.2%)
01/19 03:41:49 AM | Valid: [ 4/100] Step 078/078 Loss 0.364 Prec@(1,5) (88.7%, 99.5%)
01/19 03:41:49 AM | Train: [ 5/100] Step 000/390 Loss 0.388 Prec@(1,5) (87.5%, 99.2%)
01/19 03:41:49 AM | Valid: [ 4/100] Final Prec@1 83.9100%
01/19 03:41:49 AM | Valid: [ 4/100] Final Prec@1 88.7100%
01/19 03:41:49 AM | Current best Prec@1 = 83.9100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:41:49 AM | Current best Prec@1 = 88.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:41:50 AM | Train: [ 5/100] Step 000/390 Loss 0.400 Prec@(1,5) (85.9%, 99.2%)
01/19 03:41:50 AM | Train: [ 5/100] Step 000/390 Loss 0.165 Prec@(1,5) (93.0%, 100.0%)
01/19 03:41:51 AM | Train: [ 5/100] Step 300/390 Loss 0.212 Prec@(1,5) (92.4%, 99.9%)
01/19 03:41:55 AM | Train: [ 5/100] Step 100/390 Loss 0.408 Prec@(1,5) (85.6%, 99.6%)
01/19 03:41:57 AM | Train: [ 5/100] Step 100/390 Loss 0.408 Prec@(1,5) (85.2%, 99.5%)
01/19 03:41:57 AM | Train: [ 5/100] Step 100/390 Loss 0.203 Prec@(1,5) (92.9%, 99.9%)
01/19 03:41:57 AM | Train: [ 5/100] Step 390/390 Loss 0.211 Prec@(1,5) (92.5%, 99.9%)
01/19 03:41:57 AM | Train: [ 5/100] Final Prec@1 92.5100%
01/19 03:41:58 AM | Valid: [ 5/100] Step 000/078 Loss 0.328 Prec@(1,5) (90.6%, 99.2%)
01/19 03:41:59 AM | Valid: [ 5/100] Step 078/078 Loss 0.388 Prec@(1,5) (87.4%, 99.4%)
01/19 03:41:59 AM | Valid: [ 5/100] Final Prec@1 87.3700%
01/19 03:42:00 AM | Current best Prec@1 = 88.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:42:00 AM | Train: [ 6/100] Step 000/390 Loss 0.215 Prec@(1,5) (90.6%, 100.0%)
01/19 03:42:02 AM | Train: [ 5/100] Step 200/390 Loss 0.417 Prec@(1,5) (85.5%, 99.5%)
01/19 03:42:03 AM | Train: [ 5/100] Step 200/390 Loss 0.416 Prec@(1,5) (85.2%, 99.5%)
01/19 03:42:03 AM | Train: [ 5/100] Step 200/390 Loss 0.206 Prec@(1,5) (92.7%, 99.9%)
01/19 03:42:07 AM | Train: [ 6/100] Step 100/390 Loss 0.197 Prec@(1,5) (93.0%, 100.0%)
01/19 03:42:09 AM | Train: [ 5/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.3%, 99.5%)
01/19 03:42:09 AM | Train: [ 5/100] Step 300/390 Loss 0.416 Prec@(1,5) (85.3%, 99.5%)
01/19 03:42:09 AM | Train: [ 5/100] Step 300/390 Loss 0.208 Prec@(1,5) (92.6%, 99.9%)
01/19 03:42:13 AM | Train: [ 6/100] Step 200/390 Loss 0.202 Prec@(1,5) (92.9%, 100.0%)
01/19 03:42:15 AM | Train: [ 5/100] Step 390/390 Loss 0.419 Prec@(1,5) (85.4%, 99.5%)
01/19 03:42:15 AM | Train: [ 5/100] Step 390/390 Loss 0.208 Prec@(1,5) (92.6%, 99.9%)
01/19 03:42:15 AM | Train: [ 5/100] Step 390/390 Loss 0.417 Prec@(1,5) (85.3%, 99.5%)
01/19 03:42:15 AM | Train: [ 5/100] Final Prec@1 92.5820%
01/19 03:42:15 AM | Train: [ 5/100] Final Prec@1 85.4120%
01/19 03:42:15 AM | Train: [ 5/100] Final Prec@1 85.3300%
01/19 03:42:15 AM | Valid: [ 5/100] Step 000/078 Loss 0.399 Prec@(1,5) (88.3%, 99.2%)
01/19 03:42:15 AM | Valid: [ 5/100] Step 000/078 Loss 0.340 Prec@(1,5) (89.8%, 100.0%)
01/19 03:42:16 AM | Valid: [ 5/100] Step 000/078 Loss 0.413 Prec@(1,5) (86.7%, 99.2%)
01/19 03:42:17 AM | Valid: [ 5/100] Step 078/078 Loss 0.486 Prec@(1,5) (83.4%, 99.3%)
01/19 03:42:17 AM | Valid: [ 5/100] Step 078/078 Loss 0.497 Prec@(1,5) (83.1%, 99.3%)
01/19 03:42:17 AM | Valid: [ 5/100] Final Prec@1 83.3800%
01/19 03:42:17 AM | Valid: [ 5/100] Step 078/078 Loss 0.402 Prec@(1,5) (87.2%, 99.5%)
01/19 03:42:17 AM | Valid: [ 5/100] Final Prec@1 83.0900%
01/19 03:42:18 AM | Current best Prec@1 = 84.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:42:18 AM | Valid: [ 5/100] Final Prec@1 87.2200%
01/19 03:42:18 AM | Current best Prec@1 = 83.9100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:42:18 AM | Current best Prec@1 = 88.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:42:18 AM | Train: [ 6/100] Step 000/390 Loss 0.299 Prec@(1,5) (88.3%, 100.0%)
01/19 03:42:18 AM | Train: [ 6/100] Step 000/390 Loss 0.186 Prec@(1,5) (92.2%, 100.0%)
01/19 03:42:19 AM | Train: [ 6/100] Step 000/390 Loss 0.436 Prec@(1,5) (85.2%, 99.2%)
01/19 03:42:19 AM | Train: [ 6/100] Step 300/390 Loss 0.211 Prec@(1,5) (92.6%, 99.9%)
01/19 03:42:24 AM | Train: [ 6/100] Step 390/390 Loss 0.212 Prec@(1,5) (92.5%, 99.9%)
01/19 03:42:24 AM | Train: [ 6/100] Final Prec@1 92.4860%
01/19 03:42:24 AM | Train: [ 6/100] Step 100/390 Loss 0.393 Prec@(1,5) (85.9%, 99.6%)
01/19 03:42:24 AM | Train: [ 6/100] Step 100/390 Loss 0.201 Prec@(1,5) (92.8%, 99.9%)
01/19 03:42:25 AM | Train: [ 6/100] Step 100/390 Loss 0.401 Prec@(1,5) (86.2%, 99.6%)
01/19 03:42:25 AM | Valid: [ 6/100] Step 000/078 Loss 0.243 Prec@(1,5) (92.2%, 100.0%)
01/19 03:42:27 AM | Valid: [ 6/100] Step 078/078 Loss 0.365 Prec@(1,5) (88.5%, 99.6%)
01/19 03:42:27 AM | Valid: [ 6/100] Final Prec@1 88.4700%
01/19 03:42:27 AM | Current best Prec@1 = 88.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:42:27 AM | Train: [ 7/100] Step 000/390 Loss 0.169 Prec@(1,5) (93.0%, 100.0%)
01/19 03:42:31 AM | Train: [ 6/100] Step 200/390 Loss 0.202 Prec@(1,5) (92.8%, 99.9%)
01/19 03:42:31 AM | Train: [ 6/100] Step 200/390 Loss 0.406 Prec@(1,5) (85.7%, 99.5%)
01/19 03:42:31 AM | Train: [ 6/100] Step 200/390 Loss 0.405 Prec@(1,5) (85.9%, 99.5%)
01/19 03:42:34 AM | Train: [ 7/100] Step 100/390 Loss 0.192 Prec@(1,5) (93.3%, 99.9%)
01/19 03:42:37 AM | Train: [ 6/100] Step 300/390 Loss 0.411 Prec@(1,5) (85.7%, 99.5%)
01/19 03:42:37 AM | Train: [ 6/100] Step 300/390 Loss 0.204 Prec@(1,5) (92.7%, 99.9%)
01/19 03:42:38 AM | Train: [ 6/100] Step 300/390 Loss 0.414 Prec@(1,5) (85.6%, 99.5%)
01/19 03:42:40 AM | Train: [ 7/100] Step 200/390 Loss 0.206 Prec@(1,5) (92.9%, 99.9%)
01/19 03:42:42 AM | Train: [ 6/100] Step 390/390 Loss 0.416 Prec@(1,5) (85.5%, 99.5%)
01/19 03:42:42 AM | Train: [ 6/100] Final Prec@1 85.5020%
01/19 03:42:42 AM | Train: [ 6/100] Step 390/390 Loss 0.208 Prec@(1,5) (92.6%, 99.9%)
01/19 03:42:43 AM | Train: [ 6/100] Final Prec@1 92.6260%
01/19 03:42:43 AM | Valid: [ 6/100] Step 000/078 Loss 0.408 Prec@(1,5) (86.7%, 99.2%)
01/19 03:42:43 AM | Train: [ 6/100] Step 390/390 Loss 0.419 Prec@(1,5) (85.5%, 99.5%)
01/19 03:42:43 AM | Train: [ 6/100] Final Prec@1 85.4520%
01/19 03:42:43 AM | Valid: [ 6/100] Step 000/078 Loss 0.319 Prec@(1,5) (90.6%, 100.0%)
01/19 03:42:44 AM | Valid: [ 6/100] Step 000/078 Loss 0.408 Prec@(1,5) (89.8%, 99.2%)
01/19 03:42:45 AM | Valid: [ 6/100] Step 078/078 Loss 0.495 Prec@(1,5) (83.4%, 99.2%)
01/19 03:42:45 AM | Valid: [ 6/100] Final Prec@1 83.4100%
01/19 03:42:45 AM | Current best Prec@1 = 84.1000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:42:45 AM | Valid: [ 6/100] Step 078/078 Loss 0.404 Prec@(1,5) (87.5%, 99.4%)
01/19 03:42:46 AM | Valid: [ 6/100] Final Prec@1 87.5100%
01/19 03:42:46 AM | Valid: [ 6/100] Step 078/078 Loss 0.465 Prec@(1,5) (84.2%, 99.3%)
01/19 03:42:46 AM | Current best Prec@1 = 88.7100%
01/19 03:42:46 AM | Valid: [ 6/100] Final Prec@1 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:42:46 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:42:46 AM | Train: [ 7/100] Step 000/390 Loss 0.455 Prec@(1,5) (85.2%, 100.0%)
01/19 03:42:46 AM | Train: [ 7/100] Step 300/390 Loss 0.216 Prec@(1,5) (92.6%, 99.9%)
01/19 03:42:46 AM | Train: [ 7/100] Step 000/390 Loss 0.158 Prec@(1,5) (95.3%, 100.0%)
01/19 03:42:47 AM | Train: [ 7/100] Step 000/390 Loss 0.437 Prec@(1,5) (85.9%, 98.4%)
01/19 03:42:52 AM | Train: [ 7/100] Step 390/390 Loss 0.219 Prec@(1,5) (92.4%, 99.9%)
01/19 03:42:52 AM | Train: [ 7/100] Final Prec@1 92.4440%
01/19 03:42:52 AM | Train: [ 7/100] Step 100/390 Loss 0.409 Prec@(1,5) (86.0%, 99.6%)
01/19 03:42:52 AM | Valid: [ 7/100] Step 000/078 Loss 0.281 Prec@(1,5) (92.2%, 100.0%)
01/19 03:42:53 AM | Train: [ 7/100] Step 100/390 Loss 0.199 Prec@(1,5) (92.9%, 99.9%)
01/19 03:42:53 AM | Train: [ 7/100] Step 100/390 Loss 0.405 Prec@(1,5) (86.0%, 99.6%)
01/19 03:42:54 AM | Valid: [ 7/100] Step 078/078 Loss 0.345 Prec@(1,5) (88.9%, 99.5%)
01/19 03:42:54 AM | Valid: [ 7/100] Final Prec@1 88.9500%
01/19 03:42:54 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:42:55 AM | Train: [ 8/100] Step 000/390 Loss 0.156 Prec@(1,5) (95.3%, 100.0%)
01/19 03:42:58 AM | Train: [ 7/100] Step 200/390 Loss 0.420 Prec@(1,5) (85.3%, 99.6%)
01/19 03:42:59 AM | Train: [ 7/100] Step 200/390 Loss 0.204 Prec@(1,5) (92.9%, 99.9%)
01/19 03:42:59 AM | Train: [ 7/100] Step 200/390 Loss 0.415 Prec@(1,5) (85.5%, 99.5%)
01/19 03:43:01 AM | Train: [ 8/100] Step 100/390 Loss 0.202 Prec@(1,5) (92.9%, 99.9%)
01/19 03:43:04 AM | Train: [ 7/100] Step 300/390 Loss 0.427 Prec@(1,5) (85.0%, 99.5%)
01/19 03:43:05 AM | Train: [ 7/100] Step 300/390 Loss 0.213 Prec@(1,5) (92.6%, 99.9%)
01/19 03:43:06 AM | Train: [ 7/100] Step 300/390 Loss 0.426 Prec@(1,5) (85.2%, 99.5%)
01/19 03:43:08 AM | Train: [ 8/100] Step 200/390 Loss 0.206 Prec@(1,5) (92.9%, 99.9%)
01/19 03:43:10 AM | Train: [ 7/100] Step 390/390 Loss 0.428 Prec@(1,5) (85.0%, 99.5%)
01/19 03:43:10 AM | Train: [ 7/100] Final Prec@1 85.0260%
01/19 03:43:11 AM | Valid: [ 7/100] Step 000/078 Loss 0.376 Prec@(1,5) (82.8%, 99.2%)
01/19 03:43:11 AM | Train: [ 7/100] Step 390/390 Loss 0.217 Prec@(1,5) (92.4%, 99.9%)
01/19 03:43:11 AM | Train: [ 7/100] Step 390/390 Loss 0.426 Prec@(1,5) (85.2%, 99.5%)
01/19 03:43:11 AM | Train: [ 7/100] Final Prec@1 92.3960%
01/19 03:43:12 AM | Train: [ 7/100] Final Prec@1 85.1840%
01/19 03:43:12 AM | Valid: [ 7/100] Step 000/078 Loss 0.250 Prec@(1,5) (92.2%, 100.0%)
01/19 03:43:12 AM | Valid: [ 7/100] Step 000/078 Loss 0.442 Prec@(1,5) (82.8%, 99.2%)
01/19 03:43:13 AM | Valid: [ 7/100] Step 078/078 Loss 0.462 Prec@(1,5) (84.4%, 99.2%)
01/19 03:43:13 AM | Valid: [ 7/100] Final Prec@1 84.4300%
01/19 03:43:13 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:43:14 AM | Train: [ 8/100] Step 300/390 Loss 0.215 Prec@(1,5) (92.4%, 99.9%)
01/19 03:43:14 AM | Train: [ 8/100] Step 000/390 Loss 0.365 Prec@(1,5) (89.8%, 100.0%)
01/19 03:43:14 AM | Valid: [ 7/100] Step 078/078 Loss 0.351 Prec@(1,5) (88.9%, 99.5%)
01/19 03:43:14 AM | Valid: [ 7/100] Final Prec@1 88.9200%
01/19 03:43:14 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:43:14 AM | Valid: [ 7/100] Step 078/078 Loss 0.499 Prec@(1,5) (83.7%, 99.2%)
01/19 03:43:14 AM | Valid: [ 7/100] Final Prec@1 83.6700%
01/19 03:43:14 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:43:15 AM | Train: [ 8/100] Step 000/390 Loss 0.190 Prec@(1,5) (91.4%, 100.0%)
01/19 03:43:15 AM | Train: [ 8/100] Step 000/390 Loss 0.535 Prec@(1,5) (82.8%, 99.2%)
01/19 03:43:19 AM | Train: [ 8/100] Step 390/390 Loss 0.214 Prec@(1,5) (92.5%, 99.9%)
01/19 03:43:19 AM | Train: [ 8/100] Final Prec@1 92.4840%
01/19 03:43:19 AM | Train: [ 8/100] Step 100/390 Loss 0.417 Prec@(1,5) (85.7%, 99.4%)
01/19 03:43:20 AM | Valid: [ 8/100] Step 000/078 Loss 0.317 Prec@(1,5) (89.1%, 99.2%)
01/19 03:43:21 AM | Train: [ 8/100] Step 100/390 Loss 0.196 Prec@(1,5) (93.1%, 99.9%)
01/19 03:43:22 AM | Valid: [ 8/100] Step 078/078 Loss 0.384 Prec@(1,5) (87.8%, 99.5%)
01/19 03:43:22 AM | Train: [ 8/100] Step 100/390 Loss 0.424 Prec@(1,5) (85.0%, 99.5%)
01/19 03:43:22 AM | Valid: [ 8/100] Final Prec@1 87.7500%
01/19 03:43:22 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:43:23 AM | Train: [ 9/100] Step 000/390 Loss 0.279 Prec@(1,5) (91.4%, 99.2%)
01/19 03:43:26 AM | Train: [ 8/100] Step 200/390 Loss 0.418 Prec@(1,5) (85.5%, 99.4%)
01/19 03:43:27 AM | Train: [ 8/100] Step 200/390 Loss 0.199 Prec@(1,5) (93.0%, 99.9%)
01/19 03:43:28 AM | Train: [ 8/100] Step 200/390 Loss 0.426 Prec@(1,5) (85.0%, 99.4%)
01/19 03:43:29 AM | Train: [ 9/100] Step 100/390 Loss 0.208 Prec@(1,5) (92.8%, 99.9%)
01/19 03:43:32 AM | Train: [ 8/100] Step 300/390 Loss 0.425 Prec@(1,5) (85.3%, 99.4%)
01/19 03:43:34 AM | Train: [ 8/100] Step 300/390 Loss 0.204 Prec@(1,5) (92.8%, 99.9%)
01/19 03:43:35 AM | Train: [ 8/100] Step 300/390 Loss 0.425 Prec@(1,5) (85.1%, 99.5%)
01/19 03:43:35 AM | Train: [ 9/100] Step 200/390 Loss 0.202 Prec@(1,5) (92.9%, 99.9%)
01/19 03:43:38 AM | Train: [ 8/100] Step 390/390 Loss 0.419 Prec@(1,5) (85.5%, 99.4%)
01/19 03:43:38 AM | Train: [ 8/100] Final Prec@1 85.5200%
01/19 03:43:39 AM | Valid: [ 8/100] Step 000/078 Loss 0.444 Prec@(1,5) (88.3%, 99.2%)
01/19 03:43:39 AM | Train: [ 8/100] Step 390/390 Loss 0.209 Prec@(1,5) (92.6%, 99.9%)
01/19 03:43:40 AM | Train: [ 8/100] Final Prec@1 92.6420%
01/19 03:43:40 AM | Valid: [ 8/100] Step 000/078 Loss 0.303 Prec@(1,5) (93.0%, 99.2%)
01/19 03:43:40 AM | Train: [ 8/100] Step 390/390 Loss 0.420 Prec@(1,5) (85.3%, 99.4%)
01/19 03:43:40 AM | Train: [ 8/100] Final Prec@1 85.3100%
01/19 03:43:40 AM | Valid: [ 8/100] Step 078/078 Loss 0.496 Prec@(1,5) (83.0%, 99.2%)
01/19 03:43:41 AM | Valid: [ 8/100] Final Prec@1 82.9600%
01/19 03:43:41 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:43:41 AM | Valid: [ 8/100] Step 000/078 Loss 0.453 Prec@(1,5) (83.6%, 99.2%)
01/19 03:43:41 AM | Train: [ 9/100] Step 300/390 Loss 0.205 Prec@(1,5) (92.8%, 99.9%)
01/19 03:43:42 AM | Train: [ 9/100] Step 000/390 Loss 0.372 Prec@(1,5) (86.7%, 100.0%)
01/19 03:43:42 AM | Valid: [ 8/100] Step 078/078 Loss 0.389 Prec@(1,5) (87.5%, 99.6%)
01/19 03:43:42 AM | Valid: [ 8/100] Final Prec@1 87.5000%
01/19 03:43:42 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:43:43 AM | Valid: [ 8/100] Step 078/078 Loss 0.490 Prec@(1,5) (83.5%, 99.1%)
01/19 03:43:43 AM | Valid: [ 8/100] Final Prec@1 83.5100%
01/19 03:43:43 AM | Train: [ 9/100] Step 000/390 Loss 0.185 Prec@(1,5) (92.2%, 100.0%)
01/19 03:43:43 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:43:44 AM | Train: [ 9/100] Step 000/390 Loss 0.391 Prec@(1,5) (85.2%, 99.2%)
01/19 03:43:47 AM | Train: [ 9/100] Step 390/390 Loss 0.210 Prec@(1,5) (92.6%, 99.9%)
01/19 03:43:47 AM | Train: [ 9/100] Final Prec@1 92.5900%
01/19 03:43:47 AM | Valid: [ 9/100] Step 000/078 Loss 0.552 Prec@(1,5) (83.6%, 99.2%)
01/19 03:43:48 AM | Train: [ 9/100] Step 100/390 Loss 0.402 Prec@(1,5) (85.8%, 99.6%)
01/19 03:43:49 AM | Train: [ 9/100] Step 100/390 Loss 0.200 Prec@(1,5) (93.0%, 99.9%)
01/19 03:43:49 AM | Valid: [ 9/100] Step 078/078 Loss 0.430 Prec@(1,5) (86.5%, 99.2%)
01/19 03:43:49 AM | Valid: [ 9/100] Final Prec@1 86.4600%
01/19 03:43:50 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:43:50 AM | Train: [ 9/100] Step 100/390 Loss 0.401 Prec@(1,5) (85.7%, 99.5%)
01/19 03:43:50 AM | Train: [10/100] Step 000/390 Loss 0.232 Prec@(1,5) (90.6%, 100.0%)
01/19 03:43:54 AM | Train: [ 9/100] Step 200/390 Loss 0.413 Prec@(1,5) (85.3%, 99.6%)
01/19 03:43:55 AM | Train: [ 9/100] Step 200/390 Loss 0.200 Prec@(1,5) (93.1%, 99.9%)
01/19 03:43:56 AM | Train: [ 9/100] Step 200/390 Loss 0.411 Prec@(1,5) (85.4%, 99.5%)
01/19 03:43:57 AM | Train: [10/100] Step 100/390 Loss 0.208 Prec@(1,5) (92.6%, 99.9%)
01/19 03:44:01 AM | Train: [ 9/100] Step 300/390 Loss 0.415 Prec@(1,5) (85.4%, 99.5%)
01/19 03:44:01 AM | Train: [ 9/100] Step 300/390 Loss 0.203 Prec@(1,5) (92.9%, 99.9%)
01/19 03:44:02 AM | Train: [ 9/100] Step 300/390 Loss 0.414 Prec@(1,5) (85.4%, 99.5%)
01/19 03:44:03 AM | Train: [10/100] Step 200/390 Loss 0.211 Prec@(1,5) (92.5%, 99.9%)
01/19 03:44:06 AM | Train: [ 9/100] Step 390/390 Loss 0.419 Prec@(1,5) (85.3%, 99.5%)
01/19 03:44:07 AM | Train: [ 9/100] Final Prec@1 85.3220%
01/19 03:44:07 AM | Valid: [ 9/100] Step 000/078 Loss 0.549 Prec@(1,5) (82.8%, 97.7%)
01/19 03:44:07 AM | Train: [ 9/100] Step 390/390 Loss 0.205 Prec@(1,5) (92.8%, 99.9%)
01/19 03:44:08 AM | Train: [ 9/100] Final Prec@1 92.8420%
01/19 03:44:08 AM | Train: [ 9/100] Step 390/390 Loss 0.417 Prec@(1,5) (85.3%, 99.5%)
01/19 03:44:08 AM | Train: [ 9/100] Final Prec@1 85.3140%
01/19 03:44:08 AM | Valid: [ 9/100] Step 000/078 Loss 0.343 Prec@(1,5) (89.8%, 99.2%)
01/19 03:44:09 AM | Valid: [ 9/100] Step 000/078 Loss 0.570 Prec@(1,5) (79.7%, 99.2%)
01/19 03:44:09 AM | Train: [10/100] Step 300/390 Loss 0.212 Prec@(1,5) (92.5%, 99.9%)
01/19 03:44:09 AM | Valid: [ 9/100] Step 078/078 Loss 0.535 Prec@(1,5) (82.3%, 99.2%)
01/19 03:44:09 AM | Valid: [ 9/100] Final Prec@1 82.2900%
01/19 03:44:09 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:44:10 AM | Train: [10/100] Step 000/390 Loss 0.343 Prec@(1,5) (87.5%, 100.0%)
01/19 03:44:10 AM | Valid: [ 9/100] Step 078/078 Loss 0.412 Prec@(1,5) (87.1%, 99.4%)
01/19 03:44:10 AM | Valid: [ 9/100] Final Prec@1 87.1000%
01/19 03:44:10 AM | Valid: [ 9/100] Step 078/078 Loss 0.516 Prec@(1,5) (82.4%, 98.9%)
01/19 03:44:10 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:44:11 AM | Valid: [ 9/100] Final Prec@1 82.3800%
01/19 03:44:11 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:44:11 AM | Train: [10/100] Step 000/390 Loss 0.200 Prec@(1,5) (93.0%, 100.0%)
01/19 03:44:11 AM | Train: [10/100] Step 000/390 Loss 0.351 Prec@(1,5) (89.1%, 99.2%)
01/19 03:44:14 AM | Train: [10/100] Step 390/390 Loss 0.217 Prec@(1,5) (92.4%, 99.9%)
01/19 03:44:14 AM | Train: [10/100] Final Prec@1 92.3640%
01/19 03:44:14 AM | Valid: [10/100] Step 000/078 Loss 0.386 Prec@(1,5) (87.5%, 98.4%)
01/19 03:44:16 AM | Train: [10/100] Step 100/390 Loss 0.429 Prec@(1,5) (85.2%, 99.5%)
01/19 03:44:16 AM | Valid: [10/100] Step 078/078 Loss 0.353 Prec@(1,5) (88.5%, 99.5%)
01/19 03:44:16 AM | Valid: [10/100] Final Prec@1 88.5100%
01/19 03:44:17 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:44:17 AM | Train: [10/100] Step 100/390 Loss 0.211 Prec@(1,5) (92.5%, 99.9%)
01/19 03:44:17 AM | Train: [11/100] Step 000/390 Loss 0.214 Prec@(1,5) (91.4%, 100.0%)
01/19 03:44:18 AM | Train: [10/100] Step 100/390 Loss 0.415 Prec@(1,5) (85.8%, 99.3%)
01/19 03:44:22 AM | Train: [10/100] Step 200/390 Loss 0.428 Prec@(1,5) (85.2%, 99.5%)
01/19 03:44:23 AM | Train: [10/100] Step 200/390 Loss 0.213 Prec@(1,5) (92.5%, 99.9%)
01/19 03:44:24 AM | Train: [11/100] Step 100/390 Loss 0.205 Prec@(1,5) (92.7%, 99.9%)
01/19 03:44:24 AM | Train: [10/100] Step 200/390 Loss 0.419 Prec@(1,5) (85.5%, 99.4%)
01/19 03:44:28 AM | Train: [10/100] Step 300/390 Loss 0.425 Prec@(1,5) (85.1%, 99.5%)
01/19 03:44:30 AM | Train: [10/100] Step 300/390 Loss 0.213 Prec@(1,5) (92.5%, 99.9%)
01/19 03:44:30 AM | Train: [11/100] Step 200/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:44:31 AM | Train: [10/100] Step 300/390 Loss 0.421 Prec@(1,5) (85.4%, 99.4%)
01/19 03:44:34 AM | Train: [10/100] Step 390/390 Loss 0.426 Prec@(1,5) (85.1%, 99.5%)
01/19 03:44:34 AM | Train: [10/100] Final Prec@1 85.0640%
01/19 03:44:35 AM | Valid: [10/100] Step 000/078 Loss 0.413 Prec@(1,5) (84.4%, 99.2%)
01/19 03:44:35 AM | Train: [10/100] Step 390/390 Loss 0.218 Prec@(1,5) (92.3%, 99.9%)
01/19 03:44:36 AM | Train: [10/100] Final Prec@1 92.3160%
01/19 03:44:36 AM | Train: [10/100] Step 390/390 Loss 0.425 Prec@(1,5) (85.3%, 99.4%)
01/19 03:44:36 AM | Valid: [10/100] Step 000/078 Loss 0.313 Prec@(1,5) (92.2%, 99.2%)
01/19 03:44:36 AM | Train: [10/100] Final Prec@1 85.2760%
01/19 03:44:36 AM | Train: [11/100] Step 300/390 Loss 0.213 Prec@(1,5) (92.6%, 99.9%)
01/19 03:44:37 AM | Valid: [10/100] Step 078/078 Loss 0.463 Prec@(1,5) (84.4%, 99.3%)
01/19 03:44:37 AM | Valid: [10/100] Final Prec@1 84.4200%
01/19 03:44:37 AM | Valid: [10/100] Step 000/078 Loss 0.425 Prec@(1,5) (85.9%, 100.0%)
01/19 03:44:37 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:44:38 AM | Train: [11/100] Step 000/390 Loss 0.327 Prec@(1,5) (89.8%, 99.2%)
01/19 03:44:38 AM | Valid: [10/100] Step 078/078 Loss 0.361 Prec@(1,5) (88.4%, 99.5%)
01/19 03:44:38 AM | Valid: [10/100] Final Prec@1 88.4100%
01/19 03:44:38 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:44:39 AM | Valid: [10/100] Step 078/078 Loss 0.482 Prec@(1,5) (84.0%, 99.2%)
01/19 03:44:39 AM | Valid: [10/100] Final Prec@1 83.9600%
01/19 03:44:39 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:44:39 AM | Train: [11/100] Step 000/390 Loss 0.167 Prec@(1,5) (92.2%, 100.0%)
01/19 03:44:40 AM | Train: [11/100] Step 000/390 Loss 0.334 Prec@(1,5) (86.7%, 99.2%)
01/19 03:44:42 AM | Train: [11/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.5%, 99.9%)
01/19 03:44:42 AM | Train: [11/100] Final Prec@1 92.4740%
01/19 03:44:43 AM | Valid: [11/100] Step 000/078 Loss 0.256 Prec@(1,5) (89.8%, 99.2%)
01/19 03:44:44 AM | Train: [11/100] Step 100/390 Loss 0.415 Prec@(1,5) (85.3%, 99.5%)
01/19 03:44:44 AM | Valid: [11/100] Step 078/078 Loss 0.366 Prec@(1,5) (88.2%, 99.6%)
01/19 03:44:45 AM | Valid: [11/100] Final Prec@1 88.1900%
01/19 03:44:45 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:44:45 AM | Train: [11/100] Step 100/390 Loss 0.210 Prec@(1,5) (92.6%, 99.9%)
01/19 03:44:45 AM | Train: [12/100] Step 000/390 Loss 0.220 Prec@(1,5) (92.2%, 99.2%)
01/19 03:44:46 AM | Train: [11/100] Step 100/390 Loss 0.420 Prec@(1,5) (85.0%, 99.5%)
01/19 03:44:50 AM | Train: [11/100] Step 200/390 Loss 0.412 Prec@(1,5) (85.4%, 99.5%)
01/19 03:44:51 AM | Train: [11/100] Step 200/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:44:52 AM | Train: [12/100] Step 100/390 Loss 0.215 Prec@(1,5) (92.4%, 99.9%)
01/19 03:44:52 AM | Train: [11/100] Step 200/390 Loss 0.426 Prec@(1,5) (85.0%, 99.4%)
01/19 03:44:57 AM | Train: [11/100] Step 300/390 Loss 0.412 Prec@(1,5) (85.4%, 99.5%)
01/19 03:44:58 AM | Train: [11/100] Step 300/390 Loss 0.208 Prec@(1,5) (92.7%, 99.9%)
01/19 03:44:59 AM | Train: [12/100] Step 200/390 Loss 0.217 Prec@(1,5) (92.4%, 99.9%)
01/19 03:44:59 AM | Train: [11/100] Step 300/390 Loss 0.421 Prec@(1,5) (85.2%, 99.4%)
01/19 03:45:02 AM | Train: [11/100] Step 390/390 Loss 0.416 Prec@(1,5) (85.3%, 99.5%)
01/19 03:45:03 AM | Train: [11/100] Final Prec@1 85.2700%
01/19 03:45:03 AM | Valid: [11/100] Step 000/078 Loss 0.490 Prec@(1,5) (80.5%, 99.2%)
01/19 03:45:04 AM | Train: [11/100] Step 390/390 Loss 0.212 Prec@(1,5) (92.5%, 99.9%)
01/19 03:45:04 AM | Train: [11/100] Final Prec@1 92.4760%
01/19 03:45:05 AM | Train: [11/100] Step 390/390 Loss 0.426 Prec@(1,5) (85.1%, 99.4%)
01/19 03:45:05 AM | Train: [11/100] Final Prec@1 85.1360%
01/19 03:45:05 AM | Valid: [11/100] Step 078/078 Loss 0.509 Prec@(1,5) (83.0%, 99.2%)
01/19 03:45:05 AM | Train: [12/100] Step 300/390 Loss 0.218 Prec@(1,5) (92.4%, 99.9%)
01/19 03:45:05 AM | Valid: [11/100] Step 000/078 Loss 0.295 Prec@(1,5) (88.3%, 98.4%)
01/19 03:45:05 AM | Valid: [11/100] Final Prec@1 83.0400%
01/19 03:45:05 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:45:05 AM | Valid: [11/100] Step 000/078 Loss 0.513 Prec@(1,5) (82.8%, 97.7%)
01/19 03:45:06 AM | Train: [12/100] Step 000/390 Loss 0.530 Prec@(1,5) (81.2%, 100.0%)
01/19 03:45:07 AM | Valid: [11/100] Step 078/078 Loss 0.385 Prec@(1,5) (87.6%, 99.6%)
01/19 03:45:07 AM | Valid: [11/100] Final Prec@1 87.5900%
01/19 03:45:07 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:45:07 AM | Valid: [11/100] Step 078/078 Loss 0.545 Prec@(1,5) (81.7%, 99.0%)
01/19 03:45:07 AM | Valid: [11/100] Final Prec@1 81.6800%
01/19 03:45:07 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:45:08 AM | Train: [12/100] Step 000/390 Loss 0.154 Prec@(1,5) (94.5%, 100.0%)
01/19 03:45:08 AM | Train: [12/100] Step 000/390 Loss 0.541 Prec@(1,5) (83.6%, 98.4%)
01/19 03:45:10 AM | Train: [12/100] Step 390/390 Loss 0.221 Prec@(1,5) (92.3%, 99.9%)
01/19 03:45:10 AM | Train: [12/100] Final Prec@1 92.2740%
01/19 03:45:11 AM | Valid: [12/100] Step 000/078 Loss 0.253 Prec@(1,5) (89.8%, 99.2%)
01/19 03:45:12 AM | Train: [12/100] Step 100/390 Loss 0.426 Prec@(1,5) (85.1%, 99.5%)
01/19 03:45:13 AM | Valid: [12/100] Step 078/078 Loss 0.384 Prec@(1,5) (87.2%, 99.6%)
01/19 03:45:13 AM | Valid: [12/100] Final Prec@1 87.2200%
01/19 03:45:13 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:45:13 AM | Train: [12/100] Step 100/390 Loss 0.206 Prec@(1,5) (92.8%, 99.9%)
01/19 03:45:14 AM | Train: [13/100] Step 000/390 Loss 0.224 Prec@(1,5) (91.4%, 100.0%)
01/19 03:45:14 AM | Train: [12/100] Step 100/390 Loss 0.421 Prec@(1,5) (85.7%, 99.5%)
01/19 03:45:19 AM | Train: [12/100] Step 200/390 Loss 0.426 Prec@(1,5) (85.3%, 99.5%)
01/19 03:45:19 AM | Train: [12/100] Step 200/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:45:20 AM | Train: [13/100] Step 100/390 Loss 0.195 Prec@(1,5) (93.1%, 100.0%)
01/19 03:45:21 AM | Train: [12/100] Step 200/390 Loss 0.420 Prec@(1,5) (85.5%, 99.5%)
01/19 03:45:25 AM | Train: [12/100] Step 300/390 Loss 0.422 Prec@(1,5) (85.3%, 99.5%)
01/19 03:45:26 AM | Train: [12/100] Step 300/390 Loss 0.213 Prec@(1,5) (92.6%, 99.9%)
01/19 03:45:26 AM | Train: [13/100] Step 200/390 Loss 0.210 Prec@(1,5) (92.5%, 99.9%)
01/19 03:45:27 AM | Train: [12/100] Step 300/390 Loss 0.422 Prec@(1,5) (85.3%, 99.4%)
01/19 03:45:31 AM | Train: [12/100] Step 390/390 Loss 0.427 Prec@(1,5) (85.1%, 99.5%)
01/19 03:45:31 AM | Train: [12/100] Final Prec@1 85.1420%
01/19 03:45:31 AM | Train: [12/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.5%, 99.9%)
01/19 03:45:32 AM | Train: [12/100] Final Prec@1 92.4520%
01/19 03:45:32 AM | Valid: [12/100] Step 000/078 Loss 0.465 Prec@(1,5) (82.0%, 97.7%)
01/19 03:45:32 AM | Valid: [12/100] Step 000/078 Loss 0.292 Prec@(1,5) (93.8%, 97.7%)
01/19 03:45:32 AM | Train: [12/100] Step 390/390 Loss 0.426 Prec@(1,5) (85.2%, 99.4%)
01/19 03:45:32 AM | Train: [13/100] Step 300/390 Loss 0.214 Prec@(1,5) (92.4%, 99.9%)
01/19 03:45:33 AM | Train: [12/100] Final Prec@1 85.1660%
01/19 03:45:33 AM | Valid: [12/100] Step 000/078 Loss 0.413 Prec@(1,5) (87.5%, 97.7%)
01/19 03:45:33 AM | Valid: [12/100] Step 078/078 Loss 0.488 Prec@(1,5) (83.3%, 99.2%)
01/19 03:45:34 AM | Valid: [12/100] Final Prec@1 83.2600%
01/19 03:45:34 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:45:34 AM | Valid: [12/100] Step 078/078 Loss 0.402 Prec@(1,5) (87.1%, 99.5%)
01/19 03:45:34 AM | Valid: [12/100] Final Prec@1 87.1100%
01/19 03:45:34 AM | Train: [13/100] Step 000/390 Loss 0.384 Prec@(1,5) (86.7%, 99.2%)
01/19 03:45:34 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:45:35 AM | Valid: [12/100] Step 078/078 Loss 0.476 Prec@(1,5) (83.7%, 99.2%)
01/19 03:45:35 AM | Valid: [12/100] Final Prec@1 83.6700%
01/19 03:45:35 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:45:35 AM | Train: [13/100] Step 000/390 Loss 0.197 Prec@(1,5) (92.2%, 100.0%)
01/19 03:45:36 AM | Train: [13/100] Step 000/390 Loss 0.422 Prec@(1,5) (87.5%, 100.0%)
01/19 03:45:37 AM | Train: [13/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.4%, 99.9%)
01/19 03:45:37 AM | Train: [13/100] Final Prec@1 92.3700%
01/19 03:45:38 AM | Valid: [13/100] Step 000/078 Loss 0.359 Prec@(1,5) (89.8%, 99.2%)
01/19 03:45:40 AM | Valid: [13/100] Step 078/078 Loss 0.420 Prec@(1,5) (86.5%, 99.5%)
01/19 03:45:40 AM | Valid: [13/100] Final Prec@1 86.4600%
01/19 03:45:40 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:45:40 AM | Train: [13/100] Step 100/390 Loss 0.414 Prec@(1,5) (85.5%, 99.5%)
01/19 03:45:41 AM | Train: [13/100] Step 100/390 Loss 0.196 Prec@(1,5) (93.3%, 99.9%)
01/19 03:45:41 AM | Train: [14/100] Step 000/390 Loss 0.201 Prec@(1,5) (93.0%, 100.0%)
01/19 03:45:42 AM | Train: [13/100] Step 100/390 Loss 0.412 Prec@(1,5) (85.6%, 99.5%)
01/19 03:45:47 AM | Train: [13/100] Step 200/390 Loss 0.415 Prec@(1,5) (85.4%, 99.5%)
01/19 03:45:47 AM | Train: [14/100] Step 100/390 Loss 0.205 Prec@(1,5) (92.7%, 99.9%)
01/19 03:45:47 AM | Train: [13/100] Step 200/390 Loss 0.206 Prec@(1,5) (92.8%, 99.9%)
01/19 03:45:48 AM | Train: [13/100] Step 200/390 Loss 0.416 Prec@(1,5) (85.5%, 99.5%)
01/19 03:45:53 AM | Train: [13/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.2%, 99.5%)
01/19 03:45:53 AM | Train: [14/100] Step 200/390 Loss 0.204 Prec@(1,5) (92.7%, 99.9%)
01/19 03:45:54 AM | Train: [13/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.4%, 99.5%)
01/19 03:45:54 AM | Train: [13/100] Step 300/390 Loss 0.212 Prec@(1,5) (92.6%, 99.9%)
01/19 03:45:59 AM | Train: [13/100] Step 390/390 Loss 0.422 Prec@(1,5) (85.2%, 99.5%)
01/19 03:45:59 AM | Train: [13/100] Final Prec@1 85.1680%
01/19 03:46:00 AM | Train: [13/100] Step 390/390 Loss 0.423 Prec@(1,5) (85.2%, 99.5%)
01/19 03:46:00 AM | Train: [13/100] Final Prec@1 85.2100%
01/19 03:46:00 AM | Train: [13/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.5%, 99.9%)
01/19 03:46:00 AM | Train: [14/100] Step 300/390 Loss 0.212 Prec@(1,5) (92.4%, 99.9%)
01/19 03:46:00 AM | Valid: [13/100] Step 000/078 Loss 0.437 Prec@(1,5) (84.4%, 98.4%)
01/19 03:46:00 AM | Train: [13/100] Final Prec@1 92.5100%
01/19 03:46:00 AM | Valid: [13/100] Step 000/078 Loss 0.425 Prec@(1,5) (87.5%, 98.4%)
01/19 03:46:01 AM | Valid: [13/100] Step 000/078 Loss 0.222 Prec@(1,5) (94.5%, 99.2%)
01/19 03:46:02 AM | Valid: [13/100] Step 078/078 Loss 0.489 Prec@(1,5) (83.6%, 99.2%)
01/19 03:46:02 AM | Valid: [13/100] Step 078/078 Loss 0.481 Prec@(1,5) (83.7%, 99.2%)
01/19 03:46:02 AM | Valid: [13/100] Final Prec@1 83.6200%
01/19 03:46:02 AM | Valid: [13/100] Final Prec@1 83.6500%
01/19 03:46:02 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:46:02 AM | Valid: [13/100] Step 078/078 Loss 0.402 Prec@(1,5) (87.4%, 99.5%)
01/19 03:46:02 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:46:03 AM | Valid: [13/100] Final Prec@1 87.3800%
01/19 03:46:03 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:46:03 AM | Train: [14/100] Step 000/390 Loss 0.428 Prec@(1,5) (82.8%, 100.0%)
01/19 03:46:03 AM | Train: [14/100] Step 000/390 Loss 0.431 Prec@(1,5) (86.7%, 99.2%)
01/19 03:46:04 AM | Train: [14/100] Step 000/390 Loss 0.235 Prec@(1,5) (90.6%, 100.0%)
01/19 03:46:05 AM | Train: [14/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.3%, 99.9%)
01/19 03:46:05 AM | Train: [14/100] Final Prec@1 92.3000%
01/19 03:46:06 AM | Valid: [14/100] Step 000/078 Loss 0.272 Prec@(1,5) (91.4%, 100.0%)
01/19 03:46:08 AM | Valid: [14/100] Step 078/078 Loss 0.363 Prec@(1,5) (88.0%, 99.5%)
01/19 03:46:08 AM | Valid: [14/100] Final Prec@1 87.9800%
01/19 03:46:08 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:46:09 AM | Train: [15/100] Step 000/390 Loss 0.285 Prec@(1,5) (90.6%, 99.2%)
01/19 03:46:09 AM | Train: [14/100] Step 100/390 Loss 0.199 Prec@(1,5) (92.9%, 99.9%)
01/19 03:46:10 AM | Train: [14/100] Step 100/390 Loss 0.416 Prec@(1,5) (85.4%, 99.5%)
01/19 03:46:10 AM | Train: [14/100] Step 100/390 Loss 0.409 Prec@(1,5) (85.7%, 99.6%)
01/19 03:46:15 AM | Train: [15/100] Step 100/390 Loss 0.202 Prec@(1,5) (92.9%, 99.9%)
01/19 03:46:16 AM | Train: [14/100] Step 200/390 Loss 0.198 Prec@(1,5) (92.9%, 99.9%)
01/19 03:46:16 AM | Train: [14/100] Step 200/390 Loss 0.414 Prec@(1,5) (85.5%, 99.5%)
01/19 03:46:16 AM | Train: [14/100] Step 200/390 Loss 0.410 Prec@(1,5) (85.7%, 99.5%)
01/19 03:46:22 AM | Train: [15/100] Step 200/390 Loss 0.211 Prec@(1,5) (92.6%, 99.9%)
01/19 03:46:22 AM | Train: [14/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.2%, 99.5%)
01/19 03:46:22 AM | Train: [14/100] Step 300/390 Loss 0.209 Prec@(1,5) (92.6%, 99.9%)
01/19 03:46:23 AM | Train: [14/100] Step 300/390 Loss 0.418 Prec@(1,5) (85.5%, 99.5%)
01/19 03:46:28 AM | Train: [14/100] Step 390/390 Loss 0.424 Prec@(1,5) (85.2%, 99.5%)
01/19 03:46:28 AM | Train: [14/100] Final Prec@1 85.1920%
01/19 03:46:28 AM | Train: [15/100] Step 300/390 Loss 0.214 Prec@(1,5) (92.5%, 99.9%)
01/19 03:46:28 AM | Train: [14/100] Step 390/390 Loss 0.215 Prec@(1,5) (92.4%, 99.9%)
01/19 03:46:28 AM | Train: [14/100] Final Prec@1 92.3900%
01/19 03:46:29 AM | Train: [14/100] Step 390/390 Loss 0.425 Prec@(1,5) (85.3%, 99.5%)
01/19 03:46:29 AM | Valid: [14/100] Step 000/078 Loss 0.445 Prec@(1,5) (83.6%, 99.2%)
01/19 03:46:29 AM | Train: [14/100] Final Prec@1 85.2760%
01/19 03:46:29 AM | Valid: [14/100] Step 000/078 Loss 0.270 Prec@(1,5) (89.8%, 100.0%)
01/19 03:46:29 AM | Valid: [14/100] Step 000/078 Loss 0.471 Prec@(1,5) (83.6%, 99.2%)
01/19 03:46:30 AM | Valid: [14/100] Step 078/078 Loss 0.478 Prec@(1,5) (83.6%, 99.3%)
01/19 03:46:30 AM | Valid: [14/100] Final Prec@1 83.5600%
01/19 03:46:30 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:46:31 AM | Valid: [14/100] Step 078/078 Loss 0.410 Prec@(1,5) (87.0%, 99.5%)
01/19 03:46:31 AM | Valid: [14/100] Final Prec@1 87.0200%
01/19 03:46:31 AM | Train: [15/100] Step 000/390 Loss 0.499 Prec@(1,5) (85.9%, 99.2%)
01/19 03:46:31 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:46:31 AM | Valid: [14/100] Step 078/078 Loss 0.489 Prec@(1,5) (83.3%, 99.2%)
01/19 03:46:31 AM | Valid: [14/100] Final Prec@1 83.2800%
01/19 03:46:32 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:46:32 AM | Train: [15/100] Step 000/390 Loss 0.225 Prec@(1,5) (91.4%, 100.0%)
01/19 03:46:32 AM | Train: [15/100] Step 000/390 Loss 0.524 Prec@(1,5) (82.0%, 99.2%)
01/19 03:46:33 AM | Train: [15/100] Step 390/390 Loss 0.218 Prec@(1,5) (92.3%, 99.9%)
01/19 03:46:34 AM | Train: [15/100] Final Prec@1 92.2900%
01/19 03:46:34 AM | Valid: [15/100] Step 000/078 Loss 0.398 Prec@(1,5) (86.7%, 99.2%)
01/19 03:46:36 AM | Valid: [15/100] Step 078/078 Loss 0.389 Prec@(1,5) (87.6%, 99.5%)
01/19 03:46:36 AM | Valid: [15/100] Final Prec@1 87.6100%
01/19 03:46:36 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:46:37 AM | Train: [16/100] Step 000/390 Loss 0.171 Prec@(1,5) (95.3%, 100.0%)
01/19 03:46:37 AM | Train: [15/100] Step 100/390 Loss 0.400 Prec@(1,5) (86.3%, 99.5%)
01/19 03:46:38 AM | Train: [15/100] Step 100/390 Loss 0.200 Prec@(1,5) (93.0%, 99.9%)
01/19 03:46:39 AM | Train: [15/100] Step 100/390 Loss 0.407 Prec@(1,5) (85.9%, 99.7%)
01/19 03:46:43 AM | Train: [16/100] Step 100/390 Loss 0.211 Prec@(1,5) (92.7%, 99.9%)
01/19 03:46:44 AM | Train: [15/100] Step 200/390 Loss 0.417 Prec@(1,5) (85.5%, 99.5%)
01/19 03:46:44 AM | Train: [15/100] Step 200/390 Loss 0.210 Prec@(1,5) (92.8%, 99.9%)
01/19 03:46:45 AM | Train: [15/100] Step 200/390 Loss 0.421 Prec@(1,5) (85.3%, 99.5%)
01/19 03:46:50 AM | Train: [16/100] Step 200/390 Loss 0.214 Prec@(1,5) (92.6%, 99.9%)
01/19 03:46:50 AM | Train: [15/100] Step 300/390 Loss 0.208 Prec@(1,5) (92.8%, 99.9%)
01/19 03:46:51 AM | Train: [15/100] Step 300/390 Loss 0.421 Prec@(1,5) (85.4%, 99.4%)
01/19 03:46:52 AM | Train: [15/100] Step 300/390 Loss 0.421 Prec@(1,5) (85.3%, 99.5%)
01/19 03:46:56 AM | Train: [16/100] Step 300/390 Loss 0.216 Prec@(1,5) (92.5%, 99.9%)
01/19 03:46:56 AM | Train: [15/100] Step 390/390 Loss 0.428 Prec@(1,5) (85.1%, 99.4%)
01/19 03:46:56 AM | Train: [15/100] Final Prec@1 85.1340%
01/19 03:46:56 AM | Train: [15/100] Step 390/390 Loss 0.212 Prec@(1,5) (92.7%, 99.9%)
01/19 03:46:57 AM | Train: [15/100] Final Prec@1 92.6500%
01/19 03:46:57 AM | Train: [15/100] Step 390/390 Loss 0.423 Prec@(1,5) (85.2%, 99.5%)
01/19 03:46:57 AM | Train: [15/100] Final Prec@1 85.2320%
01/19 03:46:57 AM | Valid: [15/100] Step 000/078 Loss 0.624 Prec@(1,5) (80.5%, 98.4%)
01/19 03:46:57 AM | Valid: [15/100] Step 000/078 Loss 0.350 Prec@(1,5) (89.8%, 99.2%)
01/19 03:46:58 AM | Valid: [15/100] Step 000/078 Loss 0.499 Prec@(1,5) (82.8%, 99.2%)
01/19 03:46:59 AM | Valid: [15/100] Step 078/078 Loss 0.535 Prec@(1,5) (81.9%, 99.1%)
01/19 03:46:59 AM | Valid: [15/100] Final Prec@1 81.9400%
01/19 03:46:59 AM | Valid: [15/100] Step 078/078 Loss 0.381 Prec@(1,5) (87.4%, 99.5%)
01/19 03:46:59 AM | Valid: [15/100] Step 078/078 Loss 0.513 Prec@(1,5) (82.7%, 99.2%)
01/19 03:46:59 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:46:59 AM | Valid: [15/100] Final Prec@1 87.4200%
01/19 03:46:59 AM | Valid: [15/100] Final Prec@1 82.7400%
01/19 03:46:59 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:47:00 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:47:00 AM | Train: [16/100] Step 000/390 Loss 0.338 Prec@(1,5) (89.1%, 100.0%)
01/19 03:47:00 AM | Train: [16/100] Step 000/390 Loss 0.277 Prec@(1,5) (91.4%, 100.0%)
01/19 03:47:00 AM | Train: [16/100] Step 000/390 Loss 0.217 Prec@(1,5) (93.0%, 100.0%)
01/19 03:47:01 AM | Train: [16/100] Step 390/390 Loss 0.220 Prec@(1,5) (92.3%, 99.9%)
01/19 03:47:01 AM | Train: [16/100] Final Prec@1 92.3160%
01/19 03:47:02 AM | Valid: [16/100] Step 000/078 Loss 0.282 Prec@(1,5) (89.1%, 100.0%)
01/19 03:47:04 AM | Valid: [16/100] Step 078/078 Loss 0.380 Prec@(1,5) (87.6%, 99.4%)
01/19 03:47:04 AM | Valid: [16/100] Final Prec@1 87.6000%
01/19 03:47:04 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:47:05 AM | Train: [17/100] Step 000/390 Loss 0.247 Prec@(1,5) (93.8%, 99.2%)
01/19 03:47:06 AM | Train: [16/100] Step 100/390 Loss 0.204 Prec@(1,5) (93.0%, 99.9%)
01/19 03:47:06 AM | Train: [16/100] Step 100/390 Loss 0.422 Prec@(1,5) (85.3%, 99.6%)
01/19 03:47:06 AM | Train: [16/100] Step 100/390 Loss 0.413 Prec@(1,5) (85.4%, 99.5%)
01/19 03:47:10 AM | Train: [17/100] Step 100/390 Loss 0.194 Prec@(1,5) (93.3%, 99.9%)
01/19 03:47:12 AM | Train: [16/100] Step 200/390 Loss 0.419 Prec@(1,5) (85.4%, 99.5%)
01/19 03:47:13 AM | Train: [16/100] Step 200/390 Loss 0.206 Prec@(1,5) (92.9%, 99.9%)
01/19 03:47:13 AM | Train: [16/100] Step 200/390 Loss 0.416 Prec@(1,5) (85.4%, 99.5%)
01/19 03:47:17 AM | Train: [17/100] Step 200/390 Loss 0.202 Prec@(1,5) (92.9%, 99.9%)
01/19 03:47:18 AM | Train: [16/100] Step 300/390 Loss 0.426 Prec@(1,5) (85.1%, 99.4%)
01/19 03:47:20 AM | Train: [16/100] Step 300/390 Loss 0.207 Prec@(1,5) (92.8%, 99.9%)
01/19 03:47:20 AM | Train: [16/100] Step 300/390 Loss 0.426 Prec@(1,5) (85.1%, 99.4%)
01/19 03:47:23 AM | Train: [17/100] Step 300/390 Loss 0.211 Prec@(1,5) (92.6%, 99.9%)
01/19 03:47:24 AM | Train: [16/100] Step 390/390 Loss 0.430 Prec@(1,5) (85.0%, 99.4%)
01/19 03:47:24 AM | Train: [16/100] Final Prec@1 85.0440%
01/19 03:47:25 AM | Valid: [16/100] Step 000/078 Loss 0.421 Prec@(1,5) (83.6%, 99.2%)
01/19 03:47:25 AM | Train: [16/100] Step 390/390 Loss 0.214 Prec@(1,5) (92.6%, 99.9%)
01/19 03:47:26 AM | Train: [16/100] Final Prec@1 92.5800%
01/19 03:47:26 AM | Train: [16/100] Step 390/390 Loss 0.429 Prec@(1,5) (85.0%, 99.5%)
01/19 03:47:26 AM | Train: [16/100] Final Prec@1 84.9700%
01/19 03:47:26 AM | Valid: [16/100] Step 078/078 Loss 0.492 Prec@(1,5) (83.1%, 99.2%)
01/19 03:47:26 AM | Valid: [16/100] Final Prec@1 83.1200%
01/19 03:47:26 AM | Valid: [16/100] Step 000/078 Loss 0.330 Prec@(1,5) (91.4%, 100.0%)
01/19 03:47:26 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:47:26 AM | Valid: [16/100] Step 000/078 Loss 0.463 Prec@(1,5) (84.4%, 100.0%)
01/19 03:47:27 AM | Train: [17/100] Step 000/390 Loss 0.399 Prec@(1,5) (85.2%, 100.0%)
01/19 03:47:28 AM | Valid: [16/100] Step 078/078 Loss 0.362 Prec@(1,5) (88.5%, 99.6%)
01/19 03:47:28 AM | Valid: [16/100] Final Prec@1 88.5200%
01/19 03:47:28 AM | Valid: [16/100] Step 078/078 Loss 0.478 Prec@(1,5) (83.9%, 99.3%)
01/19 03:47:28 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:47:28 AM | Valid: [16/100] Final Prec@1 83.9200%
01/19 03:47:28 AM | Train: [17/100] Step 390/390 Loss 0.213 Prec@(1,5) (92.5%, 99.9%)
01/19 03:47:28 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:47:29 AM | Train: [17/100] Final Prec@1 92.5220%
01/19 03:47:29 AM | Train: [17/100] Step 000/390 Loss 0.201 Prec@(1,5) (94.5%, 100.0%)
01/19 03:47:29 AM | Valid: [17/100] Step 000/078 Loss 0.349 Prec@(1,5) (89.8%, 98.4%)
01/19 03:47:29 AM | Train: [17/100] Step 000/390 Loss 0.390 Prec@(1,5) (83.6%, 100.0%)
01/19 03:47:31 AM | Valid: [17/100] Step 078/078 Loss 0.350 Prec@(1,5) (88.8%, 99.6%)
01/19 03:47:31 AM | Valid: [17/100] Final Prec@1 88.8100%
01/19 03:47:31 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:47:32 AM | Train: [18/100] Step 000/390 Loss 0.119 Prec@(1,5) (96.9%, 100.0%)
01/19 03:47:33 AM | Train: [17/100] Step 100/390 Loss 0.425 Prec@(1,5) (85.2%, 99.5%)
01/19 03:47:35 AM | Train: [17/100] Step 100/390 Loss 0.199 Prec@(1,5) (93.0%, 100.0%)
01/19 03:47:36 AM | Train: [17/100] Step 100/390 Loss 0.428 Prec@(1,5) (84.8%, 99.5%)
01/19 03:47:38 AM | Train: [18/100] Step 100/390 Loss 0.200 Prec@(1,5) (93.0%, 100.0%)
01/19 03:47:39 AM | Train: [17/100] Step 200/390 Loss 0.420 Prec@(1,5) (85.3%, 99.5%)
01/19 03:47:41 AM | Train: [17/100] Step 200/390 Loss 0.199 Prec@(1,5) (93.1%, 100.0%)
01/19 03:47:43 AM | Train: [17/100] Step 200/390 Loss 0.422 Prec@(1,5) (85.2%, 99.5%)
01/19 03:47:45 AM | Train: [18/100] Step 200/390 Loss 0.208 Prec@(1,5) (92.6%, 99.9%)
01/19 03:47:46 AM | Train: [17/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.3%, 99.4%)
01/19 03:47:48 AM | Train: [17/100] Step 300/390 Loss 0.205 Prec@(1,5) (92.9%, 99.9%)
01/19 03:47:49 AM | Train: [17/100] Step 300/390 Loss 0.423 Prec@(1,5) (85.2%, 99.5%)
01/19 03:47:51 AM | Train: [18/100] Step 300/390 Loss 0.213 Prec@(1,5) (92.5%, 99.9%)
01/19 03:47:52 AM | Train: [17/100] Step 390/390 Loss 0.423 Prec@(1,5) (85.1%, 99.4%)
01/19 03:47:52 AM | Train: [17/100] Final Prec@1 85.1300%
01/19 03:47:52 AM | Valid: [17/100] Step 000/078 Loss 0.583 Prec@(1,5) (79.7%, 98.4%)
01/19 03:47:53 AM | Train: [17/100] Step 390/390 Loss 0.209 Prec@(1,5) (92.7%, 99.9%)
01/19 03:47:54 AM | Train: [17/100] Final Prec@1 92.7260%
01/19 03:47:54 AM | Valid: [17/100] Step 078/078 Loss 0.492 Prec@(1,5) (83.5%, 99.3%)
01/19 03:47:54 AM | Valid: [17/100] Final Prec@1 83.5200%
01/19 03:47:54 AM | Valid: [17/100] Step 000/078 Loss 0.416 Prec@(1,5) (85.9%, 99.2%)
01/19 03:47:54 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:47:55 AM | Train: [17/100] Step 390/390 Loss 0.423 Prec@(1,5) (85.3%, 99.5%)
01/19 03:47:55 AM | Train: [17/100] Final Prec@1 85.2980%
01/19 03:47:55 AM | Train: [18/100] Step 000/390 Loss 0.311 Prec@(1,5) (89.8%, 100.0%)
01/19 03:47:56 AM | Valid: [17/100] Step 000/078 Loss 0.579 Prec@(1,5) (82.0%, 98.4%)
01/19 03:47:56 AM | Valid: [17/100] Step 078/078 Loss 0.371 Prec@(1,5) (88.3%, 99.5%)
01/19 03:47:56 AM | Valid: [17/100] Final Prec@1 88.2600%
01/19 03:47:56 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:47:57 AM | Train: [18/100] Step 390/390 Loss 0.217 Prec@(1,5) (92.4%, 99.9%)
01/19 03:47:57 AM | Train: [18/100] Step 000/390 Loss 0.117 Prec@(1,5) (96.9%, 100.0%)
01/19 03:47:57 AM | Train: [18/100] Final Prec@1 92.3760%
01/19 03:47:57 AM | Valid: [17/100] Step 078/078 Loss 0.496 Prec@(1,5) (83.7%, 99.2%)
01/19 03:47:58 AM | Valid: [17/100] Final Prec@1 83.6900%
01/19 03:47:58 AM | Valid: [18/100] Step 000/078 Loss 0.348 Prec@(1,5) (89.8%, 99.2%)
01/19 03:47:58 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:47:59 AM | Train: [18/100] Step 000/390 Loss 0.284 Prec@(1,5) (89.8%, 100.0%)
01/19 03:48:00 AM | Valid: [18/100] Step 078/078 Loss 0.371 Prec@(1,5) (88.4%, 99.5%)
01/19 03:48:00 AM | Valid: [18/100] Final Prec@1 88.4000%
01/19 03:48:00 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:48:01 AM | Train: [19/100] Step 000/390 Loss 0.221 Prec@(1,5) (93.0%, 100.0%)
01/19 03:48:01 AM | Train: [18/100] Step 100/390 Loss 0.410 Prec@(1,5) (85.8%, 99.5%)
01/19 03:48:03 AM | Train: [18/100] Step 100/390 Loss 0.194 Prec@(1,5) (93.3%, 100.0%)
01/19 03:48:05 AM | Train: [18/100] Step 100/390 Loss 0.405 Prec@(1,5) (85.6%, 99.5%)
01/19 03:48:07 AM | Train: [19/100] Step 100/390 Loss 0.201 Prec@(1,5) (93.0%, 99.9%)
01/19 03:48:08 AM | Train: [18/100] Step 200/390 Loss 0.410 Prec@(1,5) (85.6%, 99.5%)
01/19 03:48:10 AM | Train: [18/100] Step 200/390 Loss 0.203 Prec@(1,5) (92.9%, 99.9%)
01/19 03:48:11 AM | Train: [18/100] Step 200/390 Loss 0.415 Prec@(1,5) (85.3%, 99.5%)
01/19 03:48:13 AM | Train: [19/100] Step 200/390 Loss 0.210 Prec@(1,5) (92.6%, 99.9%)
01/19 03:48:14 AM | Train: [18/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.3%, 99.4%)
01/19 03:48:16 AM | Train: [18/100] Step 300/390 Loss 0.208 Prec@(1,5) (92.8%, 99.9%)
01/19 03:48:18 AM | Train: [18/100] Step 300/390 Loss 0.424 Prec@(1,5) (85.1%, 99.4%)
01/19 03:48:20 AM | Train: [19/100] Step 300/390 Loss 0.212 Prec@(1,5) (92.5%, 99.9%)
01/19 03:48:20 AM | Train: [18/100] Step 390/390 Loss 0.424 Prec@(1,5) (85.2%, 99.4%)
01/19 03:48:20 AM | Train: [18/100] Final Prec@1 85.1560%
01/19 03:48:21 AM | Valid: [18/100] Step 000/078 Loss 0.523 Prec@(1,5) (82.8%, 98.4%)
01/19 03:48:22 AM | Train: [18/100] Step 390/390 Loss 0.213 Prec@(1,5) (92.6%, 99.9%)
01/19 03:48:22 AM | Train: [18/100] Final Prec@1 92.5860%
01/19 03:48:23 AM | Valid: [18/100] Step 078/078 Loss 0.498 Prec@(1,5) (83.5%, 99.2%)
01/19 03:48:23 AM | Valid: [18/100] Final Prec@1 83.5100%
01/19 03:48:23 AM | Valid: [18/100] Step 000/078 Loss 0.368 Prec@(1,5) (89.1%, 97.7%)
01/19 03:48:23 AM | Train: [18/100] Step 390/390 Loss 0.425 Prec@(1,5) (85.1%, 99.4%)
01/19 03:48:23 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:48:23 AM | Train: [18/100] Final Prec@1 85.1020%
01/19 03:48:24 AM | Train: [19/100] Step 000/390 Loss 0.494 Prec@(1,5) (84.4%, 100.0%)
01/19 03:48:24 AM | Valid: [18/100] Step 000/078 Loss 0.511 Prec@(1,5) (85.2%, 98.4%)
01/19 03:48:25 AM | Valid: [18/100] Step 078/078 Loss 0.390 Prec@(1,5) (87.5%, 99.6%)
01/19 03:48:25 AM | Train: [19/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.3%, 99.9%)
01/19 03:48:25 AM | Train: [19/100] Final Prec@1 92.3260%
01/19 03:48:25 AM | Valid: [18/100] Final Prec@1 87.4500%
01/19 03:48:25 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:48:25 AM | Valid: [19/100] Step 000/078 Loss 0.272 Prec@(1,5) (90.6%, 100.0%)
01/19 03:48:26 AM | Valid: [18/100] Step 078/078 Loss 0.524 Prec@(1,5) (82.3%, 99.2%)
01/19 03:48:26 AM | Valid: [18/100] Final Prec@1 82.3400%
01/19 03:48:26 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
01/19 03:48:26 AM | Train: [19/100] Step 000/390 Loss 0.228 Prec@(1,5) (92.2%, 100.0%)
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:48:27 AM | Train: [19/100] Step 000/390 Loss 0.550 Prec@(1,5) (78.1%, 98.4%)
01/19 03:48:27 AM | Valid: [19/100] Step 078/078 Loss 0.375 Prec@(1,5) (88.0%, 99.5%)
01/19 03:48:27 AM | Valid: [19/100] Final Prec@1 87.9700%
01/19 03:48:28 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:48:28 AM | Train: [20/100] Step 000/390 Loss 0.174 Prec@(1,5) (93.0%, 100.0%)
01/19 03:48:30 AM | Train: [19/100] Step 100/390 Loss 0.401 Prec@(1,5) (86.1%, 99.5%)
01/19 03:48:32 AM | Train: [19/100] Step 100/390 Loss 0.190 Prec@(1,5) (93.4%, 100.0%)
01/19 03:48:33 AM | Train: [19/100] Step 100/390 Loss 0.410 Prec@(1,5) (85.9%, 99.5%)
01/19 03:48:35 AM | Train: [20/100] Step 100/390 Loss 0.201 Prec@(1,5) (93.0%, 99.9%)
01/19 03:48:36 AM | Train: [19/100] Step 200/390 Loss 0.411 Prec@(1,5) (85.7%, 99.5%)
01/19 03:48:38 AM | Train: [19/100] Step 200/390 Loss 0.198 Prec@(1,5) (93.0%, 99.9%)
01/19 03:48:39 AM | Train: [19/100] Step 200/390 Loss 0.418 Prec@(1,5) (85.4%, 99.5%)
01/19 03:48:41 AM | Train: [20/100] Step 200/390 Loss 0.206 Prec@(1,5) (92.8%, 99.9%)
01/19 03:48:43 AM | Train: [19/100] Step 300/390 Loss 0.413 Prec@(1,5) (85.5%, 99.5%)
01/19 03:48:45 AM | Train: [19/100] Step 300/390 Loss 0.202 Prec@(1,5) (92.9%, 99.9%)
01/19 03:48:46 AM | Train: [19/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.3%, 99.5%)
01/19 03:48:47 AM | Train: [20/100] Step 300/390 Loss 0.207 Prec@(1,5) (92.8%, 99.9%)
01/19 03:48:48 AM | Train: [19/100] Step 390/390 Loss 0.417 Prec@(1,5) (85.3%, 99.5%)
01/19 03:48:48 AM | Train: [19/100] Final Prec@1 85.2860%
01/19 03:48:49 AM | Valid: [19/100] Step 000/078 Loss 0.527 Prec@(1,5) (81.2%, 99.2%)
01/19 03:48:51 AM | Valid: [19/100] Step 078/078 Loss 0.480 Prec@(1,5) (83.7%, 99.1%)
01/19 03:48:51 AM | Train: [19/100] Step 390/390 Loss 0.210 Prec@(1,5) (92.6%, 99.9%)
01/19 03:48:51 AM | Valid: [19/100] Final Prec@1 83.6600%
01/19 03:48:51 AM | Train: [19/100] Final Prec@1 92.6140%
01/19 03:48:51 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:48:51 AM | Train: [19/100] Step 390/390 Loss 0.425 Prec@(1,5) (85.2%, 99.4%)
01/19 03:48:51 AM | Train: [19/100] Final Prec@1 85.1560%
01/19 03:48:51 AM | Valid: [19/100] Step 000/078 Loss 0.284 Prec@(1,5) (89.1%, 99.2%)
01/19 03:48:52 AM | Valid: [19/100] Step 000/078 Loss 0.499 Prec@(1,5) (83.6%, 98.4%)
01/19 03:48:52 AM | Train: [20/100] Step 000/390 Loss 0.336 Prec@(1,5) (89.1%, 99.2%)
01/19 03:48:53 AM | Train: [20/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.5%, 99.9%)
01/19 03:48:53 AM | Valid: [19/100] Step 078/078 Loss 0.382 Prec@(1,5) (88.0%, 99.6%)
01/19 03:48:53 AM | Train: [20/100] Final Prec@1 92.4520%
01/19 03:48:53 AM | Valid: [19/100] Final Prec@1 87.9600%
01/19 03:48:53 AM | Valid: [19/100] Step 078/078 Loss 0.476 Prec@(1,5) (83.7%, 99.2%)
01/19 03:48:53 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:48:54 AM | Valid: [19/100] Final Prec@1 83.7100%
01/19 03:48:54 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:48:54 AM | Valid: [20/100] Step 000/078 Loss 0.382 Prec@(1,5) (82.8%, 100.0%)
01/19 03:48:54 AM | Train: [20/100] Step 000/390 Loss 0.174 Prec@(1,5) (95.3%, 100.0%)
01/19 03:48:54 AM | Train: [20/100] Step 000/390 Loss 0.274 Prec@(1,5) (90.6%, 100.0%)
01/19 03:48:55 AM | Valid: [20/100] Step 078/078 Loss 0.375 Prec@(1,5) (88.0%, 99.6%)
01/19 03:48:55 AM | Valid: [20/100] Final Prec@1 87.9600%
01/19 03:48:56 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:48:56 AM | Train: [21/100] Step 000/390 Loss 0.164 Prec@(1,5) (95.3%, 100.0%)
01/19 03:48:57 AM | Train: [20/100] Step 100/390 Loss 0.404 Prec@(1,5) (86.0%, 99.6%)
01/19 03:49:00 AM | Train: [20/100] Step 100/390 Loss 0.196 Prec@(1,5) (93.2%, 99.9%)
01/19 03:49:01 AM | Train: [20/100] Step 100/390 Loss 0.401 Prec@(1,5) (86.0%, 99.6%)
01/19 03:49:03 AM | Train: [21/100] Step 100/390 Loss 0.215 Prec@(1,5) (92.5%, 99.9%)
01/19 03:49:04 AM | Train: [20/100] Step 200/390 Loss 0.413 Prec@(1,5) (85.7%, 99.5%)
01/19 03:49:07 AM | Train: [20/100] Step 200/390 Loss 0.195 Prec@(1,5) (93.2%, 99.9%)
01/19 03:49:07 AM | Train: [20/100] Step 200/390 Loss 0.415 Prec@(1,5) (85.5%, 99.5%)
01/19 03:49:09 AM | Train: [21/100] Step 200/390 Loss 0.208 Prec@(1,5) (92.7%, 99.9%)
01/19 03:49:10 AM | Train: [20/100] Step 300/390 Loss 0.409 Prec@(1,5) (85.9%, 99.5%)
01/19 03:49:13 AM | Train: [20/100] Step 300/390 Loss 0.198 Prec@(1,5) (93.1%, 99.9%)
01/19 03:49:14 AM | Train: [20/100] Step 300/390 Loss 0.415 Prec@(1,5) (85.5%, 99.5%)
01/19 03:49:16 AM | Train: [21/100] Step 300/390 Loss 0.214 Prec@(1,5) (92.5%, 99.9%)
01/19 03:49:16 AM | Train: [20/100] Step 390/390 Loss 0.416 Prec@(1,5) (85.6%, 99.5%)
01/19 03:49:16 AM | Train: [20/100] Final Prec@1 85.6160%
01/19 03:49:17 AM | Valid: [20/100] Step 000/078 Loss 0.497 Prec@(1,5) (80.5%, 96.9%)
01/19 03:49:18 AM | Valid: [20/100] Step 078/078 Loss 0.526 Prec@(1,5) (82.5%, 99.1%)
01/19 03:49:19 AM | Valid: [20/100] Final Prec@1 82.5100%
01/19 03:49:19 AM | Current best Prec@1 = 84.4300%
01/19 03:49:19 AM | Train: [20/100] Step 390/390 Loss 0.422 Prec@(1,5) (85.3%, 99.5%)
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:49:19 AM | Train: [20/100] Step 390/390 Loss 0.209 Prec@(1,5) (92.7%, 99.9%)
01/19 03:49:19 AM | Train: [20/100] Final Prec@1 85.2880%
01/19 03:49:19 AM | Train: [20/100] Final Prec@1 92.6880%
01/19 03:49:19 AM | Train: [21/100] Step 000/390 Loss 0.439 Prec@(1,5) (82.0%, 100.0%)
01/19 03:49:20 AM | Valid: [20/100] Step 000/078 Loss 0.445 Prec@(1,5) (82.8%, 98.4%)
01/19 03:49:20 AM | Valid: [20/100] Step 000/078 Loss 0.408 Prec@(1,5) (89.1%, 100.0%)
01/19 03:49:21 AM | Train: [21/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.4%, 99.9%)
01/19 03:49:21 AM | Train: [21/100] Final Prec@1 92.4340%
01/19 03:49:21 AM | Valid: [20/100] Step 078/078 Loss 0.532 Prec@(1,5) (82.2%, 99.2%)
01/19 03:49:21 AM | Valid: [20/100] Step 078/078 Loss 0.449 Prec@(1,5) (86.3%, 99.4%)
01/19 03:49:22 AM | Valid: [20/100] Final Prec@1 82.2000%
01/19 03:49:22 AM | Valid: [20/100] Final Prec@1 86.3000%
01/19 03:49:22 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:49:22 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:49:22 AM | Valid: [21/100] Step 000/078 Loss 0.349 Prec@(1,5) (88.3%, 98.4%)
01/19 03:49:22 AM | Train: [21/100] Step 000/390 Loss 0.359 Prec@(1,5) (89.8%, 100.0%)
01/19 03:49:23 AM | Train: [21/100] Step 000/390 Loss 0.232 Prec@(1,5) (93.8%, 100.0%)
01/19 03:49:24 AM | Valid: [21/100] Step 078/078 Loss 0.390 Prec@(1,5) (87.3%, 99.5%)
01/19 03:49:24 AM | Valid: [21/100] Final Prec@1 87.3100%
01/19 03:49:24 AM | Current best Prec@1 = 88.9500%
01/19 03:49:24 AM | Perform validation on training dataset. 
01/19 03:49:25 AM | Valid on training dataset: [21/100] Step 000/390 Loss 0.242 Prec@(1,5) (93.0%, 100.0%)
01/19 03:49:25 AM | Train: [21/100] Step 100/390 Loss 0.410 Prec@(1,5) (86.1%, 99.5%)
01/19 03:49:27 AM | Valid on training dataset: [21/100] Step 100/390 Loss 0.209 Prec@(1,5) (92.6%, 99.9%)
01/19 03:49:29 AM | Train: [21/100] Step 100/390 Loss 0.201 Prec@(1,5) (93.2%, 99.9%)
01/19 03:49:29 AM | Train: [21/100] Step 100/390 Loss 0.408 Prec@(1,5) (85.9%, 99.5%)
01/19 03:49:30 AM | Valid on training dataset: [21/100] Step 200/390 Loss 0.212 Prec@(1,5) (92.6%, 99.9%)
01/19 03:49:32 AM | Train: [21/100] Step 200/390 Loss 0.408 Prec@(1,5) (86.1%, 99.5%)
01/19 03:49:33 AM | Valid on training dataset: [21/100] Step 300/390 Loss 0.211 Prec@(1,5) (92.6%, 99.9%)
01/19 03:49:35 AM | Train: [21/100] Step 200/390 Loss 0.201 Prec@(1,5) (93.2%, 99.9%)
01/19 03:49:35 AM | Valid on training dataset: [21/100] Step 390/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:49:35 AM | Train: [21/100] Step 200/390 Loss 0.405 Prec@(1,5) (85.9%, 99.5%)
01/19 03:49:35 AM | Valid on training dataset: [21/100] Final Prec@1 92.6660%
01/19 03:49:35 AM | Final train Prec@1 = 92.6660%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:49:36 AM | Train: [22/100] Step 000/390 Loss 0.242 Prec@(1,5) (92.2%, 100.0%)
01/19 03:49:38 AM | Train: [21/100] Step 300/390 Loss 0.415 Prec@(1,5) (85.7%, 99.5%)
01/19 03:49:41 AM | Train: [21/100] Step 300/390 Loss 0.204 Prec@(1,5) (93.0%, 99.9%)
01/19 03:49:41 AM | Train: [21/100] Step 300/390 Loss 0.412 Prec@(1,5) (85.7%, 99.5%)
01/19 03:49:43 AM | Train: [22/100] Step 100/390 Loss 0.204 Prec@(1,5) (92.9%, 99.9%)
01/19 03:49:44 AM | Train: [21/100] Step 390/390 Loss 0.421 Prec@(1,5) (85.6%, 99.5%)
01/19 03:49:44 AM | Train: [21/100] Final Prec@1 85.5540%
01/19 03:49:45 AM | Valid: [21/100] Step 000/078 Loss 0.439 Prec@(1,5) (85.9%, 100.0%)
01/19 03:49:46 AM | Train: [21/100] Step 390/390 Loss 0.207 Prec@(1,5) (92.8%, 99.9%)
01/19 03:49:47 AM | Train: [21/100] Step 390/390 Loss 0.419 Prec@(1,5) (85.3%, 99.4%)
01/19 03:49:47 AM | Train: [21/100] Final Prec@1 92.8320%
01/19 03:49:47 AM | Train: [21/100] Final Prec@1 85.3380%
01/19 03:49:47 AM | Valid: [21/100] Step 078/078 Loss 0.506 Prec@(1,5) (82.5%, 99.2%)
01/19 03:49:47 AM | Valid: [21/100] Final Prec@1 82.5500%
01/19 03:49:47 AM | Valid: [21/100] Step 000/078 Loss 0.413 Prec@(1,5) (85.2%, 99.2%)
01/19 03:49:47 AM | Valid: [21/100] Step 000/078 Loss 0.401 Prec@(1,5) (86.7%, 98.4%)
01/19 03:49:47 AM | Current best Prec@1 = 84.4300%
01/19 03:49:47 AM | Perform validation on training dataset. 
01/19 03:49:48 AM | Valid on training dataset: [21/100] Step 000/390 Loss 0.274 Prec@(1,5) (91.4%, 100.0%)
01/19 03:49:48 AM | Valid: [21/100] Step 078/078 Loss 0.362 Prec@(1,5) (88.2%, 99.5%)
01/19 03:49:48 AM | Valid: [21/100] Step 078/078 Loss 0.497 Prec@(1,5) (83.1%, 99.4%)
01/19 03:49:48 AM | Valid: [21/100] Final Prec@1 88.1900%
01/19 03:49:48 AM | Train: [22/100] Step 200/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:49:48 AM | Valid: [21/100] Final Prec@1 83.1200%
01/19 03:49:49 AM | Current best Prec@1 = 88.9200%
01/19 03:49:49 AM | Perform validation on training dataset. 
01/19 03:49:49 AM | Current best Prec@1 = 84.2100%
01/19 03:49:49 AM | Perform validation on training dataset. 
01/19 03:49:49 AM | Valid on training dataset: [21/100] Step 000/390 Loss 0.153 Prec@(1,5) (95.3%, 100.0%)
01/19 03:49:49 AM | Valid on training dataset: [21/100] Step 000/390 Loss 0.350 Prec@(1,5) (85.2%, 99.2%)
01/19 03:49:50 AM | Valid on training dataset: [21/100] Step 100/390 Loss 0.401 Prec@(1,5) (86.1%, 99.6%)
01/19 03:49:51 AM | Valid on training dataset: [21/100] Step 100/390 Loss 0.191 Prec@(1,5) (93.4%, 99.9%)
01/19 03:49:52 AM | Valid on training dataset: [21/100] Step 100/390 Loss 0.389 Prec@(1,5) (86.6%, 99.5%)
01/19 03:49:52 AM | Valid on training dataset: [21/100] Step 200/390 Loss 0.409 Prec@(1,5) (85.8%, 99.5%)
01/19 03:49:54 AM | Valid on training dataset: [21/100] Step 200/390 Loss 0.396 Prec@(1,5) (86.3%, 99.5%)
01/19 03:49:54 AM | Valid on training dataset: [21/100] Step 200/390 Loss 0.191 Prec@(1,5) (93.3%, 99.9%)
01/19 03:49:54 AM | Valid on training dataset: [21/100] Step 300/390 Loss 0.409 Prec@(1,5) (85.8%, 99.5%)
01/19 03:49:54 AM | Train: [22/100] Step 300/390 Loss 0.217 Prec@(1,5) (92.4%, 99.9%)
01/19 03:49:56 AM | Valid on training dataset: [21/100] Step 300/390 Loss 0.400 Prec@(1,5) (86.1%, 99.6%)
01/19 03:49:56 AM | Valid on training dataset: [21/100] Step 300/390 Loss 0.191 Prec@(1,5) (93.3%, 99.9%)
01/19 03:49:56 AM | Valid on training dataset: [21/100] Step 390/390 Loss 0.412 Prec@(1,5) (85.6%, 99.5%)
01/19 03:49:57 AM | Valid on training dataset: [21/100] Final Prec@1 85.6240%
01/19 03:49:57 AM | Final train Prec@1 = 85.6240%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:49:57 AM | Train: [22/100] Step 000/390 Loss 0.466 Prec@(1,5) (82.0%, 100.0%)
01/19 03:49:58 AM | Valid on training dataset: [21/100] Step 390/390 Loss 0.192 Prec@(1,5) (93.3%, 99.9%)
01/19 03:49:58 AM | Valid on training dataset: [21/100] Step 390/390 Loss 0.402 Prec@(1,5) (85.9%, 99.6%)
01/19 03:49:58 AM | Valid on training dataset: [21/100] Final Prec@1 85.9400%
01/19 03:49:58 AM | Final train Prec@1 = 85.9400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:49:58 AM | Valid on training dataset: [21/100] Final Prec@1 93.2780%
01/19 03:49:58 AM | Final train Prec@1 = 93.2780%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:49:59 AM | Train: [22/100] Step 000/390 Loss 0.510 Prec@(1,5) (79.7%, 99.2%)
01/19 03:49:59 AM | Train: [22/100] Step 000/390 Loss 0.332 Prec@(1,5) (88.3%, 100.0%)
01/19 03:50:00 AM | Train: [22/100] Step 390/390 Loss 0.218 Prec@(1,5) (92.4%, 99.9%)
01/19 03:50:00 AM | Train: [22/100] Final Prec@1 92.3500%
01/19 03:50:00 AM | Valid: [22/100] Step 000/078 Loss 0.322 Prec@(1,5) (90.6%, 100.0%)
01/19 03:50:02 AM | Valid: [22/100] Step 078/078 Loss 0.380 Prec@(1,5) (88.0%, 99.5%)
01/19 03:50:02 AM | Valid: [22/100] Final Prec@1 88.0100%
01/19 03:50:02 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:50:03 AM | Train: [23/100] Step 000/390 Loss 0.299 Prec@(1,5) (89.8%, 100.0%)
01/19 03:50:03 AM | Train: [22/100] Step 100/390 Loss 0.394 Prec@(1,5) (86.3%, 99.5%)
01/19 03:50:05 AM | Train: [22/100] Step 100/390 Loss 0.199 Prec@(1,5) (93.2%, 99.9%)
01/19 03:50:05 AM | Train: [22/100] Step 100/390 Loss 0.406 Prec@(1,5) (86.0%, 99.4%)
01/19 03:50:09 AM | Train: [23/100] Step 100/390 Loss 0.198 Prec@(1,5) (92.8%, 99.9%)
01/19 03:50:09 AM | Train: [22/100] Step 200/390 Loss 0.412 Prec@(1,5) (85.6%, 99.5%)
01/19 03:50:11 AM | Train: [22/100] Step 200/390 Loss 0.205 Prec@(1,5) (92.8%, 99.9%)
01/19 03:50:11 AM | Train: [22/100] Step 200/390 Loss 0.419 Prec@(1,5) (85.5%, 99.4%)
01/19 03:50:15 AM | Train: [23/100] Step 200/390 Loss 0.212 Prec@(1,5) (92.4%, 99.9%)
01/19 03:50:15 AM | Train: [22/100] Step 300/390 Loss 0.413 Prec@(1,5) (85.6%, 99.5%)
01/19 03:50:17 AM | Train: [22/100] Step 300/390 Loss 0.418 Prec@(1,5) (85.6%, 99.4%)
01/19 03:50:17 AM | Train: [22/100] Step 300/390 Loss 0.208 Prec@(1,5) (92.7%, 99.9%)
01/19 03:50:20 AM | Train: [22/100] Step 390/390 Loss 0.415 Prec@(1,5) (85.5%, 99.5%)
01/19 03:50:21 AM | Train: [22/100] Final Prec@1 85.5300%
01/19 03:50:21 AM | Train: [23/100] Step 300/390 Loss 0.215 Prec@(1,5) (92.4%, 99.9%)
01/19 03:50:21 AM | Valid: [22/100] Step 000/078 Loss 0.471 Prec@(1,5) (83.6%, 99.2%)
01/19 03:50:22 AM | Train: [22/100] Step 390/390 Loss 0.421 Prec@(1,5) (85.5%, 99.4%)
01/19 03:50:22 AM | Train: [22/100] Final Prec@1 85.4660%
01/19 03:50:22 AM | Train: [22/100] Step 390/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:50:22 AM | Train: [22/100] Final Prec@1 92.6820%
01/19 03:50:23 AM | Valid: [22/100] Step 078/078 Loss 0.495 Prec@(1,5) (83.6%, 99.2%)
01/19 03:50:23 AM | Valid: [22/100] Step 000/078 Loss 0.458 Prec@(1,5) (82.8%, 98.4%)
01/19 03:50:23 AM | Valid: [22/100] Final Prec@1 83.6200%
01/19 03:50:23 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:50:23 AM | Valid: [22/100] Step 000/078 Loss 0.277 Prec@(1,5) (89.1%, 100.0%)
01/19 03:50:23 AM | Train: [23/100] Step 000/390 Loss 0.422 Prec@(1,5) (81.2%, 100.0%)
01/19 03:50:24 AM | Valid: [22/100] Step 078/078 Loss 0.467 Prec@(1,5) (83.7%, 99.2%)
01/19 03:50:24 AM | Valid: [22/100] Final Prec@1 83.7300%
01/19 03:50:25 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:50:25 AM | Valid: [22/100] Step 078/078 Loss 0.381 Prec@(1,5) (87.6%, 99.4%)
01/19 03:50:25 AM | Valid: [22/100] Final Prec@1 87.5600%
01/19 03:50:25 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:50:25 AM | Train: [23/100] Step 000/390 Loss 0.417 Prec@(1,5) (85.2%, 99.2%)
01/19 03:50:26 AM | Train: [23/100] Step 000/390 Loss 0.251 Prec@(1,5) (91.4%, 100.0%)
01/19 03:50:26 AM | Train: [23/100] Step 390/390 Loss 0.216 Prec@(1,5) (92.4%, 99.9%)
01/19 03:50:26 AM | Train: [23/100] Final Prec@1 92.3520%
01/19 03:50:26 AM | Valid: [23/100] Step 000/078 Loss 0.295 Prec@(1,5) (91.4%, 98.4%)
01/19 03:50:28 AM | Valid: [23/100] Step 078/078 Loss 0.387 Prec@(1,5) (87.6%, 99.4%)
01/19 03:50:28 AM | Valid: [23/100] Final Prec@1 87.5900%
01/19 03:50:28 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:50:29 AM | Train: [24/100] Step 000/390 Loss 0.168 Prec@(1,5) (95.3%, 100.0%)
01/19 03:50:29 AM | Train: [23/100] Step 100/390 Loss 0.408 Prec@(1,5) (85.9%, 99.4%)
01/19 03:50:31 AM | Train: [23/100] Step 100/390 Loss 0.195 Prec@(1,5) (93.2%, 99.9%)
01/19 03:50:31 AM | Train: [23/100] Step 100/390 Loss 0.397 Prec@(1,5) (86.5%, 99.5%)
01/19 03:50:35 AM | Train: [24/100] Step 100/390 Loss 0.202 Prec@(1,5) (93.1%, 99.9%)
01/19 03:50:36 AM | Train: [23/100] Step 200/390 Loss 0.412 Prec@(1,5) (85.7%, 99.4%)
01/19 03:50:37 AM | Train: [23/100] Step 200/390 Loss 0.200 Prec@(1,5) (93.1%, 99.9%)
01/19 03:50:37 AM | Train: [23/100] Step 200/390 Loss 0.410 Prec@(1,5) (86.0%, 99.5%)
01/19 03:50:41 AM | Train: [24/100] Step 200/390 Loss 0.206 Prec@(1,5) (92.9%, 99.9%)
01/19 03:50:42 AM | Train: [23/100] Step 300/390 Loss 0.415 Prec@(1,5) (85.6%, 99.4%)
01/19 03:50:43 AM | Train: [23/100] Step 300/390 Loss 0.205 Prec@(1,5) (93.0%, 99.9%)
01/19 03:50:44 AM | Train: [23/100] Step 300/390 Loss 0.416 Prec@(1,5) (85.8%, 99.4%)
01/19 03:50:47 AM | Train: [23/100] Step 390/390 Loss 0.418 Prec@(1,5) (85.5%, 99.4%)
01/19 03:50:47 AM | Train: [24/100] Step 300/390 Loss 0.214 Prec@(1,5) (92.6%, 99.9%)
01/19 03:50:47 AM | Train: [23/100] Final Prec@1 85.4660%
01/19 03:50:48 AM | Valid: [23/100] Step 000/078 Loss 0.418 Prec@(1,5) (83.6%, 98.4%)
01/19 03:50:49 AM | Train: [23/100] Step 390/390 Loss 0.206 Prec@(1,5) (92.9%, 99.9%)
01/19 03:50:49 AM | Train: [23/100] Step 390/390 Loss 0.421 Prec@(1,5) (85.5%, 99.4%)
01/19 03:50:49 AM | Train: [23/100] Final Prec@1 92.9020%
01/19 03:50:49 AM | Train: [23/100] Final Prec@1 85.4640%
01/19 03:50:49 AM | Valid: [23/100] Step 078/078 Loss 0.484 Prec@(1,5) (83.6%, 99.3%)
01/19 03:50:49 AM | Valid: [23/100] Final Prec@1 83.5600%
01/19 03:50:49 AM | Valid: [23/100] Step 000/078 Loss 0.341 Prec@(1,5) (89.8%, 98.4%)
01/19 03:50:49 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:50:49 AM | Valid: [23/100] Step 000/078 Loss 0.456 Prec@(1,5) (85.2%, 99.2%)
01/19 03:50:50 AM | Train: [24/100] Step 000/390 Loss 0.438 Prec@(1,5) (84.4%, 100.0%)
01/19 03:50:51 AM | Valid: [23/100] Step 078/078 Loss 0.412 Prec@(1,5) (87.1%, 99.3%)
01/19 03:50:51 AM | Valid: [23/100] Final Prec@1 87.0900%
01/19 03:50:51 AM | Valid: [23/100] Step 078/078 Loss 0.505 Prec@(1,5) (82.8%, 99.3%)
01/19 03:50:51 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:50:51 AM | Valid: [23/100] Final Prec@1 82.7800%
01/19 03:50:51 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:50:52 AM | Train: [24/100] Step 000/390 Loss 0.197 Prec@(1,5) (92.2%, 100.0%)
01/19 03:50:52 AM | Train: [24/100] Step 000/390 Loss 0.383 Prec@(1,5) (86.7%, 100.0%)
01/19 03:50:52 AM | Train: [24/100] Step 390/390 Loss 0.213 Prec@(1,5) (92.5%, 99.9%)
01/19 03:50:52 AM | Train: [24/100] Final Prec@1 92.5200%
01/19 03:50:53 AM | Valid: [24/100] Step 000/078 Loss 0.297 Prec@(1,5) (90.6%, 99.2%)
01/19 03:50:54 AM | Valid: [24/100] Step 078/078 Loss 0.372 Prec@(1,5) (88.0%, 99.4%)
01/19 03:50:55 AM | Valid: [24/100] Final Prec@1 87.9700%
01/19 03:50:55 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:50:55 AM | Train: [25/100] Step 000/390 Loss 0.185 Prec@(1,5) (91.4%, 100.0%)
01/19 03:50:56 AM | Train: [24/100] Step 100/390 Loss 0.405 Prec@(1,5) (86.2%, 99.5%)
01/19 03:50:57 AM | Train: [24/100] Step 100/390 Loss 0.207 Prec@(1,5) (92.6%, 99.9%)
01/19 03:50:58 AM | Train: [24/100] Step 100/390 Loss 0.405 Prec@(1,5) (86.0%, 99.5%)
01/19 03:51:01 AM | Train: [25/100] Step 100/390 Loss 0.195 Prec@(1,5) (93.0%, 99.9%)
01/19 03:51:01 AM | Train: [24/100] Step 200/390 Loss 0.414 Prec@(1,5) (85.6%, 99.5%)
01/19 03:51:03 AM | Train: [24/100] Step 200/390 Loss 0.208 Prec@(1,5) (92.6%, 99.9%)
01/19 03:51:04 AM | Train: [24/100] Step 200/390 Loss 0.411 Prec@(1,5) (85.7%, 99.5%)
01/19 03:51:07 AM | Train: [25/100] Step 200/390 Loss 0.200 Prec@(1,5) (92.9%, 99.9%)
01/19 03:51:07 AM | Train: [24/100] Step 300/390 Loss 0.419 Prec@(1,5) (85.4%, 99.5%)
01/19 03:51:08 AM | Train: [24/100] Step 300/390 Loss 0.214 Prec@(1,5) (92.5%, 99.9%)
01/19 03:51:09 AM | Train: [24/100] Step 300/390 Loss 0.420 Prec@(1,5) (85.4%, 99.5%)
01/19 03:51:12 AM | Train: [24/100] Step 390/390 Loss 0.417 Prec@(1,5) (85.5%, 99.5%)
01/19 03:51:12 AM | Train: [24/100] Final Prec@1 85.5040%
01/19 03:51:13 AM | Valid: [24/100] Step 000/078 Loss 0.437 Prec@(1,5) (80.5%, 99.2%)
01/19 03:51:13 AM | Train: [25/100] Step 300/390 Loss 0.208 Prec@(1,5) (92.6%, 99.9%)
01/19 03:51:13 AM | Train: [24/100] Step 390/390 Loss 0.215 Prec@(1,5) (92.4%, 99.9%)
01/19 03:51:14 AM | Train: [24/100] Final Prec@1 92.4500%
01/19 03:51:14 AM | Train: [24/100] Step 390/390 Loss 0.422 Prec@(1,5) (85.3%, 99.4%)
01/19 03:51:14 AM | Valid: [24/100] Step 000/078 Loss 0.319 Prec@(1,5) (90.6%, 100.0%)
01/19 03:51:14 AM | Valid: [24/100] Step 078/078 Loss 0.498 Prec@(1,5) (83.6%, 99.3%)
01/19 03:51:14 AM | Train: [24/100] Final Prec@1 85.3200%
01/19 03:51:14 AM | Valid: [24/100] Final Prec@1 83.5800%
01/19 03:51:14 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:51:15 AM | Train: [25/100] Step 000/390 Loss 0.492 Prec@(1,5) (85.2%, 98.4%)
01/19 03:51:15 AM | Valid: [24/100] Step 000/078 Loss 0.473 Prec@(1,5) (85.2%, 98.4%)
01/19 03:51:15 AM | Valid: [24/100] Step 078/078 Loss 0.376 Prec@(1,5) (88.1%, 99.5%)
01/19 03:51:16 AM | Valid: [24/100] Final Prec@1 88.0600%
01/19 03:51:16 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:51:16 AM | Valid: [24/100] Step 078/078 Loss 0.481 Prec@(1,5) (83.8%, 99.2%)
01/19 03:51:16 AM | Train: [25/100] Step 000/390 Loss 0.204 Prec@(1,5) (92.2%, 100.0%)
01/19 03:51:16 AM | Valid: [24/100] Final Prec@1 83.7600%
01/19 03:51:16 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:51:17 AM | Train: [25/100] Step 000/390 Loss 0.434 Prec@(1,5) (85.2%, 99.2%)
01/19 03:51:17 AM | Train: [25/100] Step 390/390 Loss 0.211 Prec@(1,5) (92.6%, 99.9%)
01/19 03:51:17 AM | Train: [25/100] Final Prec@1 92.5540%
01/19 03:51:18 AM | Valid: [25/100] Step 000/078 Loss 0.338 Prec@(1,5) (89.8%, 100.0%)
01/19 03:51:19 AM | Valid: [25/100] Step 078/078 Loss 0.419 Prec@(1,5) (87.0%, 99.3%)
01/19 03:51:19 AM | Valid: [25/100] Final Prec@1 87.0200%
01/19 03:51:19 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:51:20 AM | Train: [26/100] Step 000/390 Loss 0.127 Prec@(1,5) (96.9%, 100.0%)
01/19 03:51:20 AM | Train: [25/100] Step 100/390 Loss 0.402 Prec@(1,5) (86.3%, 99.5%)
01/19 03:51:21 AM | Train: [25/100] Step 100/390 Loss 0.187 Prec@(1,5) (93.5%, 99.9%)
01/19 03:51:23 AM | Train: [25/100] Step 100/390 Loss 0.401 Prec@(1,5) (85.9%, 99.5%)
01/19 03:51:26 AM | Train: [26/100] Step 100/390 Loss 0.205 Prec@(1,5) (92.8%, 99.9%)
01/19 03:51:26 AM | Train: [25/100] Step 200/390 Loss 0.405 Prec@(1,5) (85.9%, 99.5%)
01/19 03:51:27 AM | Train: [25/100] Step 200/390 Loss 0.192 Prec@(1,5) (93.3%, 99.9%)
01/19 03:51:28 AM | Train: [25/100] Step 200/390 Loss 0.405 Prec@(1,5) (85.8%, 99.5%)
01/19 03:51:31 AM | Train: [26/100] Step 200/390 Loss 0.203 Prec@(1,5) (92.9%, 99.9%)
01/19 03:51:32 AM | Train: [25/100] Step 300/390 Loss 0.410 Prec@(1,5) (85.7%, 99.5%)
01/19 03:51:33 AM | Train: [25/100] Step 300/390 Loss 0.199 Prec@(1,5) (93.1%, 99.9%)
01/19 03:51:34 AM | Train: [25/100] Step 300/390 Loss 0.410 Prec@(1,5) (85.8%, 99.4%)
01/19 03:51:37 AM | Train: [26/100] Step 300/390 Loss 0.206 Prec@(1,5) (92.8%, 99.9%)
01/19 03:51:37 AM | Train: [25/100] Step 390/390 Loss 0.412 Prec@(1,5) (85.6%, 99.5%)
01/19 03:51:37 AM | Train: [25/100] Final Prec@1 85.6260%
01/19 03:51:38 AM | Valid: [25/100] Step 000/078 Loss 0.498 Prec@(1,5) (85.2%, 98.4%)
01/19 03:51:38 AM | Train: [25/100] Step 390/390 Loss 0.203 Prec@(1,5) (93.0%, 99.9%)
01/19 03:51:39 AM | Train: [25/100] Final Prec@1 92.9720%
01/19 03:51:39 AM | Valid: [25/100] Step 078/078 Loss 0.488 Prec@(1,5) (83.4%, 99.2%)
01/19 03:51:39 AM | Valid: [25/100] Step 000/078 Loss 0.433 Prec@(1,5) (88.3%, 99.2%)
01/19 03:51:39 AM | Train: [25/100] Step 390/390 Loss 0.411 Prec@(1,5) (85.8%, 99.4%)
01/19 03:51:39 AM | Valid: [25/100] Final Prec@1 83.3800%
01/19 03:51:39 AM | Train: [25/100] Final Prec@1 85.7780%
01/19 03:51:39 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:51:40 AM | Valid: [25/100] Step 000/078 Loss 0.492 Prec@(1,5) (84.4%, 99.2%)
01/19 03:51:40 AM | Train: [26/100] Step 000/390 Loss 0.302 Prec@(1,5) (87.5%, 99.2%)
01/19 03:51:40 AM | Valid: [25/100] Step 078/078 Loss 0.440 Prec@(1,5) (86.5%, 99.3%)
01/19 03:51:41 AM | Valid: [25/100] Final Prec@1 86.5100%
01/19 03:51:41 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:51:41 AM | Valid: [25/100] Step 078/078 Loss 0.522 Prec@(1,5) (82.4%, 99.1%)
01/19 03:51:41 AM | Valid: [25/100] Final Prec@1 82.4000%
01/19 03:51:41 AM | Train: [26/100] Step 000/390 Loss 0.135 Prec@(1,5) (95.3%, 100.0%)
01/19 03:51:41 AM | Train: [26/100] Step 390/390 Loss 0.208 Prec@(1,5) (92.7%, 99.9%)
01/19 03:51:41 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:51:41 AM | Train: [26/100] Final Prec@1 92.7180%
01/19 03:51:42 AM | Valid: [26/100] Step 000/078 Loss 0.297 Prec@(1,5) (91.4%, 97.7%)
01/19 03:51:42 AM | Train: [26/100] Step 000/390 Loss 0.349 Prec@(1,5) (87.5%, 99.2%)
01/19 03:51:44 AM | Valid: [26/100] Step 078/078 Loss 0.403 Prec@(1,5) (87.4%, 99.5%)
01/19 03:51:44 AM | Valid: [26/100] Final Prec@1 87.3900%
01/19 03:51:44 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:51:44 AM | Train: [27/100] Step 000/390 Loss 0.137 Prec@(1,5) (95.3%, 100.0%)
01/19 03:51:45 AM | Train: [26/100] Step 100/390 Loss 0.402 Prec@(1,5) (86.1%, 99.5%)
01/19 03:51:46 AM | Train: [26/100] Step 100/390 Loss 0.200 Prec@(1,5) (93.0%, 99.9%)
01/19 03:51:48 AM | Train: [26/100] Step 100/390 Loss 0.399 Prec@(1,5) (86.2%, 99.5%)
01/19 03:51:50 AM | Train: [27/100] Step 100/390 Loss 0.201 Prec@(1,5) (93.0%, 99.9%)
01/19 03:51:51 AM | Train: [26/100] Step 200/390 Loss 0.404 Prec@(1,5) (86.0%, 99.5%)
01/19 03:51:52 AM | Train: [26/100] Step 200/390 Loss 0.196 Prec@(1,5) (93.2%, 99.9%)
01/19 03:51:53 AM | Train: [26/100] Step 200/390 Loss 0.401 Prec@(1,5) (86.1%, 99.5%)
01/19 03:51:56 AM | Train: [27/100] Step 200/390 Loss 0.204 Prec@(1,5) (92.9%, 99.9%)
01/19 03:51:57 AM | Train: [26/100] Step 300/390 Loss 0.410 Prec@(1,5) (85.8%, 99.5%)
01/19 03:51:58 AM | Train: [26/100] Step 300/390 Loss 0.203 Prec@(1,5) (92.9%, 99.9%)
01/19 03:51:59 AM | Train: [26/100] Step 300/390 Loss 0.410 Prec@(1,5) (85.8%, 99.5%)
01/19 03:52:01 AM | Train: [27/100] Step 300/390 Loss 0.212 Prec@(1,5) (92.6%, 99.9%)
01/19 03:52:02 AM | Train: [26/100] Step 390/390 Loss 0.414 Prec@(1,5) (85.6%, 99.5%)
01/19 03:52:02 AM | Train: [26/100] Final Prec@1 85.6360%
01/19 03:52:02 AM | Valid: [26/100] Step 000/078 Loss 0.496 Prec@(1,5) (86.7%, 98.4%)
01/19 03:52:03 AM | Train: [26/100] Step 390/390 Loss 0.206 Prec@(1,5) (92.8%, 99.9%)
01/19 03:52:04 AM | Train: [26/100] Final Prec@1 92.7620%
01/19 03:52:04 AM | Valid: [26/100] Step 078/078 Loss 0.489 Prec@(1,5) (83.8%, 99.1%)
01/19 03:52:04 AM | Train: [26/100] Step 390/390 Loss 0.411 Prec@(1,5) (85.7%, 99.5%)
01/19 03:52:04 AM | Valid: [26/100] Final Prec@1 83.8300%
01/19 03:52:04 AM | Train: [26/100] Final Prec@1 85.6920%
01/19 03:52:04 AM | Valid: [26/100] Step 000/078 Loss 0.285 Prec@(1,5) (90.6%, 100.0%)
01/19 03:52:04 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:52:04 AM | Valid: [26/100] Step 000/078 Loss 0.504 Prec@(1,5) (85.9%, 98.4%)
01/19 03:52:05 AM | Train: [27/100] Step 000/390 Loss 0.350 Prec@(1,5) (89.1%, 99.2%)
01/19 03:52:05 AM | Valid: [26/100] Step 078/078 Loss 0.384 Prec@(1,5) (87.8%, 99.4%)
01/19 03:52:06 AM | Valid: [26/100] Final Prec@1 87.7700%
01/19 03:52:06 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:52:06 AM | Valid: [26/100] Step 078/078 Loss 0.488 Prec@(1,5) (83.8%, 99.1%)
01/19 03:52:06 AM | Train: [27/100] Step 390/390 Loss 0.214 Prec@(1,5) (92.5%, 99.9%)
01/19 03:52:06 AM | Valid: [26/100] Final Prec@1 83.8200%
01/19 03:52:06 AM | Train: [27/100] Final Prec@1 92.5160%
01/19 03:52:06 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:52:06 AM | Train: [27/100] Step 000/390 Loss 0.196 Prec@(1,5) (93.0%, 100.0%)
01/19 03:52:06 AM | Valid: [27/100] Step 000/078 Loss 0.322 Prec@(1,5) (92.2%, 97.7%)
01/19 03:52:07 AM | Train: [27/100] Step 000/390 Loss 0.291 Prec@(1,5) (90.6%, 100.0%)
01/19 03:52:08 AM | Valid: [27/100] Step 078/078 Loss 0.378 Prec@(1,5) (88.0%, 99.5%)
01/19 03:52:08 AM | Valid: [27/100] Final Prec@1 87.9700%
01/19 03:52:08 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:52:09 AM | Train: [28/100] Step 000/390 Loss 0.220 Prec@(1,5) (91.4%, 100.0%)
01/19 03:52:10 AM | Train: [27/100] Step 100/390 Loss 0.392 Prec@(1,5) (86.1%, 99.5%)
01/19 03:52:12 AM | Train: [27/100] Step 100/390 Loss 0.192 Prec@(1,5) (93.2%, 99.9%)
01/19 03:52:13 AM | Train: [27/100] Step 100/390 Loss 0.396 Prec@(1,5) (86.0%, 99.5%)
01/19 03:52:15 AM | Train: [28/100] Step 100/390 Loss 0.199 Prec@(1,5) (92.9%, 99.9%)
01/19 03:52:16 AM | Train: [27/100] Step 200/390 Loss 0.394 Prec@(1,5) (86.2%, 99.5%)
01/19 03:52:17 AM | Train: [27/100] Step 200/390 Loss 0.196 Prec@(1,5) (93.1%, 99.9%)
01/19 03:52:18 AM | Train: [27/100] Step 200/390 Loss 0.401 Prec@(1,5) (85.8%, 99.5%)
01/19 03:52:20 AM | Train: [28/100] Step 200/390 Loss 0.197 Prec@(1,5) (93.0%, 99.9%)
01/19 03:52:22 AM | Train: [27/100] Step 300/390 Loss 0.404 Prec@(1,5) (85.9%, 99.5%)
01/19 03:52:23 AM | Train: [27/100] Step 300/390 Loss 0.208 Prec@(1,5) (92.7%, 99.9%)
01/19 03:52:24 AM | Train: [27/100] Step 300/390 Loss 0.408 Prec@(1,5) (85.6%, 99.5%)
01/19 03:52:26 AM | Train: [28/100] Step 300/390 Loss 0.199 Prec@(1,5) (92.9%, 99.9%)
01/19 03:52:27 AM | Train: [27/100] Step 390/390 Loss 0.412 Prec@(1,5) (85.6%, 99.4%)
01/19 03:52:27 AM | Train: [27/100] Final Prec@1 85.5740%
01/19 03:52:27 AM | Valid: [27/100] Step 000/078 Loss 0.487 Prec@(1,5) (85.2%, 99.2%)
01/19 03:52:28 AM | Train: [27/100] Step 390/390 Loss 0.210 Prec@(1,5) (92.7%, 99.9%)
01/19 03:52:29 AM | Train: [27/100] Final Prec@1 92.6500%
01/19 03:52:29 AM | Valid: [27/100] Step 078/078 Loss 0.475 Prec@(1,5) (84.1%, 99.3%)
01/19 03:52:29 AM | Train: [27/100] Step 390/390 Loss 0.411 Prec@(1,5) (85.6%, 99.5%)
01/19 03:52:29 AM | Valid: [27/100] Final Prec@1 84.1100%
01/19 03:52:29 AM | Valid: [27/100] Step 000/078 Loss 0.275 Prec@(1,5) (92.2%, 99.2%)
01/19 03:52:29 AM | Train: [27/100] Final Prec@1 85.6440%
01/19 03:52:29 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:52:29 AM | Valid: [27/100] Step 000/078 Loss 0.441 Prec@(1,5) (85.2%, 98.4%)
01/19 03:52:30 AM | Train: [28/100] Step 000/390 Loss 0.281 Prec@(1,5) (87.5%, 100.0%)
01/19 03:52:30 AM | Valid: [27/100] Step 078/078 Loss 0.360 Prec@(1,5) (88.5%, 99.6%)
01/19 03:52:31 AM | Valid: [27/100] Final Prec@1 88.5400%
01/19 03:52:31 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:52:31 AM | Train: [28/100] Step 390/390 Loss 0.200 Prec@(1,5) (92.9%, 99.9%)
01/19 03:52:31 AM | Valid: [27/100] Step 078/078 Loss 0.485 Prec@(1,5) (83.8%, 99.3%)
01/19 03:52:31 AM | Train: [28/100] Final Prec@1 92.9000%
01/19 03:52:31 AM | Valid: [27/100] Final Prec@1 83.8300%
01/19 03:52:31 AM | Train: [28/100] Step 000/390 Loss 0.164 Prec@(1,5) (94.5%, 100.0%)
01/19 03:52:31 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:52:32 AM | Valid: [28/100] Step 000/078 Loss 0.336 Prec@(1,5) (91.4%, 99.2%)
01/19 03:52:32 AM | Train: [28/100] Step 000/390 Loss 0.284 Prec@(1,5) (90.6%, 99.2%)
01/19 03:52:33 AM | Valid: [28/100] Step 078/078 Loss 0.368 Prec@(1,5) (88.2%, 99.5%)
01/19 03:52:33 AM | Valid: [28/100] Final Prec@1 88.1600%
01/19 03:52:34 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:52:34 AM | Train: [29/100] Step 000/390 Loss 0.195 Prec@(1,5) (91.4%, 100.0%)
01/19 03:52:35 AM | Train: [28/100] Step 100/390 Loss 0.383 Prec@(1,5) (86.8%, 99.5%)
01/19 03:52:37 AM | Train: [28/100] Step 100/390 Loss 0.193 Prec@(1,5) (93.1%, 99.9%)
01/19 03:52:38 AM | Train: [28/100] Step 100/390 Loss 0.395 Prec@(1,5) (86.4%, 99.5%)
01/19 03:52:41 AM | Train: [29/100] Step 100/390 Loss 0.188 Prec@(1,5) (93.3%, 99.9%)
01/19 03:52:42 AM | Train: [28/100] Step 200/390 Loss 0.388 Prec@(1,5) (86.6%, 99.5%)
01/19 03:52:44 AM | Train: [28/100] Step 200/390 Loss 0.188 Prec@(1,5) (93.4%, 99.9%)
01/19 03:52:45 AM | Train: [28/100] Step 200/390 Loss 0.402 Prec@(1,5) (86.1%, 99.5%)
01/19 03:52:47 AM | Train: [29/100] Step 200/390 Loss 0.191 Prec@(1,5) (93.1%, 99.9%)
01/19 03:52:48 AM | Train: [28/100] Step 300/390 Loss 0.395 Prec@(1,5) (86.3%, 99.5%)
01/19 03:52:49 AM | Train: [28/100] Step 300/390 Loss 0.192 Prec@(1,5) (93.3%, 99.9%)
01/19 03:52:51 AM | Train: [28/100] Step 300/390 Loss 0.405 Prec@(1,5) (86.0%, 99.5%)
01/19 03:52:53 AM | Train: [29/100] Step 300/390 Loss 0.198 Prec@(1,5) (93.0%, 99.9%)
01/19 03:52:53 AM | Train: [28/100] Step 390/390 Loss 0.398 Prec@(1,5) (86.2%, 99.5%)
01/19 03:52:53 AM | Train: [28/100] Final Prec@1 86.2240%
01/19 03:52:54 AM | Valid: [28/100] Step 000/078 Loss 0.455 Prec@(1,5) (85.2%, 99.2%)
01/19 03:52:55 AM | Train: [28/100] Step 390/390 Loss 0.196 Prec@(1,5) (93.2%, 99.9%)
01/19 03:52:55 AM | Train: [28/100] Final Prec@1 93.1600%
01/19 03:52:55 AM | Valid: [28/100] Step 078/078 Loss 0.471 Prec@(1,5) (84.0%, 99.3%)
01/19 03:52:55 AM | Valid: [28/100] Final Prec@1 84.0100%
01/19 03:52:55 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:52:56 AM | Train: [28/100] Step 390/390 Loss 0.407 Prec@(1,5) (86.0%, 99.5%)
01/19 03:52:56 AM | Valid: [28/100] Step 000/078 Loss 0.340 Prec@(1,5) (91.4%, 100.0%)
01/19 03:52:56 AM | Train: [28/100] Final Prec@1 85.9800%
01/19 03:52:56 AM | Train: [29/100] Step 000/390 Loss 0.267 Prec@(1,5) (93.8%, 100.0%)
01/19 03:52:56 AM | Valid: [28/100] Step 000/078 Loss 0.414 Prec@(1,5) (84.4%, 99.2%)
01/19 03:52:57 AM | Valid: [28/100] Step 078/078 Loss 0.384 Prec@(1,5) (88.1%, 99.5%)
01/19 03:52:57 AM | Train: [29/100] Step 390/390 Loss 0.202 Prec@(1,5) (92.8%, 99.9%)
01/19 03:52:57 AM | Valid: [28/100] Step 078/078 Loss 0.476 Prec@(1,5) (83.8%, 99.2%)
01/19 03:52:57 AM | Valid: [28/100] Final Prec@1 88.1100%
01/19 03:52:58 AM | Valid: [28/100] Final Prec@1 83.8000%
01/19 03:52:58 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:52:58 AM | Train: [29/100] Final Prec@1 92.8260%
01/19 03:52:58 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:52:58 AM | Valid: [29/100] Step 000/078 Loss 0.316 Prec@(1,5) (88.3%, 99.2%)
01/19 03:52:58 AM | Train: [29/100] Step 000/390 Loss 0.146 Prec@(1,5) (96.9%, 100.0%)
01/19 03:52:58 AM | Train: [29/100] Step 000/390 Loss 0.392 Prec@(1,5) (86.7%, 100.0%)
01/19 03:52:59 AM | Valid: [29/100] Step 078/078 Loss 0.363 Prec@(1,5) (88.5%, 99.5%)
01/19 03:53:00 AM | Valid: [29/100] Final Prec@1 88.5000%
01/19 03:53:00 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:53:00 AM | Train: [30/100] Step 000/390 Loss 0.240 Prec@(1,5) (89.1%, 100.0%)
01/19 03:53:02 AM | Train: [29/100] Step 100/390 Loss 0.389 Prec@(1,5) (86.6%, 99.5%)
01/19 03:53:04 AM | Train: [29/100] Step 100/390 Loss 0.185 Prec@(1,5) (93.7%, 99.9%)
01/19 03:53:04 AM | Train: [29/100] Step 100/390 Loss 0.397 Prec@(1,5) (86.4%, 99.5%)
01/19 03:53:06 AM | Train: [30/100] Step 100/390 Loss 0.188 Prec@(1,5) (93.3%, 100.0%)
01/19 03:53:07 AM | Train: [29/100] Step 200/390 Loss 0.391 Prec@(1,5) (86.6%, 99.5%)
01/19 03:53:09 AM | Train: [29/100] Step 200/390 Loss 0.185 Prec@(1,5) (93.7%, 99.9%)
01/19 03:53:10 AM | Train: [29/100] Step 200/390 Loss 0.398 Prec@(1,5) (86.3%, 99.5%)
01/19 03:53:12 AM | Train: [30/100] Step 200/390 Loss 0.191 Prec@(1,5) (93.2%, 99.9%)
01/19 03:53:13 AM | Train: [29/100] Step 300/390 Loss 0.399 Prec@(1,5) (86.2%, 99.5%)
01/19 03:53:15 AM | Train: [29/100] Step 300/390 Loss 0.193 Prec@(1,5) (93.3%, 99.9%)
01/19 03:53:16 AM | Train: [29/100] Step 300/390 Loss 0.403 Prec@(1,5) (86.0%, 99.5%)
01/19 03:53:18 AM | Train: [30/100] Step 300/390 Loss 0.195 Prec@(1,5) (93.0%, 99.9%)
01/19 03:53:19 AM | Train: [29/100] Step 390/390 Loss 0.401 Prec@(1,5) (86.0%, 99.5%)
01/19 03:53:19 AM | Train: [29/100] Final Prec@1 86.0260%
01/19 03:53:19 AM | Valid: [29/100] Step 000/078 Loss 0.438 Prec@(1,5) (82.8%, 98.4%)
01/19 03:53:21 AM | Valid: [29/100] Step 078/078 Loss 0.487 Prec@(1,5) (83.8%, 99.2%)
01/19 03:53:21 AM | Valid: [29/100] Final Prec@1 83.8200%
01/19 03:53:21 AM | Train: [29/100] Step 390/390 Loss 0.198 Prec@(1,5) (93.2%, 99.9%)
01/19 03:53:21 AM | Train: [29/100] Step 390/390 Loss 0.407 Prec@(1,5) (85.9%, 99.4%)
01/19 03:53:21 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
01/19 03:53:21 AM | Train: [29/100] Final Prec@1 93.1500%
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:53:21 AM | Train: [29/100] Final Prec@1 85.8800%
01/19 03:53:21 AM | Valid: [29/100] Step 000/078 Loss 0.362 Prec@(1,5) (88.3%, 99.2%)
01/19 03:53:21 AM | Valid: [29/100] Step 000/078 Loss 0.445 Prec@(1,5) (85.2%, 100.0%)
01/19 03:53:21 AM | Train: [30/100] Step 000/390 Loss 0.369 Prec@(1,5) (89.8%, 100.0%)
01/19 03:53:23 AM | Valid: [29/100] Step 078/078 Loss 0.362 Prec@(1,5) (88.5%, 99.5%)
01/19 03:53:23 AM | Train: [30/100] Step 390/390 Loss 0.199 Prec@(1,5) (93.0%, 99.9%)
01/19 03:53:23 AM | Valid: [29/100] Final Prec@1 88.4900%
01/19 03:53:23 AM | Train: [30/100] Final Prec@1 92.9540%
01/19 03:53:23 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:53:23 AM | Valid: [29/100] Step 078/078 Loss 0.504 Prec@(1,5) (82.9%, 99.3%)
01/19 03:53:23 AM | Valid: [29/100] Final Prec@1 82.9400%
01/19 03:53:23 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:53:23 AM | Valid: [30/100] Step 000/078 Loss 0.386 Prec@(1,5) (89.1%, 99.2%)
01/19 03:53:24 AM | Train: [30/100] Step 000/390 Loss 0.185 Prec@(1,5) (93.8%, 100.0%)
01/19 03:53:24 AM | Train: [30/100] Step 000/390 Loss 0.416 Prec@(1,5) (85.2%, 100.0%)
01/19 03:53:25 AM | Valid: [30/100] Step 078/078 Loss 0.366 Prec@(1,5) (88.1%, 99.6%)
01/19 03:53:25 AM | Valid: [30/100] Final Prec@1 88.1500%
01/19 03:53:25 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:53:26 AM | Train: [31/100] Step 000/390 Loss 0.156 Prec@(1,5) (94.5%, 100.0%)
01/19 03:53:27 AM | Train: [30/100] Step 100/390 Loss 0.402 Prec@(1,5) (85.9%, 99.6%)
01/19 03:53:29 AM | Train: [30/100] Step 100/390 Loss 0.179 Prec@(1,5) (93.7%, 99.9%)
01/19 03:53:30 AM | Train: [30/100] Step 100/390 Loss 0.398 Prec@(1,5) (86.0%, 99.5%)
01/19 03:53:32 AM | Train: [31/100] Step 100/390 Loss 0.187 Prec@(1,5) (93.4%, 100.0%)
01/19 03:53:33 AM | Train: [30/100] Step 200/390 Loss 0.392 Prec@(1,5) (86.3%, 99.6%)
01/19 03:53:35 AM | Train: [30/100] Step 200/390 Loss 0.182 Prec@(1,5) (93.6%, 99.9%)
01/19 03:53:36 AM | Train: [30/100] Step 200/390 Loss 0.392 Prec@(1,5) (86.4%, 99.5%)
01/19 03:53:38 AM | Train: [31/100] Step 200/390 Loss 0.192 Prec@(1,5) (93.2%, 100.0%)
01/19 03:53:39 AM | Train: [30/100] Step 300/390 Loss 0.398 Prec@(1,5) (86.1%, 99.6%)
01/19 03:53:41 AM | Train: [30/100] Step 300/390 Loss 0.190 Prec@(1,5) (93.4%, 99.9%)
01/19 03:53:42 AM | Train: [30/100] Step 300/390 Loss 0.399 Prec@(1,5) (86.1%, 99.5%)
01/19 03:53:44 AM | Train: [31/100] Step 300/390 Loss 0.191 Prec@(1,5) (93.3%, 99.9%)
01/19 03:53:44 AM | Train: [30/100] Step 390/390 Loss 0.401 Prec@(1,5) (86.0%, 99.5%)
01/19 03:53:45 AM | Train: [30/100] Final Prec@1 86.0040%
01/19 03:53:45 AM | Valid: [30/100] Step 000/078 Loss 0.441 Prec@(1,5) (85.2%, 99.2%)
01/19 03:53:47 AM | Valid: [30/100] Step 078/078 Loss 0.480 Prec@(1,5) (83.4%, 99.4%)
01/19 03:53:47 AM | Valid: [30/100] Final Prec@1 83.3900%
01/19 03:53:47 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:53:47 AM | Train: [30/100] Step 390/390 Loss 0.195 Prec@(1,5) (93.2%, 99.9%)
01/19 03:53:47 AM | Train: [30/100] Final Prec@1 93.1600%
01/19 03:53:47 AM | Train: [30/100] Step 390/390 Loss 0.403 Prec@(1,5) (86.0%, 99.5%)
01/19 03:53:47 AM | Train: [31/100] Step 000/390 Loss 0.413 Prec@(1,5) (85.9%, 100.0%)
01/19 03:53:48 AM | Train: [30/100] Final Prec@1 85.9720%
01/19 03:53:48 AM | Valid: [30/100] Step 000/078 Loss 0.324 Prec@(1,5) (89.1%, 99.2%)
01/19 03:53:48 AM | Valid: [30/100] Step 000/078 Loss 0.447 Prec@(1,5) (85.9%, 99.2%)
01/19 03:53:49 AM | Valid: [30/100] Step 078/078 Loss 0.372 Prec@(1,5) (88.0%, 99.5%)
01/19 03:53:49 AM | Valid: [30/100] Final Prec@1 88.0100%
01/19 03:53:49 AM | Train: [31/100] Step 390/390 Loss 0.191 Prec@(1,5) (93.3%, 100.0%)
01/19 03:53:49 AM | Train: [31/100] Final Prec@1 93.3040%
01/19 03:53:49 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:53:50 AM | Valid: [30/100] Step 078/078 Loss 0.497 Prec@(1,5) (83.3%, 99.3%)
01/19 03:53:50 AM | Valid: [30/100] Final Prec@1 83.2800%
01/19 03:53:50 AM | Valid: [31/100] Step 000/078 Loss 0.405 Prec@(1,5) (88.3%, 99.2%)
01/19 03:53:50 AM | Train: [31/100] Step 000/390 Loss 0.215 Prec@(1,5) (90.6%, 100.0%)
01/19 03:53:50 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:53:51 AM | Train: [31/100] Step 000/390 Loss 0.306 Prec@(1,5) (89.1%, 100.0%)
01/19 03:53:51 AM | Valid: [31/100] Step 078/078 Loss 0.390 Prec@(1,5) (87.9%, 99.6%)
01/19 03:53:52 AM | Valid: [31/100] Final Prec@1 87.9000%
01/19 03:53:52 AM | Current best Prec@1 = 88.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:53:52 AM | Train: [32/100] Step 000/390 Loss 0.235 Prec@(1,5) (92.2%, 99.2%)
01/19 03:53:53 AM | Train: [31/100] Step 100/390 Loss 0.384 Prec@(1,5) (86.7%, 99.6%)
01/19 03:53:56 AM | Train: [31/100] Step 100/390 Loss 0.180 Prec@(1,5) (93.7%, 99.9%)
01/19 03:53:57 AM | Train: [31/100] Step 100/390 Loss 0.392 Prec@(1,5) (86.5%, 99.6%)
01/19 03:53:58 AM | Train: [32/100] Step 100/390 Loss 0.199 Prec@(1,5) (92.9%, 99.9%)
01/19 03:53:59 AM | Train: [31/100] Step 200/390 Loss 0.386 Prec@(1,5) (86.5%, 99.6%)
01/19 03:54:02 AM | Train: [31/100] Step 200/390 Loss 0.186 Prec@(1,5) (93.5%, 99.9%)
01/19 03:54:03 AM | Train: [31/100] Step 200/390 Loss 0.393 Prec@(1,5) (86.4%, 99.5%)
01/19 03:54:04 AM | Train: [32/100] Step 200/390 Loss 0.202 Prec@(1,5) (92.9%, 99.9%)
01/19 03:54:06 AM | Train: [31/100] Step 300/390 Loss 0.389 Prec@(1,5) (86.4%, 99.6%)
01/19 03:54:08 AM | Train: [31/100] Step 300/390 Loss 0.186 Prec@(1,5) (93.6%, 99.9%)
01/19 03:54:09 AM | Train: [31/100] Step 300/390 Loss 0.395 Prec@(1,5) (86.3%, 99.5%)
01/19 03:54:11 AM | Train: [32/100] Step 300/390 Loss 0.202 Prec@(1,5) (92.9%, 99.9%)
01/19 03:54:11 AM | Train: [31/100] Step 390/390 Loss 0.393 Prec@(1,5) (86.3%, 99.6%)
01/19 03:54:11 AM | Train: [31/100] Final Prec@1 86.2540%
01/19 03:54:12 AM | Valid: [31/100] Step 000/078 Loss 0.514 Prec@(1,5) (82.0%, 100.0%)
01/19 03:54:13 AM | Valid: [31/100] Step 078/078 Loss 0.495 Prec@(1,5) (83.6%, 99.3%)
01/19 03:54:13 AM | Valid: [31/100] Final Prec@1 83.6000%
01/19 03:54:13 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:54:14 AM | Train: [31/100] Step 390/390 Loss 0.192 Prec@(1,5) (93.3%, 99.9%)
01/19 03:54:14 AM | Train: [31/100] Step 390/390 Loss 0.401 Prec@(1,5) (86.0%, 99.5%)
01/19 03:54:14 AM | Train: [31/100] Final Prec@1 93.3380%
01/19 03:54:14 AM | Train: [32/100] Step 000/390 Loss 0.387 Prec@(1,5) (89.8%, 100.0%)
01/19 03:54:14 AM | Train: [31/100] Final Prec@1 86.0160%
01/19 03:54:14 AM | Valid: [31/100] Step 000/078 Loss 0.362 Prec@(1,5) (89.8%, 97.7%)
01/19 03:54:15 AM | Valid: [31/100] Step 000/078 Loss 0.450 Prec@(1,5) (81.2%, 99.2%)
01/19 03:54:16 AM | Train: [32/100] Step 390/390 Loss 0.200 Prec@(1,5) (93.1%, 99.9%)
01/19 03:54:16 AM | Train: [32/100] Final Prec@1 93.0700%
01/19 03:54:16 AM | Valid: [31/100] Step 078/078 Loss 0.366 Prec@(1,5) (88.6%, 99.6%)
01/19 03:54:16 AM | Valid: [31/100] Final Prec@1 88.6200%
01/19 03:54:16 AM | Valid: [31/100] Step 078/078 Loss 0.478 Prec@(1,5) (83.5%, 99.3%)
01/19 03:54:16 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:54:16 AM | Valid: [32/100] Step 000/078 Loss 0.289 Prec@(1,5) (92.2%, 98.4%)
01/19 03:54:16 AM | Valid: [31/100] Final Prec@1 83.4700%
01/19 03:54:17 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:54:17 AM | Train: [32/100] Step 000/390 Loss 0.259 Prec@(1,5) (91.4%, 99.2%)
01/19 03:54:17 AM | Train: [32/100] Step 000/390 Loss 0.373 Prec@(1,5) (87.5%, 100.0%)
01/19 03:54:18 AM | Valid: [32/100] Step 078/078 Loss 0.336 Prec@(1,5) (89.3%, 99.5%)
01/19 03:54:18 AM | Valid: [32/100] Final Prec@1 89.3400%
01/19 03:54:18 AM | Current best Prec@1 = 89.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:54:19 AM | Train: [33/100] Step 000/390 Loss 0.197 Prec@(1,5) (92.2%, 100.0%)
01/19 03:54:19 AM | Train: [32/100] Step 100/390 Loss 0.402 Prec@(1,5) (86.1%, 99.6%)
01/19 03:54:23 AM | Train: [32/100] Step 100/390 Loss 0.189 Prec@(1,5) (93.4%, 99.9%)
01/19 03:54:23 AM | Train: [32/100] Step 100/390 Loss 0.394 Prec@(1,5) (86.3%, 99.6%)
01/19 03:54:25 AM | Train: [33/100] Step 100/390 Loss 0.176 Prec@(1,5) (93.8%, 99.9%)
01/19 03:54:26 AM | Train: [32/100] Step 200/390 Loss 0.400 Prec@(1,5) (86.0%, 99.6%)
01/19 03:54:29 AM | Train: [32/100] Step 200/390 Loss 0.189 Prec@(1,5) (93.3%, 99.9%)
01/19 03:54:30 AM | Train: [32/100] Step 200/390 Loss 0.395 Prec@(1,5) (86.3%, 99.6%)
01/19 03:54:31 AM | Train: [33/100] Step 200/390 Loss 0.182 Prec@(1,5) (93.6%, 99.9%)
01/19 03:54:32 AM | Train: [32/100] Step 300/390 Loss 0.403 Prec@(1,5) (86.0%, 99.6%)
01/19 03:54:35 AM | Train: [32/100] Step 300/390 Loss 0.197 Prec@(1,5) (93.1%, 99.9%)
01/19 03:54:36 AM | Train: [32/100] Step 300/390 Loss 0.400 Prec@(1,5) (86.2%, 99.6%)
01/19 03:54:37 AM | Train: [33/100] Step 300/390 Loss 0.188 Prec@(1,5) (93.4%, 99.9%)
01/19 03:54:37 AM | Train: [32/100] Step 390/390 Loss 0.400 Prec@(1,5) (86.1%, 99.6%)
01/19 03:54:37 AM | Train: [32/100] Final Prec@1 86.1340%
01/19 03:54:38 AM | Valid: [32/100] Step 000/078 Loss 0.458 Prec@(1,5) (84.4%, 99.2%)
01/19 03:54:40 AM | Valid: [32/100] Step 078/078 Loss 0.516 Prec@(1,5) (82.5%, 99.3%)
01/19 03:54:40 AM | Valid: [32/100] Final Prec@1 82.5200%
01/19 03:54:40 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:54:40 AM | Train: [32/100] Step 390/390 Loss 0.197 Prec@(1,5) (93.2%, 99.9%)
01/19 03:54:40 AM | Train: [32/100] Final Prec@1 93.1840%
01/19 03:54:41 AM | Train: [32/100] Step 390/390 Loss 0.397 Prec@(1,5) (86.3%, 99.6%)
01/19 03:54:41 AM | Train: [33/100] Step 000/390 Loss 0.476 Prec@(1,5) (83.6%, 98.4%)
01/19 03:54:41 AM | Train: [32/100] Final Prec@1 86.3200%
01/19 03:54:41 AM | Valid: [32/100] Step 000/078 Loss 0.254 Prec@(1,5) (91.4%, 100.0%)
01/19 03:54:41 AM | Valid: [32/100] Step 000/078 Loss 0.455 Prec@(1,5) (84.4%, 99.2%)
01/19 03:54:42 AM | Train: [33/100] Step 390/390 Loss 0.193 Prec@(1,5) (93.2%, 99.9%)
01/19 03:54:42 AM | Train: [33/100] Final Prec@1 93.2100%
01/19 03:54:42 AM | Valid: [32/100] Step 078/078 Loss 0.356 Prec@(1,5) (88.2%, 99.5%)
01/19 03:54:43 AM | Valid: [33/100] Step 000/078 Loss 0.303 Prec@(1,5) (92.2%, 99.2%)
01/19 03:54:43 AM | Valid: [32/100] Final Prec@1 88.2500%
01/19 03:54:43 AM | Current best Prec@1 = 88.9200%
01/19 03:54:43 AM | Valid: [32/100] Step 078/078 Loss 0.502 Prec@(1,5) (83.4%, 99.2%)
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:54:43 AM | Valid: [32/100] Final Prec@1 83.3500%
01/19 03:54:43 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:54:43 AM | Train: [33/100] Step 000/390 Loss 0.190 Prec@(1,5) (98.4%, 99.2%)
01/19 03:54:44 AM | Train: [33/100] Step 000/390 Loss 0.445 Prec@(1,5) (82.8%, 98.4%)
01/19 03:54:44 AM | Valid: [33/100] Step 078/078 Loss 0.404 Prec@(1,5) (87.4%, 99.4%)
01/19 03:54:44 AM | Valid: [33/100] Final Prec@1 87.3900%
01/19 03:54:45 AM | Current best Prec@1 = 89.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:54:45 AM | Train: [34/100] Step 000/390 Loss 0.154 Prec@(1,5) (96.1%, 100.0%)
01/19 03:54:47 AM | Train: [33/100] Step 100/390 Loss 0.392 Prec@(1,5) (86.1%, 99.5%)
01/19 03:54:49 AM | Train: [33/100] Step 100/390 Loss 0.184 Prec@(1,5) (93.7%, 99.9%)
01/19 03:54:49 AM | Train: [33/100] Step 100/390 Loss 0.403 Prec@(1,5) (86.2%, 99.3%)
01/19 03:54:51 AM | Train: [34/100] Step 100/390 Loss 0.185 Prec@(1,5) (93.6%, 99.9%)
01/19 03:54:53 AM | Train: [33/100] Step 200/390 Loss 0.391 Prec@(1,5) (86.2%, 99.6%)
01/19 03:54:55 AM | Train: [33/100] Step 200/390 Loss 0.396 Prec@(1,5) (86.4%, 99.4%)
01/19 03:54:55 AM | Train: [33/100] Step 200/390 Loss 0.183 Prec@(1,5) (93.7%, 99.9%)
01/19 03:54:57 AM | Train: [34/100] Step 200/390 Loss 0.185 Prec@(1,5) (93.5%, 99.9%)
01/19 03:54:59 AM | Train: [33/100] Step 300/390 Loss 0.398 Prec@(1,5) (86.1%, 99.5%)
01/19 03:55:01 AM | Train: [33/100] Step 300/390 Loss 0.401 Prec@(1,5) (86.1%, 99.4%)
01/19 03:55:02 AM | Train: [33/100] Step 300/390 Loss 0.185 Prec@(1,5) (93.6%, 99.9%)
01/19 03:55:03 AM | Train: [34/100] Step 300/390 Loss 0.188 Prec@(1,5) (93.5%, 99.9%)
01/19 03:55:04 AM | Train: [33/100] Step 390/390 Loss 0.400 Prec@(1,5) (86.2%, 99.5%)
01/19 03:55:04 AM | Train: [33/100] Final Prec@1 86.1600%
01/19 03:55:05 AM | Valid: [33/100] Step 000/078 Loss 0.424 Prec@(1,5) (85.9%, 99.2%)
01/19 03:55:06 AM | Valid: [33/100] Step 078/078 Loss 0.474 Prec@(1,5) (84.4%, 99.3%)
01/19 03:55:06 AM | Valid: [33/100] Final Prec@1 84.3800%
01/19 03:55:06 AM | Train: [33/100] Step 390/390 Loss 0.402 Prec@(1,5) (86.1%, 99.4%)
01/19 03:55:07 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:55:07 AM | Train: [33/100] Final Prec@1 86.0740%
01/19 03:55:07 AM | Train: [34/100] Step 000/390 Loss 0.379 Prec@(1,5) (85.2%, 100.0%)
01/19 03:55:07 AM | Valid: [33/100] Step 000/078 Loss 0.349 Prec@(1,5) (89.1%, 99.2%)
01/19 03:55:07 AM | Train: [33/100] Step 390/390 Loss 0.188 Prec@(1,5) (93.4%, 99.9%)
01/19 03:55:07 AM | Train: [33/100] Final Prec@1 93.4360%
01/19 03:55:08 AM | Valid: [33/100] Step 000/078 Loss 0.363 Prec@(1,5) (88.3%, 100.0%)
01/19 03:55:08 AM | Train: [34/100] Step 390/390 Loss 0.190 Prec@(1,5) (93.4%, 99.9%)
01/19 03:55:08 AM | Train: [34/100] Final Prec@1 93.3640%
01/19 03:55:09 AM | Valid: [33/100] Step 078/078 Loss 0.475 Prec@(1,5) (84.0%, 99.3%)
01/19 03:55:09 AM | Valid: [33/100] Final Prec@1 84.0100%
01/19 03:55:09 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:55:09 AM | Valid: [34/100] Step 000/078 Loss 0.276 Prec@(1,5) (91.4%, 99.2%)
01/19 03:55:09 AM | Valid: [33/100] Step 078/078 Loss 0.389 Prec@(1,5) (87.5%, 99.6%)
01/19 03:55:09 AM | Valid: [33/100] Final Prec@1 87.5300%
01/19 03:55:09 AM | Train: [34/100] Step 000/390 Loss 0.415 Prec@(1,5) (83.6%, 100.0%)
01/19 03:55:10 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:55:10 AM | Train: [34/100] Step 000/390 Loss 0.200 Prec@(1,5) (93.8%, 100.0%)
01/19 03:55:10 AM | Valid: [34/100] Step 078/078 Loss 0.369 Prec@(1,5) (88.5%, 99.5%)
01/19 03:55:11 AM | Valid: [34/100] Final Prec@1 88.5400%
01/19 03:55:11 AM | Current best Prec@1 = 89.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:55:11 AM | Train: [35/100] Step 000/390 Loss 0.181 Prec@(1,5) (93.0%, 99.2%)
01/19 03:55:13 AM | Train: [34/100] Step 100/390 Loss 0.386 Prec@(1,5) (86.7%, 99.6%)
01/19 03:55:16 AM | Train: [34/100] Step 100/390 Loss 0.388 Prec@(1,5) (86.6%, 99.5%)
01/19 03:55:16 AM | Train: [34/100] Step 100/390 Loss 0.180 Prec@(1,5) (93.6%, 99.9%)
01/19 03:55:17 AM | Train: [35/100] Step 100/390 Loss 0.187 Prec@(1,5) (93.5%, 99.9%)
01/19 03:55:19 AM | Train: [34/100] Step 200/390 Loss 0.384 Prec@(1,5) (86.7%, 99.6%)
01/19 03:55:21 AM | Train: [34/100] Step 200/390 Loss 0.387 Prec@(1,5) (86.6%, 99.5%)
01/19 03:55:22 AM | Train: [34/100] Step 200/390 Loss 0.180 Prec@(1,5) (93.7%, 99.9%)
01/19 03:55:23 AM | Train: [35/100] Step 200/390 Loss 0.189 Prec@(1,5) (93.4%, 99.9%)
01/19 03:55:25 AM | Train: [34/100] Step 300/390 Loss 0.390 Prec@(1,5) (86.4%, 99.6%)
01/19 03:55:28 AM | Train: [34/100] Step 300/390 Loss 0.390 Prec@(1,5) (86.4%, 99.5%)
01/19 03:55:28 AM | Train: [34/100] Step 300/390 Loss 0.180 Prec@(1,5) (93.6%, 99.9%)
01/19 03:55:29 AM | Train: [35/100] Step 300/390 Loss 0.191 Prec@(1,5) (93.2%, 99.9%)
01/19 03:55:30 AM | Train: [34/100] Step 390/390 Loss 0.390 Prec@(1,5) (86.4%, 99.6%)
01/19 03:55:31 AM | Train: [34/100] Final Prec@1 86.4040%
01/19 03:55:31 AM | Valid: [34/100] Step 000/078 Loss 0.493 Prec@(1,5) (82.0%, 100.0%)
01/19 03:55:33 AM | Valid: [34/100] Step 078/078 Loss 0.466 Prec@(1,5) (84.4%, 99.2%)
01/19 03:55:33 AM | Valid: [34/100] Final Prec@1 84.4200%
01/19 03:55:33 AM | Train: [34/100] Step 390/390 Loss 0.392 Prec@(1,5) (86.3%, 99.5%)
01/19 03:55:33 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:55:33 AM | Train: [34/100] Final Prec@1 86.3320%
01/19 03:55:33 AM | Valid: [34/100] Step 000/078 Loss 0.445 Prec@(1,5) (83.6%, 99.2%)
01/19 03:55:34 AM | Train: [35/100] Step 000/390 Loss 0.287 Prec@(1,5) (89.1%, 100.0%)
01/19 03:55:34 AM | Train: [34/100] Step 390/390 Loss 0.185 Prec@(1,5) (93.4%, 99.9%)
01/19 03:55:34 AM | Train: [34/100] Final Prec@1 93.4420%
01/19 03:55:34 AM | Train: [35/100] Step 390/390 Loss 0.196 Prec@(1,5) (93.1%, 99.9%)
01/19 03:55:34 AM | Valid: [34/100] Step 000/078 Loss 0.267 Prec@(1,5) (92.2%, 100.0%)
01/19 03:55:34 AM | Train: [35/100] Final Prec@1 93.0720%
01/19 03:55:35 AM | Valid: [34/100] Step 078/078 Loss 0.472 Prec@(1,5) (83.8%, 99.3%)
01/19 03:55:35 AM | Valid: [35/100] Step 000/078 Loss 0.393 Prec@(1,5) (86.7%, 98.4%)
01/19 03:55:35 AM | Valid: [34/100] Final Prec@1 83.7700%
01/19 03:55:35 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:55:36 AM | Valid: [34/100] Step 078/078 Loss 0.365 Prec@(1,5) (88.3%, 99.5%)
01/19 03:55:36 AM | Train: [35/100] Step 000/390 Loss 0.315 Prec@(1,5) (88.3%, 100.0%)
01/19 03:55:36 AM | Valid: [34/100] Final Prec@1 88.2600%
01/19 03:55:36 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:55:36 AM | Valid: [35/100] Step 078/078 Loss 0.382 Prec@(1,5) (87.6%, 99.4%)
01/19 03:55:37 AM | Valid: [35/100] Final Prec@1 87.6500%
01/19 03:55:37 AM | Current best Prec@1 = 89.3400%
01/19 03:55:37 AM | Train: [35/100] Step 000/390 Loss 0.168 Prec@(1,5) (94.5%, 100.0%)
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:55:37 AM | Train: [36/100] Step 000/390 Loss 0.159 Prec@(1,5) (95.3%, 100.0%)
01/19 03:55:39 AM | Train: [35/100] Step 100/390 Loss 0.388 Prec@(1,5) (86.3%, 99.6%)
01/19 03:55:42 AM | Train: [35/100] Step 100/390 Loss 0.397 Prec@(1,5) (85.9%, 99.5%)
01/19 03:55:43 AM | Train: [35/100] Step 100/390 Loss 0.188 Prec@(1,5) (93.5%, 100.0%)
01/19 03:55:43 AM | Train: [36/100] Step 100/390 Loss 0.175 Prec@(1,5) (93.9%, 100.0%)
01/19 03:55:45 AM | Train: [35/100] Step 200/390 Loss 0.386 Prec@(1,5) (86.5%, 99.5%)
01/19 03:55:48 AM | Train: [35/100] Step 200/390 Loss 0.388 Prec@(1,5) (86.4%, 99.5%)
01/19 03:55:48 AM | Train: [35/100] Step 200/390 Loss 0.190 Prec@(1,5) (93.3%, 100.0%)
01/19 03:55:49 AM | Train: [36/100] Step 200/390 Loss 0.179 Prec@(1,5) (93.8%, 99.9%)
01/19 03:55:51 AM | Train: [35/100] Step 300/390 Loss 0.391 Prec@(1,5) (86.4%, 99.5%)
01/19 03:55:54 AM | Train: [35/100] Step 300/390 Loss 0.390 Prec@(1,5) (86.4%, 99.5%)
01/19 03:55:55 AM | Train: [35/100] Step 300/390 Loss 0.191 Prec@(1,5) (93.3%, 99.9%)
01/19 03:55:55 AM | Train: [36/100] Step 300/390 Loss 0.182 Prec@(1,5) (93.6%, 99.9%)
01/19 03:55:57 AM | Train: [35/100] Step 390/390 Loss 0.394 Prec@(1,5) (86.3%, 99.5%)
01/19 03:55:57 AM | Train: [35/100] Final Prec@1 86.2680%
01/19 03:55:57 AM | Valid: [35/100] Step 000/078 Loss 0.477 Prec@(1,5) (83.6%, 99.2%)
01/19 03:55:59 AM | Train: [35/100] Step 390/390 Loss 0.393 Prec@(1,5) (86.3%, 99.5%)
01/19 03:55:59 AM | Train: [35/100] Final Prec@1 86.3000%
01/19 03:55:59 AM | Valid: [35/100] Step 078/078 Loss 0.486 Prec@(1,5) (83.3%, 99.2%)
01/19 03:55:59 AM | Valid: [35/100] Final Prec@1 83.3300%
01/19 03:55:59 AM | Current best Prec@1 = 84.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:56:00 AM | Valid: [35/100] Step 000/078 Loss 0.451 Prec@(1,5) (84.4%, 99.2%)
01/19 03:56:00 AM | Train: [35/100] Step 390/390 Loss 0.194 Prec@(1,5) (93.2%, 99.9%)
01/19 03:56:00 AM | Train: [36/100] Step 000/390 Loss 0.344 Prec@(1,5) (89.8%, 99.2%)
01/19 03:56:00 AM | Train: [35/100] Final Prec@1 93.1960%
01/19 03:56:01 AM | Train: [36/100] Step 390/390 Loss 0.187 Prec@(1,5) (93.5%, 99.9%)
01/19 03:56:01 AM | Valid: [35/100] Step 000/078 Loss 0.399 Prec@(1,5) (88.3%, 100.0%)
01/19 03:56:01 AM | Train: [36/100] Final Prec@1 93.4840%
01/19 03:56:01 AM | Valid: [35/100] Step 078/078 Loss 0.482 Prec@(1,5) (84.1%, 99.3%)
01/19 03:56:01 AM | Valid: [35/100] Final Prec@1 84.0700%
01/19 03:56:01 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:56:01 AM | Valid: [36/100] Step 000/078 Loss 0.310 Prec@(1,5) (88.3%, 98.4%)
01/19 03:56:02 AM | Train: [36/100] Step 000/390 Loss 0.346 Prec@(1,5) (89.8%, 98.4%)
01/19 03:56:02 AM | Valid: [35/100] Step 078/078 Loss 0.371 Prec@(1,5) (88.0%, 99.6%)
01/19 03:56:02 AM | Valid: [35/100] Final Prec@1 88.0100%
01/19 03:56:02 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:56:03 AM | Valid: [36/100] Step 078/078 Loss 0.390 Prec@(1,5) (87.8%, 99.5%)
01/19 03:56:03 AM | Valid: [36/100] Final Prec@1 87.8300%
01/19 03:56:03 AM | Current best Prec@1 = 89.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:56:03 AM | Train: [36/100] Step 000/390 Loss 0.160 Prec@(1,5) (93.8%, 100.0%)
01/19 03:56:03 AM | Train: [37/100] Step 000/390 Loss 0.115 Prec@(1,5) (96.9%, 100.0%)
01/19 03:56:06 AM | Train: [36/100] Step 100/390 Loss 0.374 Prec@(1,5) (86.6%, 99.6%)
01/19 03:56:08 AM | Train: [36/100] Step 100/390 Loss 0.372 Prec@(1,5) (87.1%, 99.6%)
01/19 03:56:09 AM | Train: [37/100] Step 100/390 Loss 0.171 Prec@(1,5) (94.2%, 100.0%)
01/19 03:56:09 AM | Train: [36/100] Step 100/390 Loss 0.166 Prec@(1,5) (94.2%, 99.9%)
01/19 03:56:12 AM | Train: [36/100] Step 200/390 Loss 0.385 Prec@(1,5) (86.3%, 99.6%)
01/19 03:56:14 AM | Train: [36/100] Step 200/390 Loss 0.388 Prec@(1,5) (86.5%, 99.5%)
01/19 03:56:15 AM | Train: [37/100] Step 200/390 Loss 0.175 Prec@(1,5) (94.0%, 100.0%)
01/19 03:56:15 AM | Train: [36/100] Step 200/390 Loss 0.176 Prec@(1,5) (93.9%, 99.9%)
01/19 03:56:18 AM | Train: [36/100] Step 300/390 Loss 0.389 Prec@(1,5) (86.2%, 99.6%)
01/19 03:56:20 AM | Train: [36/100] Step 300/390 Loss 0.388 Prec@(1,5) (86.4%, 99.5%)
01/19 03:56:21 AM | Train: [37/100] Step 300/390 Loss 0.181 Prec@(1,5) (93.8%, 100.0%)
01/19 03:56:22 AM | Train: [36/100] Step 300/390 Loss 0.176 Prec@(1,5) (93.9%, 99.9%)
01/19 03:56:23 AM | Train: [36/100] Step 390/390 Loss 0.391 Prec@(1,5) (86.2%, 99.5%)
01/19 03:56:24 AM | Train: [36/100] Final Prec@1 86.2460%
01/19 03:56:24 AM | Valid: [36/100] Step 000/078 Loss 0.398 Prec@(1,5) (87.5%, 99.2%)
01/19 03:56:25 AM | Train: [36/100] Step 390/390 Loss 0.395 Prec@(1,5) (86.2%, 99.5%)
01/19 03:56:25 AM | Train: [36/100] Final Prec@1 86.2300%
01/19 03:56:26 AM | Valid: [36/100] Step 078/078 Loss 0.457 Prec@(1,5) (84.5%, 99.4%)
01/19 03:56:26 AM | Valid: [36/100] Step 000/078 Loss 0.373 Prec@(1,5) (84.4%, 100.0%)
01/19 03:56:26 AM | Valid: [36/100] Final Prec@1 84.4900%
01/19 03:56:26 AM | Current best Prec@1 = 84.4900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:56:27 AM | Train: [37/100] Step 390/390 Loss 0.183 Prec@(1,5) (93.7%, 99.9%)
01/19 03:56:27 AM | Train: [37/100] Step 000/390 Loss 0.257 Prec@(1,5) (89.8%, 100.0%)
01/19 03:56:27 AM | Train: [37/100] Final Prec@1 93.6860%
01/19 03:56:27 AM | Train: [36/100] Step 390/390 Loss 0.183 Prec@(1,5) (93.7%, 99.9%)
01/19 03:56:27 AM | Train: [36/100] Final Prec@1 93.6700%
01/19 03:56:27 AM | Valid: [37/100] Step 000/078 Loss 0.258 Prec@(1,5) (91.4%, 100.0%)
01/19 03:56:27 AM | Valid: [36/100] Step 078/078 Loss 0.490 Prec@(1,5) (83.7%, 99.2%)
01/19 03:56:27 AM | Valid: [36/100] Final Prec@1 83.6600%
01/19 03:56:27 AM | Current best Prec@1 = 84.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:56:27 AM | Valid: [36/100] Step 000/078 Loss 0.367 Prec@(1,5) (89.8%, 98.4%)
01/19 03:56:28 AM | Train: [37/100] Step 000/390 Loss 0.285 Prec@(1,5) (89.8%, 100.0%)
01/19 03:56:29 AM | Valid: [37/100] Step 078/078 Loss 0.363 Prec@(1,5) (88.4%, 99.4%)
01/19 03:56:29 AM | Valid: [37/100] Final Prec@1 88.4500%
01/19 03:56:29 AM | Current best Prec@1 = 89.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:56:29 AM | Valid: [36/100] Step 078/078 Loss 0.363 Prec@(1,5) (88.1%, 99.5%)
01/19 03:56:29 AM | Valid: [36/100] Final Prec@1 88.1200%
01/19 03:56:29 AM | Current best Prec@1 = 88.9200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:56:30 AM | Train: [38/100] Step 000/390 Loss 0.168 Prec@(1,5) (94.5%, 99.2%)
01/19 03:56:30 AM | Train: [37/100] Step 000/390 Loss 0.100 Prec@(1,5) (96.1%, 100.0%)
01/19 03:56:32 AM | Train: [37/100] Step 100/390 Loss 0.358 Prec@(1,5) (87.6%, 99.6%)
01/19 03:56:34 AM | Train: [37/100] Step 100/390 Loss 0.360 Prec@(1,5) (87.4%, 99.6%)
01/19 03:56:36 AM | Train: [37/100] Step 100/390 Loss 0.160 Prec@(1,5) (94.5%, 100.0%)
01/19 03:56:36 AM | Train: [38/100] Step 100/390 Loss 0.168 Prec@(1,5) (94.3%, 99.9%)
01/19 03:56:38 AM | Train: [37/100] Step 200/390 Loss 0.368 Prec@(1,5) (87.3%, 99.6%)
01/19 03:56:40 AM | Train: [37/100] Step 200/390 Loss 0.373 Prec@(1,5) (87.2%, 99.6%)
01/19 03:56:42 AM | Train: [37/100] Step 200/390 Loss 0.165 Prec@(1,5) (94.3%, 100.0%)
01/19 03:56:42 AM | Train: [38/100] Step 200/390 Loss 0.171 Prec@(1,5) (94.1%, 100.0%)
01/19 03:56:44 AM | Train: [37/100] Step 300/390 Loss 0.373 Prec@(1,5) (87.2%, 99.6%)
01/19 03:56:46 AM | Train: [37/100] Step 300/390 Loss 0.375 Prec@(1,5) (87.1%, 99.6%)
01/19 03:56:48 AM | Train: [37/100] Step 300/390 Loss 0.170 Prec@(1,5) (94.1%, 100.0%)
01/19 03:56:48 AM | Train: [38/100] Step 300/390 Loss 0.177 Prec@(1,5) (93.8%, 99.9%)
01/19 03:56:50 AM | Train: [37/100] Step 390/390 Loss 0.377 Prec@(1,5) (86.9%, 99.6%)
01/19 03:56:50 AM | Train: [37/100] Final Prec@1 86.9340%
01/19 03:56:51 AM | Valid: [37/100] Step 000/078 Loss 0.445 Prec@(1,5) (85.9%, 98.4%)
01/19 03:56:51 AM | Train: [37/100] Step 390/390 Loss 0.378 Prec@(1,5) (87.0%, 99.6%)
01/19 03:56:51 AM | Train: [37/100] Final Prec@1 86.9920%
01/19 03:56:52 AM | Valid: [37/100] Step 000/078 Loss 0.444 Prec@(1,5) (84.4%, 99.2%)
01/19 03:56:52 AM | Valid: [37/100] Step 078/078 Loss 0.463 Prec@(1,5) (84.6%, 99.2%)
01/19 03:56:52 AM | Valid: [37/100] Final Prec@1 84.6100%
01/19 03:56:52 AM | Current best Prec@1 = 84.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:56:53 AM | Train: [38/100] Step 000/390 Loss 0.345 Prec@(1,5) (88.3%, 99.2%)
01/19 03:56:53 AM | Train: [37/100] Step 390/390 Loss 0.173 Prec@(1,5) (94.0%, 99.9%)
01/19 03:56:53 AM | Train: [38/100] Step 390/390 Loss 0.184 Prec@(1,5) (93.6%, 99.9%)
01/19 03:56:53 AM | Train: [38/100] Final Prec@1 93.6200%
01/19 03:56:53 AM | Train: [37/100] Final Prec@1 93.9660%
01/19 03:56:53 AM | Valid: [37/100] Step 078/078 Loss 0.471 Prec@(1,5) (84.5%, 99.3%)
01/19 03:56:53 AM | Valid: [37/100] Final Prec@1 84.5200%
01/19 03:56:54 AM | Current best Prec@1 = 84.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:56:54 AM | Valid: [38/100] Step 000/078 Loss 0.359 Prec@(1,5) (89.1%, 100.0%)
01/19 03:56:54 AM | Valid: [37/100] Step 000/078 Loss 0.263 Prec@(1,5) (92.2%, 99.2%)
01/19 03:56:54 AM | Train: [38/100] Step 000/390 Loss 0.399 Prec@(1,5) (83.6%, 99.2%)
01/19 03:56:55 AM | Valid: [38/100] Step 078/078 Loss 0.370 Prec@(1,5) (88.3%, 99.5%)
01/19 03:56:55 AM | Valid: [37/100] Step 078/078 Loss 0.350 Prec@(1,5) (89.1%, 99.5%)
01/19 03:56:55 AM | Valid: [38/100] Final Prec@1 88.3300%
01/19 03:56:55 AM | Valid: [37/100] Final Prec@1 89.0600%
01/19 03:56:56 AM | Current best Prec@1 = 89.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:56:56 AM | Current best Prec@1 = 89.0600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:56:56 AM | Train: [39/100] Step 000/390 Loss 0.214 Prec@(1,5) (92.2%, 100.0%)
01/19 03:56:56 AM | Train: [38/100] Step 000/390 Loss 0.149 Prec@(1,5) (95.3%, 100.0%)
01/19 03:56:59 AM | Train: [38/100] Step 100/390 Loss 0.360 Prec@(1,5) (87.7%, 99.5%)
01/19 03:57:00 AM | Train: [38/100] Step 100/390 Loss 0.376 Prec@(1,5) (87.1%, 99.5%)
01/19 03:57:02 AM | Train: [39/100] Step 100/390 Loss 0.166 Prec@(1,5) (94.2%, 100.0%)
01/19 03:57:02 AM | Train: [38/100] Step 100/390 Loss 0.163 Prec@(1,5) (94.4%, 100.0%)
01/19 03:57:04 AM | Train: [38/100] Step 200/390 Loss 0.369 Prec@(1,5) (87.1%, 99.6%)
01/19 03:57:06 AM | Train: [38/100] Step 200/390 Loss 0.379 Prec@(1,5) (86.8%, 99.6%)
01/19 03:57:08 AM | Train: [39/100] Step 200/390 Loss 0.178 Prec@(1,5) (93.8%, 100.0%)
01/19 03:57:09 AM | Train: [38/100] Step 200/390 Loss 0.170 Prec@(1,5) (94.1%, 100.0%)
01/19 03:57:10 AM | Train: [38/100] Step 300/390 Loss 0.377 Prec@(1,5) (86.8%, 99.5%)
01/19 03:57:12 AM | Train: [38/100] Step 300/390 Loss 0.382 Prec@(1,5) (86.6%, 99.6%)
01/19 03:57:14 AM | Train: [39/100] Step 300/390 Loss 0.177 Prec@(1,5) (93.7%, 99.9%)
01/19 03:57:14 AM | Train: [38/100] Step 300/390 Loss 0.174 Prec@(1,5) (94.0%, 100.0%)
01/19 03:57:16 AM | Train: [38/100] Step 390/390 Loss 0.381 Prec@(1,5) (86.7%, 99.5%)
01/19 03:57:16 AM | Train: [38/100] Final Prec@1 86.6920%
01/19 03:57:17 AM | Valid: [38/100] Step 000/078 Loss 0.440 Prec@(1,5) (85.9%, 99.2%)
01/19 03:57:17 AM | Train: [38/100] Step 390/390 Loss 0.388 Prec@(1,5) (86.5%, 99.6%)
01/19 03:57:17 AM | Train: [38/100] Final Prec@1 86.4540%
01/19 03:57:18 AM | Valid: [38/100] Step 000/078 Loss 0.411 Prec@(1,5) (86.7%, 98.4%)
01/19 03:57:18 AM | Valid: [38/100] Step 078/078 Loss 0.472 Prec@(1,5) (84.4%, 99.4%)
01/19 03:57:18 AM | Valid: [38/100] Final Prec@1 84.3800%
01/19 03:57:19 AM | Current best Prec@1 = 84.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:57:19 AM | Valid: [38/100] Step 078/078 Loss 0.476 Prec@(1,5) (84.4%, 99.3%)
01/19 03:57:19 AM | Train: [39/100] Step 390/390 Loss 0.180 Prec@(1,5) (93.6%, 100.0%)
01/19 03:57:19 AM | Valid: [38/100] Final Prec@1 84.4000%
01/19 03:57:19 AM | Train: [39/100] Step 000/390 Loss 0.346 Prec@(1,5) (86.7%, 99.2%)
01/19 03:57:19 AM | Train: [39/100] Final Prec@1 93.6140%
01/19 03:57:19 AM | Current best Prec@1 = 84.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:57:20 AM | Valid: [39/100] Step 000/078 Loss 0.233 Prec@(1,5) (91.4%, 100.0%)
01/19 03:57:20 AM | Train: [39/100] Step 000/390 Loss 0.404 Prec@(1,5) (85.2%, 100.0%)
01/19 03:57:20 AM | Train: [38/100] Step 390/390 Loss 0.180 Prec@(1,5) (93.8%, 99.9%)
01/19 03:57:20 AM | Train: [38/100] Final Prec@1 93.7800%
01/19 03:57:21 AM | Valid: [38/100] Step 000/078 Loss 0.349 Prec@(1,5) (89.1%, 99.2%)
01/19 03:57:21 AM | Valid: [39/100] Step 078/078 Loss 0.360 Prec@(1,5) (88.5%, 99.6%)
01/19 03:57:22 AM | Valid: [39/100] Final Prec@1 88.5100%
01/19 03:57:22 AM | Current best Prec@1 = 89.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:57:22 AM | Train: [40/100] Step 000/390 Loss 0.083 Prec@(1,5) (97.7%, 100.0%)
01/19 03:57:22 AM | Valid: [38/100] Step 078/078 Loss 0.392 Prec@(1,5) (87.9%, 99.5%)
01/19 03:57:22 AM | Valid: [38/100] Final Prec@1 87.8600%
01/19 03:57:23 AM | Current best Prec@1 = 89.0600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:57:23 AM | Train: [39/100] Step 000/390 Loss 0.202 Prec@(1,5) (93.8%, 100.0%)
01/19 03:57:25 AM | Train: [39/100] Step 100/390 Loss 0.360 Prec@(1,5) (87.6%, 99.6%)
01/19 03:57:26 AM | Train: [39/100] Step 100/390 Loss 0.365 Prec@(1,5) (87.3%, 99.6%)
01/19 03:57:28 AM | Train: [40/100] Step 100/390 Loss 0.161 Prec@(1,5) (94.4%, 100.0%)
01/19 03:57:29 AM | Train: [39/100] Step 100/390 Loss 0.161 Prec@(1,5) (94.6%, 99.9%)
01/19 03:57:31 AM | Train: [39/100] Step 200/390 Loss 0.371 Prec@(1,5) (87.3%, 99.5%)
01/19 03:57:32 AM | Train: [39/100] Step 200/390 Loss 0.380 Prec@(1,5) (86.9%, 99.5%)
01/19 03:57:34 AM | Train: [40/100] Step 200/390 Loss 0.162 Prec@(1,5) (94.3%, 100.0%)
01/19 03:57:35 AM | Train: [39/100] Step 200/390 Loss 0.169 Prec@(1,5) (94.3%, 99.9%)
01/19 03:57:37 AM | Train: [39/100] Step 300/390 Loss 0.371 Prec@(1,5) (87.3%, 99.6%)
01/19 03:57:38 AM | Train: [39/100] Step 300/390 Loss 0.379 Prec@(1,5) (86.9%, 99.5%)
01/19 03:57:39 AM | Train: [40/100] Step 300/390 Loss 0.171 Prec@(1,5) (93.9%, 100.0%)
01/19 03:57:42 AM | Train: [39/100] Step 300/390 Loss 0.169 Prec@(1,5) (94.3%, 99.9%)
01/19 03:57:43 AM | Train: [39/100] Step 390/390 Loss 0.373 Prec@(1,5) (87.2%, 99.6%)
01/19 03:57:43 AM | Train: [39/100] Final Prec@1 87.1980%
01/19 03:57:43 AM | Train: [39/100] Step 390/390 Loss 0.379 Prec@(1,5) (86.8%, 99.5%)
01/19 03:57:43 AM | Train: [39/100] Final Prec@1 86.8440%
01/19 03:57:44 AM | Valid: [39/100] Step 000/078 Loss 0.420 Prec@(1,5) (85.2%, 99.2%)
01/19 03:57:44 AM | Valid: [39/100] Step 000/078 Loss 0.365 Prec@(1,5) (89.1%, 100.0%)
01/19 03:57:45 AM | Train: [40/100] Step 390/390 Loss 0.175 Prec@(1,5) (93.8%, 99.9%)
01/19 03:57:45 AM | Train: [40/100] Final Prec@1 93.7600%
01/19 03:57:45 AM | Valid: [39/100] Step 078/078 Loss 0.459 Prec@(1,5) (84.2%, 99.4%)
01/19 03:57:45 AM | Valid: [39/100] Final Prec@1 84.2300%
01/19 03:57:45 AM | Valid: [39/100] Step 078/078 Loss 0.458 Prec@(1,5) (84.5%, 99.4%)
01/19 03:57:45 AM | Current best Prec@1 = 84.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:57:45 AM | Valid: [39/100] Final Prec@1 84.5400%
01/19 03:57:46 AM | Current best Prec@1 = 84.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:57:46 AM | Valid: [40/100] Step 000/078 Loss 0.275 Prec@(1,5) (92.2%, 100.0%)
01/19 03:57:46 AM | Train: [40/100] Step 000/390 Loss 0.197 Prec@(1,5) (95.3%, 100.0%)
01/19 03:57:46 AM | Train: [40/100] Step 000/390 Loss 0.267 Prec@(1,5) (86.7%, 100.0%)
01/19 03:57:47 AM | Train: [39/100] Step 390/390 Loss 0.172 Prec@(1,5) (94.1%, 99.9%)
01/19 03:57:47 AM | Train: [39/100] Final Prec@1 94.1220%
01/19 03:57:47 AM | Valid: [40/100] Step 078/078 Loss 0.341 Prec@(1,5) (89.1%, 99.5%)
01/19 03:57:47 AM | Valid: [40/100] Final Prec@1 89.1000%
01/19 03:57:48 AM | Current best Prec@1 = 89.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:57:48 AM | Valid: [39/100] Step 000/078 Loss 0.251 Prec@(1,5) (91.4%, 100.0%)
01/19 03:57:48 AM | Train: [41/100] Step 000/390 Loss 0.204 Prec@(1,5) (92.2%, 100.0%)
01/19 03:57:49 AM | Valid: [39/100] Step 078/078 Loss 0.367 Prec@(1,5) (88.7%, 99.5%)
01/19 03:57:49 AM | Valid: [39/100] Final Prec@1 88.7300%
01/19 03:57:49 AM | Current best Prec@1 = 89.0600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:57:50 AM | Train: [40/100] Step 000/390 Loss 0.120 Prec@(1,5) (95.3%, 100.0%)
01/19 03:57:52 AM | Train: [40/100] Step 100/390 Loss 0.360 Prec@(1,5) (87.4%, 99.7%)
01/19 03:57:52 AM | Train: [40/100] Step 100/390 Loss 0.356 Prec@(1,5) (87.5%, 99.6%)
01/19 03:57:54 AM | Train: [41/100] Step 100/390 Loss 0.172 Prec@(1,5) (93.8%, 99.9%)
01/19 03:57:56 AM | Train: [40/100] Step 100/390 Loss 0.165 Prec@(1,5) (94.3%, 100.0%)
01/19 03:57:58 AM | Train: [40/100] Step 200/390 Loss 0.361 Prec@(1,5) (87.5%, 99.6%)
01/19 03:57:58 AM | Train: [40/100] Step 200/390 Loss 0.361 Prec@(1,5) (87.4%, 99.6%)
01/19 03:58:00 AM | Train: [41/100] Step 200/390 Loss 0.175 Prec@(1,5) (93.7%, 99.9%)
01/19 03:58:02 AM | Train: [40/100] Step 200/390 Loss 0.161 Prec@(1,5) (94.5%, 100.0%)
01/19 03:58:04 AM | Train: [40/100] Step 300/390 Loss 0.370 Prec@(1,5) (87.2%, 99.6%)
01/19 03:58:04 AM | Train: [40/100] Step 300/390 Loss 0.371 Prec@(1,5) (87.1%, 99.6%)
01/19 03:58:06 AM | Train: [41/100] Step 300/390 Loss 0.173 Prec@(1,5) (93.8%, 99.9%)
01/19 03:58:08 AM | Train: [40/100] Step 300/390 Loss 0.164 Prec@(1,5) (94.4%, 100.0%)
01/19 03:58:09 AM | Train: [40/100] Step 390/390 Loss 0.372 Prec@(1,5) (87.1%, 99.6%)
01/19 03:58:10 AM | Train: [40/100] Final Prec@1 87.0920%
01/19 03:58:10 AM | Train: [40/100] Step 390/390 Loss 0.371 Prec@(1,5) (87.0%, 99.6%)
01/19 03:58:10 AM | Train: [40/100] Final Prec@1 87.0120%
01/19 03:58:10 AM | Valid: [40/100] Step 000/078 Loss 0.441 Prec@(1,5) (86.7%, 99.2%)
01/19 03:58:11 AM | Valid: [40/100] Step 000/078 Loss 0.428 Prec@(1,5) (85.9%, 98.4%)
01/19 03:58:12 AM | Train: [41/100] Step 390/390 Loss 0.172 Prec@(1,5) (93.9%, 99.9%)
01/19 03:58:12 AM | Valid: [40/100] Step 078/078 Loss 0.459 Prec@(1,5) (84.6%, 99.3%)
01/19 03:58:12 AM | Train: [41/100] Final Prec@1 93.8500%
01/19 03:58:12 AM | Valid: [40/100] Final Prec@1 84.6100%
01/19 03:58:12 AM | Valid: [40/100] Step 078/078 Loss 0.453 Prec@(1,5) (84.6%, 99.3%)
01/19 03:58:12 AM | Current best Prec@1 = 84.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:58:12 AM | Valid: [40/100] Final Prec@1 84.5900%
01/19 03:58:12 AM | Valid: [41/100] Step 000/078 Loss 0.245 Prec@(1,5) (92.2%, 100.0%)
01/19 03:58:12 AM | Current best Prec@1 = 84.6100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:58:13 AM | Train: [41/100] Step 000/390 Loss 0.401 Prec@(1,5) (89.1%, 97.7%)
01/19 03:58:13 AM | Train: [41/100] Step 000/390 Loss 0.389 Prec@(1,5) (87.5%, 98.4%)
01/19 03:58:14 AM | Train: [40/100] Step 390/390 Loss 0.169 Prec@(1,5) (94.2%, 99.9%)
01/19 03:58:14 AM | Train: [40/100] Final Prec@1 94.1660%
01/19 03:58:14 AM | Valid: [41/100] Step 078/078 Loss 0.335 Prec@(1,5) (89.5%, 99.6%)
01/19 03:58:14 AM | Valid: [41/100] Final Prec@1 89.4800%
01/19 03:58:14 AM | Current best Prec@1 = 89.4800%
01/19 03:58:14 AM | Perform validation on training dataset. 
01/19 03:58:14 AM | Valid: [40/100] Step 000/078 Loss 0.341 Prec@(1,5) (89.1%, 100.0%)
01/19 03:58:15 AM | Valid on training dataset: [41/100] Step 000/390 Loss 0.164 Prec@(1,5) (92.2%, 100.0%)
01/19 03:58:16 AM | Valid: [40/100] Step 078/078 Loss 0.375 Prec@(1,5) (88.1%, 99.5%)
01/19 03:58:16 AM | Valid: [40/100] Final Prec@1 88.1300%
01/19 03:58:16 AM | Current best Prec@1 = 89.0600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:58:17 AM | Train: [41/100] Step 000/390 Loss 0.194 Prec@(1,5) (93.0%, 100.0%)
01/19 03:58:17 AM | Valid on training dataset: [41/100] Step 100/390 Loss 0.153 Prec@(1,5) (94.7%, 99.9%)
01/19 03:58:19 AM | Train: [41/100] Step 100/390 Loss 0.373 Prec@(1,5) (87.2%, 99.5%)
01/19 03:58:19 AM | Train: [41/100] Step 100/390 Loss 0.364 Prec@(1,5) (87.3%, 99.5%)
01/19 03:58:19 AM | Valid on training dataset: [41/100] Step 200/390 Loss 0.150 Prec@(1,5) (94.8%, 99.9%)
01/19 03:58:22 AM | Valid on training dataset: [41/100] Step 300/390 Loss 0.144 Prec@(1,5) (95.0%, 99.9%)
01/19 03:58:23 AM | Train: [41/100] Step 100/390 Loss 0.154 Prec@(1,5) (94.8%, 99.9%)
01/19 03:58:24 AM | Valid on training dataset: [41/100] Step 390/390 Loss 0.145 Prec@(1,5) (95.0%, 99.9%)
01/19 03:58:24 AM | Valid on training dataset: [41/100] Final Prec@1 94.9680%
01/19 03:58:24 AM | Final train Prec@1 = 94.9680%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:58:24 AM | Train: [42/100] Step 000/390 Loss 0.182 Prec@(1,5) (93.8%, 100.0%)
01/19 03:58:25 AM | Train: [41/100] Step 200/390 Loss 0.362 Prec@(1,5) (87.3%, 99.6%)
01/19 03:58:25 AM | Train: [41/100] Step 200/390 Loss 0.369 Prec@(1,5) (87.2%, 99.5%)
01/19 03:58:28 AM | Train: [41/100] Step 200/390 Loss 0.156 Prec@(1,5) (94.6%, 99.9%)
01/19 03:58:31 AM | Train: [42/100] Step 100/390 Loss 0.150 Prec@(1,5) (94.6%, 100.0%)
01/19 03:58:31 AM | Train: [41/100] Step 300/390 Loss 0.364 Prec@(1,5) (87.2%, 99.6%)
01/19 03:58:31 AM | Train: [41/100] Step 300/390 Loss 0.369 Prec@(1,5) (87.1%, 99.5%)
01/19 03:58:34 AM | Train: [41/100] Step 300/390 Loss 0.159 Prec@(1,5) (94.4%, 99.9%)
01/19 03:58:36 AM | Train: [41/100] Step 390/390 Loss 0.369 Prec@(1,5) (87.1%, 99.5%)
01/19 03:58:37 AM | Train: [41/100] Final Prec@1 87.1180%
01/19 03:58:37 AM | Train: [41/100] Step 390/390 Loss 0.365 Prec@(1,5) (87.2%, 99.6%)
01/19 03:58:37 AM | Train: [41/100] Final Prec@1 87.2200%
01/19 03:58:37 AM | Train: [42/100] Step 200/390 Loss 0.157 Prec@(1,5) (94.5%, 100.0%)
01/19 03:58:37 AM | Valid: [41/100] Step 000/078 Loss 0.317 Prec@(1,5) (88.3%, 100.0%)
01/19 03:58:37 AM | Valid: [41/100] Step 000/078 Loss 0.361 Prec@(1,5) (88.3%, 99.2%)
01/19 03:58:39 AM | Valid: [41/100] Step 078/078 Loss 0.439 Prec@(1,5) (85.2%, 99.2%)
01/19 03:58:39 AM | Valid: [41/100] Final Prec@1 85.2400%
01/19 03:58:39 AM | Current best Prec@1 = 85.2400%
01/19 03:58:39 AM | Perform validation on training dataset. 
01/19 03:58:39 AM | Valid: [41/100] Step 078/078 Loss 0.448 Prec@(1,5) (84.9%, 99.3%)
01/19 03:58:39 AM | Valid: [41/100] Final Prec@1 84.8600%
01/19 03:58:39 AM | Current best Prec@1 = 84.8600%
01/19 03:58:39 AM | Perform validation on training dataset. 
01/19 03:58:39 AM | Valid on training dataset: [41/100] Step 000/390 Loss 0.267 Prec@(1,5) (90.6%, 100.0%)
01/19 03:58:39 AM | Train: [41/100] Step 390/390 Loss 0.161 Prec@(1,5) (94.3%, 99.9%)
01/19 03:58:40 AM | Train: [41/100] Final Prec@1 94.3280%
01/19 03:58:40 AM | Valid on training dataset: [41/100] Step 000/390 Loss 0.343 Prec@(1,5) (86.7%, 99.2%)
01/19 03:58:40 AM | Valid: [41/100] Step 000/078 Loss 0.268 Prec@(1,5) (91.4%, 99.2%)
01/19 03:58:41 AM | Valid on training dataset: [41/100] Step 100/390 Loss 0.351 Prec@(1,5) (88.1%, 99.6%)
01/19 03:58:42 AM | Valid: [41/100] Step 078/078 Loss 0.348 Prec@(1,5) (89.0%, 99.6%)
01/19 03:58:42 AM | Valid on training dataset: [41/100] Step 100/390 Loss 0.335 Prec@(1,5) (88.3%, 99.7%)
01/19 03:58:42 AM | Train: [42/100] Step 300/390 Loss 0.162 Prec@(1,5) (94.3%, 100.0%)
01/19 03:58:42 AM | Valid: [41/100] Final Prec@1 89.0300%
01/19 03:58:42 AM | Current best Prec@1 = 89.0600%
01/19 03:58:42 AM | Perform validation on training dataset. 
01/19 03:58:43 AM | Valid on training dataset: [41/100] Step 000/390 Loss 0.222 Prec@(1,5) (90.6%, 100.0%)
01/19 03:58:43 AM | Valid on training dataset: [41/100] Step 200/390 Loss 0.345 Prec@(1,5) (88.2%, 99.6%)
01/19 03:58:44 AM | Valid on training dataset: [41/100] Step 200/390 Loss 0.330 Prec@(1,5) (88.4%, 99.7%)
01/19 03:58:45 AM | Valid on training dataset: [41/100] Step 300/390 Loss 0.341 Prec@(1,5) (88.2%, 99.6%)
01/19 03:58:45 AM | Valid on training dataset: [41/100] Step 100/390 Loss 0.154 Prec@(1,5) (94.6%, 100.0%)
01/19 03:58:46 AM | Valid on training dataset: [41/100] Step 300/390 Loss 0.327 Prec@(1,5) (88.5%, 99.7%)
01/19 03:58:47 AM | Train: [42/100] Step 390/390 Loss 0.165 Prec@(1,5) (94.2%, 100.0%)
01/19 03:58:47 AM | Train: [42/100] Final Prec@1 94.1700%
01/19 03:58:47 AM | Valid on training dataset: [41/100] Step 390/390 Loss 0.342 Prec@(1,5) (88.2%, 99.7%)
01/19 03:58:47 AM | Valid on training dataset: [41/100] Final Prec@1 88.2200%
01/19 03:58:47 AM | Final train Prec@1 = 88.2200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:58:48 AM | Valid on training dataset: [41/100] Step 200/390 Loss 0.155 Prec@(1,5) (94.6%, 100.0%)
01/19 03:58:48 AM | Valid: [42/100] Step 000/078 Loss 0.328 Prec@(1,5) (88.3%, 100.0%)
01/19 03:58:48 AM | Train: [42/100] Step 000/390 Loss 0.276 Prec@(1,5) (91.4%, 99.2%)
01/19 03:58:49 AM | Valid on training dataset: [41/100] Step 390/390 Loss 0.329 Prec@(1,5) (88.5%, 99.7%)
01/19 03:58:49 AM | Valid on training dataset: [41/100] Final Prec@1 88.4780%
01/19 03:58:49 AM | Final train Prec@1 = 88.4780%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:58:49 AM | Valid: [42/100] Step 078/078 Loss 0.341 Prec@(1,5) (89.1%, 99.6%)
01/19 03:58:49 AM | Train: [42/100] Step 000/390 Loss 0.315 Prec@(1,5) (89.8%, 100.0%)
01/19 03:58:50 AM | Valid: [42/100] Final Prec@1 89.1200%
01/19 03:58:50 AM | Valid on training dataset: [41/100] Step 300/390 Loss 0.153 Prec@(1,5) (94.6%, 100.0%)
01/19 03:58:50 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:58:50 AM | Train: [43/100] Step 000/390 Loss 0.138 Prec@(1,5) (95.3%, 100.0%)
01/19 03:58:52 AM | Valid on training dataset: [41/100] Step 390/390 Loss 0.154 Prec@(1,5) (94.6%, 100.0%)
01/19 03:58:52 AM | Valid on training dataset: [41/100] Final Prec@1 94.5900%
01/19 03:58:52 AM | Final train Prec@1 = 94.5900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:58:52 AM | Train: [42/100] Step 000/390 Loss 0.196 Prec@(1,5) (93.8%, 100.0%)
01/19 03:58:53 AM | Train: [42/100] Step 100/390 Loss 0.349 Prec@(1,5) (88.2%, 99.6%)
01/19 03:58:56 AM | Train: [42/100] Step 100/390 Loss 0.348 Prec@(1,5) (87.9%, 99.7%)
01/19 03:58:56 AM | Train: [43/100] Step 100/390 Loss 0.163 Prec@(1,5) (94.5%, 100.0%)
01/19 03:58:59 AM | Train: [42/100] Step 100/390 Loss 0.154 Prec@(1,5) (94.7%, 100.0%)
01/19 03:59:00 AM | Train: [42/100] Step 200/390 Loss 0.358 Prec@(1,5) (87.7%, 99.6%)
01/19 03:59:02 AM | Train: [42/100] Step 200/390 Loss 0.358 Prec@(1,5) (87.5%, 99.7%)
01/19 03:59:03 AM | Train: [43/100] Step 200/390 Loss 0.165 Prec@(1,5) (94.2%, 99.9%)
01/19 03:59:05 AM | Train: [42/100] Step 200/390 Loss 0.154 Prec@(1,5) (94.6%, 100.0%)
01/19 03:59:06 AM | Train: [42/100] Step 300/390 Loss 0.363 Prec@(1,5) (87.5%, 99.6%)
01/19 03:59:08 AM | Train: [42/100] Step 300/390 Loss 0.364 Prec@(1,5) (87.3%, 99.6%)
01/19 03:59:08 AM | Train: [43/100] Step 300/390 Loss 0.163 Prec@(1,5) (94.3%, 100.0%)
01/19 03:59:11 AM | Train: [42/100] Step 300/390 Loss 0.160 Prec@(1,5) (94.4%, 100.0%)
01/19 03:59:11 AM | Train: [42/100] Step 390/390 Loss 0.365 Prec@(1,5) (87.4%, 99.6%)
01/19 03:59:11 AM | Train: [42/100] Final Prec@1 87.4340%
01/19 03:59:12 AM | Valid: [42/100] Step 000/078 Loss 0.422 Prec@(1,5) (85.9%, 99.2%)
01/19 03:59:13 AM | Valid: [42/100] Step 078/078 Loss 0.444 Prec@(1,5) (85.0%, 99.3%)
01/19 03:59:13 AM | Train: [42/100] Step 390/390 Loss 0.365 Prec@(1,5) (87.3%, 99.6%)
01/19 03:59:13 AM | Valid: [42/100] Final Prec@1 85.0400%
01/19 03:59:13 AM | Train: [42/100] Final Prec@1 87.2660%
01/19 03:59:13 AM | Current best Prec@1 = 85.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:59:14 AM | Train: [43/100] Step 390/390 Loss 0.165 Prec@(1,5) (94.2%, 99.9%)
01/19 03:59:14 AM | Valid: [42/100] Step 000/078 Loss 0.416 Prec@(1,5) (84.4%, 99.2%)
01/19 03:59:14 AM | Train: [43/100] Final Prec@1 94.2280%
01/19 03:59:14 AM | Train: [43/100] Step 000/390 Loss 0.342 Prec@(1,5) (91.4%, 99.2%)
01/19 03:59:15 AM | Valid: [43/100] Step 000/078 Loss 0.256 Prec@(1,5) (91.4%, 100.0%)
01/19 03:59:15 AM | Valid: [42/100] Step 078/078 Loss 0.437 Prec@(1,5) (85.2%, 99.5%)
01/19 03:59:16 AM | Valid: [42/100] Final Prec@1 85.1700%
01/19 03:59:16 AM | Current best Prec@1 = 85.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:59:16 AM | Valid: [43/100] Step 078/078 Loss 0.358 Prec@(1,5) (89.0%, 99.5%)
01/19 03:59:16 AM | Valid: [43/100] Final Prec@1 88.9800%
01/19 03:59:16 AM | Train: [42/100] Step 390/390 Loss 0.161 Prec@(1,5) (94.3%, 100.0%)
01/19 03:59:16 AM | Train: [43/100] Step 000/390 Loss 0.418 Prec@(1,5) (87.5%, 99.2%)
01/19 03:59:16 AM | Train: [42/100] Final Prec@1 94.2840%
01/19 03:59:16 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:59:17 AM | Train: [44/100] Step 000/390 Loss 0.250 Prec@(1,5) (93.8%, 99.2%)
01/19 03:59:17 AM | Valid: [42/100] Step 000/078 Loss 0.359 Prec@(1,5) (92.2%, 100.0%)
01/19 03:59:19 AM | Valid: [42/100] Step 078/078 Loss 0.341 Prec@(1,5) (89.4%, 99.5%)
01/19 03:59:19 AM | Valid: [42/100] Final Prec@1 89.4000%
01/19 03:59:19 AM | Current best Prec@1 = 89.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:59:19 AM | Train: [43/100] Step 100/390 Loss 0.351 Prec@(1,5) (87.5%, 99.7%)
01/19 03:59:20 AM | Train: [43/100] Step 000/390 Loss 0.172 Prec@(1,5) (92.2%, 100.0%)
01/19 03:59:22 AM | Train: [43/100] Step 100/390 Loss 0.360 Prec@(1,5) (87.3%, 99.6%)
01/19 03:59:23 AM | Train: [44/100] Step 100/390 Loss 0.156 Prec@(1,5) (94.4%, 100.0%)
01/19 03:59:26 AM | Train: [43/100] Step 100/390 Loss 0.156 Prec@(1,5) (94.6%, 100.0%)
01/19 03:59:26 AM | Train: [43/100] Step 200/390 Loss 0.353 Prec@(1,5) (87.5%, 99.6%)
01/19 03:59:28 AM | Train: [43/100] Step 200/390 Loss 0.356 Prec@(1,5) (87.5%, 99.6%)
01/19 03:59:29 AM | Train: [44/100] Step 200/390 Loss 0.162 Prec@(1,5) (94.3%, 99.9%)
01/19 03:59:32 AM | Train: [43/100] Step 300/390 Loss 0.355 Prec@(1,5) (87.5%, 99.6%)
01/19 03:59:32 AM | Train: [43/100] Step 200/390 Loss 0.159 Prec@(1,5) (94.4%, 100.0%)
01/19 03:59:34 AM | Train: [43/100] Step 300/390 Loss 0.354 Prec@(1,5) (87.6%, 99.6%)
01/19 03:59:35 AM | Train: [44/100] Step 300/390 Loss 0.164 Prec@(1,5) (94.2%, 100.0%)
01/19 03:59:37 AM | Train: [43/100] Step 390/390 Loss 0.362 Prec@(1,5) (87.3%, 99.6%)
01/19 03:59:37 AM | Train: [43/100] Final Prec@1 87.3360%
01/19 03:59:38 AM | Train: [43/100] Step 300/390 Loss 0.160 Prec@(1,5) (94.4%, 100.0%)
01/19 03:59:38 AM | Valid: [43/100] Step 000/078 Loss 0.382 Prec@(1,5) (86.7%, 99.2%)
01/19 03:59:39 AM | Valid: [43/100] Step 078/078 Loss 0.449 Prec@(1,5) (84.9%, 99.4%)
01/19 03:59:39 AM | Valid: [43/100] Final Prec@1 84.9300%
01/19 03:59:39 AM | Train: [43/100] Step 390/390 Loss 0.362 Prec@(1,5) (87.4%, 99.6%)
01/19 03:59:39 AM | Current best Prec@1 = 85.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 03:59:40 AM | Train: [43/100] Final Prec@1 87.3540%
01/19 03:59:40 AM | Train: [44/100] Step 000/390 Loss 0.455 Prec@(1,5) (86.7%, 100.0%)
01/19 03:59:40 AM | Valid: [43/100] Step 000/078 Loss 0.492 Prec@(1,5) (83.6%, 100.0%)
01/19 03:59:40 AM | Train: [44/100] Step 390/390 Loss 0.164 Prec@(1,5) (94.2%, 100.0%)
01/19 03:59:41 AM | Train: [44/100] Final Prec@1 94.2260%
01/19 03:59:41 AM | Valid: [44/100] Step 000/078 Loss 0.269 Prec@(1,5) (89.8%, 99.2%)
01/19 03:59:42 AM | Valid: [43/100] Step 078/078 Loss 0.527 Prec@(1,5) (82.3%, 99.4%)
01/19 03:59:42 AM | Valid: [43/100] Final Prec@1 82.3100%
01/19 03:59:42 AM | Current best Prec@1 = 85.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 03:59:42 AM | Train: [44/100] Step 000/390 Loss 0.418 Prec@(1,5) (85.2%, 98.4%)
01/19 03:59:43 AM | Valid: [44/100] Step 078/078 Loss 0.358 Prec@(1,5) (89.2%, 99.7%)
01/19 03:59:43 AM | Valid: [44/100] Final Prec@1 89.1900%
01/19 03:59:43 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 03:59:43 AM | Train: [43/100] Step 390/390 Loss 0.164 Prec@(1,5) (94.2%, 100.0%)
01/19 03:59:43 AM | Train: [43/100] Final Prec@1 94.2140%
01/19 03:59:43 AM | Train: [45/100] Step 000/390 Loss 0.142 Prec@(1,5) (94.5%, 100.0%)
01/19 03:59:44 AM | Valid: [43/100] Step 000/078 Loss 0.249 Prec@(1,5) (91.4%, 99.2%)
01/19 03:59:45 AM | Valid: [43/100] Step 078/078 Loss 0.368 Prec@(1,5) (88.6%, 99.6%)
01/19 03:59:46 AM | Train: [44/100] Step 100/390 Loss 0.349 Prec@(1,5) (87.7%, 99.7%)
01/19 03:59:46 AM | Valid: [43/100] Final Prec@1 88.5700%
01/19 03:59:46 AM | Current best Prec@1 = 89.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 03:59:46 AM | Train: [44/100] Step 000/390 Loss 0.255 Prec@(1,5) (92.2%, 100.0%)
01/19 03:59:48 AM | Train: [44/100] Step 100/390 Loss 0.351 Prec@(1,5) (87.9%, 99.7%)
01/19 03:59:49 AM | Train: [45/100] Step 100/390 Loss 0.150 Prec@(1,5) (94.7%, 100.0%)
01/19 03:59:52 AM | Train: [44/100] Step 200/390 Loss 0.352 Prec@(1,5) (87.8%, 99.6%)
01/19 03:59:52 AM | Train: [44/100] Step 100/390 Loss 0.144 Prec@(1,5) (95.1%, 99.9%)
01/19 03:59:55 AM | Train: [44/100] Step 200/390 Loss 0.353 Prec@(1,5) (87.8%, 99.6%)
01/19 03:59:55 AM | Train: [45/100] Step 200/390 Loss 0.155 Prec@(1,5) (94.7%, 99.9%)
01/19 03:59:58 AM | Train: [44/100] Step 300/390 Loss 0.356 Prec@(1,5) (87.7%, 99.6%)
01/19 03:59:59 AM | Train: [44/100] Step 200/390 Loss 0.148 Prec@(1,5) (95.0%, 99.9%)
01/19 04:00:01 AM | Train: [44/100] Step 300/390 Loss 0.355 Prec@(1,5) (87.7%, 99.6%)
01/19 04:00:01 AM | Train: [45/100] Step 300/390 Loss 0.156 Prec@(1,5) (94.6%, 99.9%)
01/19 04:00:03 AM | Train: [44/100] Step 390/390 Loss 0.357 Prec@(1,5) (87.5%, 99.6%)
01/19 04:00:03 AM | Train: [44/100] Final Prec@1 87.5000%
01/19 04:00:04 AM | Valid: [44/100] Step 000/078 Loss 0.342 Prec@(1,5) (88.3%, 100.0%)
01/19 04:00:05 AM | Train: [44/100] Step 300/390 Loss 0.154 Prec@(1,5) (94.8%, 99.9%)
01/19 04:00:06 AM | Valid: [44/100] Step 078/078 Loss 0.454 Prec@(1,5) (84.7%, 99.3%)
01/19 04:00:06 AM | Valid: [44/100] Final Prec@1 84.6900%
01/19 04:00:06 AM | Current best Prec@1 = 85.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:00:06 AM | Train: [44/100] Step 390/390 Loss 0.355 Prec@(1,5) (87.6%, 99.7%)
01/19 04:00:06 AM | Train: [44/100] Final Prec@1 87.6080%
01/19 04:00:06 AM | Train: [45/100] Step 390/390 Loss 0.160 Prec@(1,5) (94.5%, 99.9%)
01/19 04:00:06 AM | Train: [45/100] Final Prec@1 94.5000%
01/19 04:00:07 AM | Train: [45/100] Step 000/390 Loss 0.322 Prec@(1,5) (89.1%, 100.0%)
01/19 04:00:07 AM | Valid: [44/100] Step 000/078 Loss 0.505 Prec@(1,5) (81.2%, 99.2%)
01/19 04:00:07 AM | Valid: [45/100] Step 000/078 Loss 0.222 Prec@(1,5) (93.0%, 99.2%)
01/19 04:00:09 AM | Valid: [45/100] Step 078/078 Loss 0.366 Prec@(1,5) (88.9%, 99.5%)
01/19 04:00:09 AM | Valid: [44/100] Step 078/078 Loss 0.450 Prec@(1,5) (84.7%, 99.4%)
01/19 04:00:09 AM | Valid: [45/100] Final Prec@1 88.9100%
01/19 04:00:09 AM | Valid: [44/100] Final Prec@1 84.7300%
01/19 04:00:09 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:00:09 AM | Current best Prec@1 = 85.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:00:10 AM | Train: [46/100] Step 000/390 Loss 0.128 Prec@(1,5) (95.3%, 100.0%)
01/19 04:00:10 AM | Train: [45/100] Step 000/390 Loss 0.331 Prec@(1,5) (85.9%, 100.0%)
01/19 04:00:10 AM | Train: [44/100] Step 390/390 Loss 0.156 Prec@(1,5) (94.7%, 99.9%)
01/19 04:00:10 AM | Train: [44/100] Final Prec@1 94.7220%
01/19 04:00:11 AM | Valid: [44/100] Step 000/078 Loss 0.267 Prec@(1,5) (90.6%, 100.0%)
01/19 04:00:13 AM | Train: [45/100] Step 100/390 Loss 0.349 Prec@(1,5) (87.8%, 99.6%)
01/19 04:00:13 AM | Valid: [44/100] Step 078/078 Loss 0.356 Prec@(1,5) (89.1%, 99.6%)
01/19 04:00:13 AM | Valid: [44/100] Final Prec@1 89.0800%
01/19 04:00:13 AM | Current best Prec@1 = 89.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:00:13 AM | Train: [45/100] Step 000/390 Loss 0.111 Prec@(1,5) (98.4%, 100.0%)
01/19 04:00:15 AM | Train: [46/100] Step 100/390 Loss 0.145 Prec@(1,5) (94.9%, 100.0%)
01/19 04:00:16 AM | Train: [45/100] Step 100/390 Loss 0.346 Prec@(1,5) (87.7%, 99.6%)
01/19 04:00:19 AM | Train: [45/100] Step 200/390 Loss 0.352 Prec@(1,5) (87.8%, 99.6%)
01/19 04:00:19 AM | Train: [45/100] Step 100/390 Loss 0.148 Prec@(1,5) (94.8%, 100.0%)
01/19 04:00:21 AM | Train: [46/100] Step 200/390 Loss 0.148 Prec@(1,5) (94.7%, 100.0%)
01/19 04:00:22 AM | Train: [45/100] Step 200/390 Loss 0.346 Prec@(1,5) (87.7%, 99.6%)
01/19 04:00:25 AM | Train: [45/100] Step 300/390 Loss 0.354 Prec@(1,5) (87.7%, 99.6%)
01/19 04:00:25 AM | Train: [45/100] Step 200/390 Loss 0.147 Prec@(1,5) (94.9%, 100.0%)
01/19 04:00:27 AM | Train: [46/100] Step 300/390 Loss 0.154 Prec@(1,5) (94.6%, 100.0%)
01/19 04:00:28 AM | Train: [45/100] Step 300/390 Loss 0.350 Prec@(1,5) (87.7%, 99.6%)
01/19 04:00:30 AM | Train: [45/100] Step 390/390 Loss 0.355 Prec@(1,5) (87.6%, 99.6%)
01/19 04:00:30 AM | Train: [45/100] Final Prec@1 87.6220%
01/19 04:00:31 AM | Valid: [45/100] Step 000/078 Loss 0.376 Prec@(1,5) (87.5%, 97.7%)
01/19 04:00:31 AM | Train: [45/100] Step 300/390 Loss 0.150 Prec@(1,5) (94.8%, 100.0%)
01/19 04:00:32 AM | Valid: [45/100] Step 078/078 Loss 0.470 Prec@(1,5) (84.4%, 99.3%)
01/19 04:00:32 AM | Valid: [45/100] Final Prec@1 84.3500%
01/19 04:00:32 AM | Train: [46/100] Step 390/390 Loss 0.154 Prec@(1,5) (94.6%, 100.0%)
01/19 04:00:32 AM | Train: [46/100] Final Prec@1 94.6080%
01/19 04:00:32 AM | Current best Prec@1 = 85.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:00:33 AM | Train: [45/100] Step 390/390 Loss 0.353 Prec@(1,5) (87.7%, 99.6%)
01/19 04:00:33 AM | Valid: [46/100] Step 000/078 Loss 0.186 Prec@(1,5) (91.4%, 100.0%)
01/19 04:00:33 AM | Train: [46/100] Step 000/390 Loss 0.259 Prec@(1,5) (91.4%, 100.0%)
01/19 04:00:33 AM | Train: [45/100] Final Prec@1 87.6660%
01/19 04:00:34 AM | Valid: [45/100] Step 000/078 Loss 0.414 Prec@(1,5) (87.5%, 99.2%)
01/19 04:00:35 AM | Valid: [46/100] Step 078/078 Loss 0.346 Prec@(1,5) (89.2%, 99.4%)
01/19 04:00:35 AM | Valid: [46/100] Final Prec@1 89.1700%
01/19 04:00:35 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:00:35 AM | Valid: [45/100] Step 078/078 Loss 0.474 Prec@(1,5) (84.2%, 99.1%)
01/19 04:00:35 AM | Valid: [45/100] Final Prec@1 84.1800%
01/19 04:00:36 AM | Train: [47/100] Step 000/390 Loss 0.173 Prec@(1,5) (94.5%, 100.0%)
01/19 04:00:36 AM | Current best Prec@1 = 85.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:00:36 AM | Train: [46/100] Step 000/390 Loss 0.270 Prec@(1,5) (90.6%, 100.0%)
01/19 04:00:36 AM | Train: [45/100] Step 390/390 Loss 0.152 Prec@(1,5) (94.7%, 100.0%)
01/19 04:00:36 AM | Train: [45/100] Final Prec@1 94.7260%
01/19 04:00:37 AM | Valid: [45/100] Step 000/078 Loss 0.289 Prec@(1,5) (89.8%, 99.2%)
01/19 04:00:39 AM | Valid: [45/100] Step 078/078 Loss 0.360 Prec@(1,5) (88.7%, 99.5%)
01/19 04:00:39 AM | Valid: [45/100] Final Prec@1 88.6900%
01/19 04:00:39 AM | Train: [46/100] Step 100/390 Loss 0.343 Prec@(1,5) (88.1%, 99.6%)
01/19 04:00:39 AM | Current best Prec@1 = 89.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:00:40 AM | Train: [46/100] Step 000/390 Loss 0.115 Prec@(1,5) (95.3%, 100.0%)
01/19 04:00:41 AM | Train: [47/100] Step 100/390 Loss 0.136 Prec@(1,5) (95.3%, 99.9%)
01/19 04:00:42 AM | Train: [46/100] Step 100/390 Loss 0.343 Prec@(1,5) (88.0%, 99.7%)
01/19 04:00:45 AM | Train: [46/100] Step 200/390 Loss 0.345 Prec@(1,5) (87.9%, 99.7%)
01/19 04:00:46 AM | Train: [46/100] Step 100/390 Loss 0.138 Prec@(1,5) (95.3%, 99.9%)
01/19 04:00:47 AM | Train: [47/100] Step 200/390 Loss 0.140 Prec@(1,5) (95.2%, 100.0%)
01/19 04:00:48 AM | Train: [46/100] Step 200/390 Loss 0.345 Prec@(1,5) (87.9%, 99.7%)
01/19 04:00:51 AM | Train: [46/100] Step 300/390 Loss 0.349 Prec@(1,5) (87.8%, 99.6%)
01/19 04:00:52 AM | Train: [46/100] Step 200/390 Loss 0.141 Prec@(1,5) (95.2%, 100.0%)
01/19 04:00:53 AM | Train: [47/100] Step 300/390 Loss 0.142 Prec@(1,5) (95.1%, 99.9%)
01/19 04:00:55 AM | Train: [46/100] Step 300/390 Loss 0.348 Prec@(1,5) (87.8%, 99.7%)
01/19 04:00:56 AM | Train: [46/100] Step 390/390 Loss 0.351 Prec@(1,5) (87.7%, 99.6%)
01/19 04:00:57 AM | Train: [46/100] Final Prec@1 87.6840%
01/19 04:00:57 AM | Valid: [46/100] Step 000/078 Loss 0.313 Prec@(1,5) (88.3%, 100.0%)
01/19 04:00:58 AM | Train: [46/100] Step 300/390 Loss 0.148 Prec@(1,5) (94.9%, 100.0%)
01/19 04:00:58 AM | Train: [47/100] Step 390/390 Loss 0.148 Prec@(1,5) (94.9%, 99.9%)
01/19 04:00:59 AM | Train: [47/100] Final Prec@1 94.8920%
01/19 04:00:59 AM | Valid: [46/100] Step 078/078 Loss 0.445 Prec@(1,5) (85.1%, 99.4%)
01/19 04:00:59 AM | Valid: [46/100] Final Prec@1 85.0700%
01/19 04:00:59 AM | Current best Prec@1 = 85.2400%
01/19 04:00:59 AM | Valid: [47/100] Step 000/078 Loss 0.289 Prec@(1,5) (93.0%, 100.0%)
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:01:00 AM | Train: [46/100] Step 390/390 Loss 0.352 Prec@(1,5) (87.7%, 99.7%)
01/19 04:01:00 AM | Train: [46/100] Final Prec@1 87.6820%
01/19 04:01:00 AM | Train: [47/100] Step 000/390 Loss 0.460 Prec@(1,5) (83.6%, 99.2%)
01/19 04:01:00 AM | Valid: [46/100] Step 000/078 Loss 0.319 Prec@(1,5) (89.8%, 100.0%)
01/19 04:01:01 AM | Valid: [47/100] Step 078/078 Loss 0.386 Prec@(1,5) (88.5%, 99.5%)
01/19 04:01:01 AM | Valid: [47/100] Final Prec@1 88.5400%
01/19 04:01:01 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:01:02 AM | Valid: [46/100] Step 078/078 Loss 0.434 Prec@(1,5) (85.6%, 99.4%)
01/19 04:01:02 AM | Valid: [46/100] Final Prec@1 85.5600%
01/19 04:01:02 AM | Train: [48/100] Step 000/390 Loss 0.116 Prec@(1,5) (94.5%, 100.0%)
01/19 04:01:02 AM | Current best Prec@1 = 85.5600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:01:03 AM | Train: [47/100] Step 000/390 Loss 0.372 Prec@(1,5) (85.2%, 99.2%)
01/19 04:01:03 AM | Train: [46/100] Step 390/390 Loss 0.152 Prec@(1,5) (94.7%, 100.0%)
01/19 04:01:03 AM | Train: [46/100] Final Prec@1 94.7320%
01/19 04:01:03 AM | Valid: [46/100] Step 000/078 Loss 0.308 Prec@(1,5) (89.8%, 100.0%)
01/19 04:01:05 AM | Valid: [46/100] Step 078/078 Loss 0.364 Prec@(1,5) (88.8%, 99.5%)
01/19 04:01:05 AM | Valid: [46/100] Final Prec@1 88.8400%
01/19 04:01:05 AM | Train: [47/100] Step 100/390 Loss 0.339 Prec@(1,5) (88.2%, 99.7%)
01/19 04:01:05 AM | Current best Prec@1 = 89.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:01:06 AM | Train: [47/100] Step 000/390 Loss 0.191 Prec@(1,5) (93.8%, 100.0%)
01/19 04:01:08 AM | Train: [48/100] Step 100/390 Loss 0.146 Prec@(1,5) (95.0%, 100.0%)
01/19 04:01:09 AM | Train: [47/100] Step 100/390 Loss 0.336 Prec@(1,5) (88.4%, 99.7%)
01/19 04:01:11 AM | Train: [47/100] Step 200/390 Loss 0.342 Prec@(1,5) (87.9%, 99.7%)
01/19 04:01:12 AM | Train: [47/100] Step 100/390 Loss 0.132 Prec@(1,5) (95.4%, 100.0%)
01/19 04:01:14 AM | Train: [48/100] Step 200/390 Loss 0.145 Prec@(1,5) (95.0%, 100.0%)
01/19 04:01:15 AM | Train: [47/100] Step 200/390 Loss 0.342 Prec@(1,5) (88.1%, 99.7%)
01/19 04:01:17 AM | Train: [47/100] Step 300/390 Loss 0.343 Prec@(1,5) (87.9%, 99.7%)
01/19 04:01:18 AM | Train: [47/100] Step 200/390 Loss 0.134 Prec@(1,5) (95.4%, 100.0%)
01/19 04:01:20 AM | Train: [48/100] Step 300/390 Loss 0.146 Prec@(1,5) (94.9%, 100.0%)
01/19 04:01:21 AM | Train: [47/100] Step 300/390 Loss 0.340 Prec@(1,5) (88.2%, 99.7%)
01/19 04:01:23 AM | Train: [47/100] Step 390/390 Loss 0.346 Prec@(1,5) (87.8%, 99.7%)
01/19 04:01:23 AM | Train: [47/100] Final Prec@1 87.7620%
01/19 04:01:23 AM | Valid: [47/100] Step 000/078 Loss 0.402 Prec@(1,5) (84.4%, 100.0%)
01/19 04:01:24 AM | Train: [47/100] Step 300/390 Loss 0.136 Prec@(1,5) (95.4%, 100.0%)
01/19 04:01:25 AM | Valid: [47/100] Step 078/078 Loss 0.474 Prec@(1,5) (84.3%, 99.3%)
01/19 04:01:25 AM | Valid: [47/100] Final Prec@1 84.2600%
01/19 04:01:25 AM | Current best Prec@1 = 85.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:01:26 AM | Train: [48/100] Step 000/390 Loss 0.268 Prec@(1,5) (91.4%, 99.2%)
01/19 04:01:26 AM | Train: [48/100] Step 390/390 Loss 0.150 Prec@(1,5) (94.8%, 100.0%)
01/19 04:01:26 AM | Train: [48/100] Final Prec@1 94.7980%
01/19 04:01:26 AM | Train: [47/100] Step 390/390 Loss 0.345 Prec@(1,5) (88.0%, 99.7%)
01/19 04:01:26 AM | Train: [47/100] Final Prec@1 88.0300%
01/19 04:01:26 AM | Valid: [48/100] Step 000/078 Loss 0.288 Prec@(1,5) (89.8%, 99.2%)
01/19 04:01:27 AM | Valid: [47/100] Step 000/078 Loss 0.533 Prec@(1,5) (75.8%, 100.0%)
01/19 04:01:28 AM | Valid: [48/100] Step 078/078 Loss 0.381 Prec@(1,5) (88.6%, 99.5%)
01/19 04:01:28 AM | Valid: [48/100] Final Prec@1 88.5900%
01/19 04:01:28 AM | Valid: [47/100] Step 078/078 Loss 0.500 Prec@(1,5) (84.0%, 99.2%)
01/19 04:01:28 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:01:28 AM | Valid: [47/100] Final Prec@1 83.9700%
01/19 04:01:29 AM | Train: [47/100] Step 390/390 Loss 0.143 Prec@(1,5) (95.1%, 100.0%)
01/19 04:01:29 AM | Current best Prec@1 = 85.5600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:01:29 AM | Train: [47/100] Final Prec@1 95.1120%
01/19 04:01:29 AM | Train: [49/100] Step 000/390 Loss 0.177 Prec@(1,5) (93.8%, 100.0%)
01/19 04:01:29 AM | Train: [48/100] Step 000/390 Loss 0.225 Prec@(1,5) (94.5%, 100.0%)
01/19 04:01:29 AM | Valid: [47/100] Step 000/078 Loss 0.245 Prec@(1,5) (90.6%, 100.0%)
01/19 04:01:31 AM | Valid: [47/100] Step 078/078 Loss 0.362 Prec@(1,5) (88.9%, 99.5%)
01/19 04:01:31 AM | Valid: [47/100] Final Prec@1 88.9300%
01/19 04:01:31 AM | Current best Prec@1 = 89.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:01:32 AM | Train: [48/100] Step 100/390 Loss 0.338 Prec@(1,5) (88.2%, 99.7%)
01/19 04:01:32 AM | Train: [48/100] Step 000/390 Loss 0.076 Prec@(1,5) (97.7%, 100.0%)
01/19 04:01:35 AM | Train: [49/100] Step 100/390 Loss 0.152 Prec@(1,5) (94.9%, 100.0%)
01/19 04:01:35 AM | Train: [48/100] Step 100/390 Loss 0.338 Prec@(1,5) (88.1%, 99.6%)
01/19 04:01:38 AM | Train: [48/100] Step 100/390 Loss 0.139 Prec@(1,5) (95.0%, 100.0%)
01/19 04:01:38 AM | Train: [48/100] Step 200/390 Loss 0.337 Prec@(1,5) (88.2%, 99.7%)
01/19 04:01:41 AM | Train: [49/100] Step 200/390 Loss 0.151 Prec@(1,5) (94.8%, 100.0%)
01/19 04:01:41 AM | Train: [48/100] Step 200/390 Loss 0.336 Prec@(1,5) (88.3%, 99.6%)
01/19 04:01:44 AM | Train: [48/100] Step 200/390 Loss 0.144 Prec@(1,5) (94.9%, 100.0%)
01/19 04:01:44 AM | Train: [48/100] Step 300/390 Loss 0.339 Prec@(1,5) (88.1%, 99.7%)
01/19 04:01:47 AM | Train: [49/100] Step 300/390 Loss 0.153 Prec@(1,5) (94.7%, 100.0%)
01/19 04:01:47 AM | Train: [48/100] Step 300/390 Loss 0.340 Prec@(1,5) (88.2%, 99.7%)
01/19 04:01:50 AM | Train: [48/100] Step 390/390 Loss 0.341 Prec@(1,5) (88.1%, 99.7%)
01/19 04:01:50 AM | Train: [48/100] Final Prec@1 88.0860%
01/19 04:01:50 AM | Train: [48/100] Step 300/390 Loss 0.141 Prec@(1,5) (95.0%, 100.0%)
01/19 04:01:50 AM | Valid: [48/100] Step 000/078 Loss 0.321 Prec@(1,5) (88.3%, 98.4%)
01/19 04:01:52 AM | Valid: [48/100] Step 078/078 Loss 0.434 Prec@(1,5) (85.5%, 99.4%)
01/19 04:01:52 AM | Valid: [48/100] Final Prec@1 85.5200%
01/19 04:01:52 AM | Train: [48/100] Step 390/390 Loss 0.341 Prec@(1,5) (88.2%, 99.7%)
01/19 04:01:52 AM | Current best Prec@1 = 85.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:01:52 AM | Train: [48/100] Final Prec@1 88.1540%
01/19 04:01:52 AM | Train: [49/100] Step 390/390 Loss 0.152 Prec@(1,5) (94.8%, 100.0%)
01/19 04:01:52 AM | Train: [49/100] Final Prec@1 94.7780%
01/19 04:01:52 AM | Train: [49/100] Step 000/390 Loss 0.297 Prec@(1,5) (92.2%, 99.2%)
01/19 04:01:53 AM | Valid: [48/100] Step 000/078 Loss 0.332 Prec@(1,5) (88.3%, 100.0%)
01/19 04:01:53 AM | Valid: [49/100] Step 000/078 Loss 0.252 Prec@(1,5) (92.2%, 99.2%)
01/19 04:01:54 AM | Valid: [48/100] Step 078/078 Loss 0.427 Prec@(1,5) (85.7%, 99.4%)
01/19 04:01:54 AM | Valid: [48/100] Final Prec@1 85.7100%
01/19 04:01:54 AM | Valid: [49/100] Step 078/078 Loss 0.348 Prec@(1,5) (89.3%, 99.6%)
01/19 04:01:54 AM | Current best Prec@1 = 85.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:01:55 AM | Valid: [49/100] Final Prec@1 89.3400%
01/19 04:01:55 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:01:55 AM | Train: [48/100] Step 390/390 Loss 0.144 Prec@(1,5) (95.0%, 100.0%)
01/19 04:01:55 AM | Train: [49/100] Step 000/390 Loss 0.293 Prec@(1,5) (92.2%, 99.2%)
01/19 04:01:55 AM | Train: [48/100] Final Prec@1 94.9740%
01/19 04:01:55 AM | Train: [50/100] Step 000/390 Loss 0.066 Prec@(1,5) (98.4%, 100.0%)
01/19 04:01:56 AM | Valid: [48/100] Step 000/078 Loss 0.206 Prec@(1,5) (93.8%, 100.0%)
01/19 04:01:57 AM | Valid: [48/100] Step 078/078 Loss 0.349 Prec@(1,5) (89.3%, 99.5%)
01/19 04:01:57 AM | Valid: [48/100] Final Prec@1 89.2800%
01/19 04:01:58 AM | Current best Prec@1 = 89.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:01:58 AM | Train: [49/100] Step 100/390 Loss 0.335 Prec@(1,5) (88.5%, 99.7%)
01/19 04:01:58 AM | Train: [49/100] Step 000/390 Loss 0.113 Prec@(1,5) (96.1%, 100.0%)
01/19 04:02:01 AM | Train: [50/100] Step 100/390 Loss 0.135 Prec@(1,5) (95.4%, 100.0%)
01/19 04:02:01 AM | Train: [49/100] Step 100/390 Loss 0.326 Prec@(1,5) (88.7%, 99.6%)
01/19 04:02:04 AM | Train: [49/100] Step 200/390 Loss 0.338 Prec@(1,5) (88.2%, 99.6%)
01/19 04:02:04 AM | Train: [49/100] Step 100/390 Loss 0.134 Prec@(1,5) (95.4%, 100.0%)
01/19 04:02:07 AM | Train: [50/100] Step 200/390 Loss 0.133 Prec@(1,5) (95.5%, 100.0%)
01/19 04:02:07 AM | Train: [49/100] Step 200/390 Loss 0.329 Prec@(1,5) (88.6%, 99.6%)
01/19 04:02:10 AM | Train: [49/100] Step 300/390 Loss 0.342 Prec@(1,5) (88.2%, 99.6%)
01/19 04:02:11 AM | Train: [49/100] Step 200/390 Loss 0.138 Prec@(1,5) (95.2%, 100.0%)
01/19 04:02:13 AM | Train: [50/100] Step 300/390 Loss 0.134 Prec@(1,5) (95.4%, 100.0%)
01/19 04:02:13 AM | Train: [49/100] Step 300/390 Loss 0.334 Prec@(1,5) (88.5%, 99.6%)
01/19 04:02:15 AM | Train: [49/100] Step 390/390 Loss 0.341 Prec@(1,5) (88.2%, 99.6%)
01/19 04:02:16 AM | Train: [49/100] Final Prec@1 88.1860%
01/19 04:02:16 AM | Valid: [49/100] Step 000/078 Loss 0.446 Prec@(1,5) (85.9%, 98.4%)
01/19 04:02:17 AM | Train: [49/100] Step 300/390 Loss 0.140 Prec@(1,5) (95.1%, 100.0%)
01/19 04:02:18 AM | Valid: [49/100] Step 078/078 Loss 0.459 Prec@(1,5) (85.0%, 99.2%)
01/19 04:02:18 AM | Valid: [49/100] Final Prec@1 85.0500%
01/19 04:02:18 AM | Train: [49/100] Step 390/390 Loss 0.334 Prec@(1,5) (88.5%, 99.6%)
01/19 04:02:18 AM | Current best Prec@1 = 85.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:02:18 AM | Train: [50/100] Step 390/390 Loss 0.137 Prec@(1,5) (95.2%, 100.0%)
01/19 04:02:18 AM | Train: [49/100] Final Prec@1 88.4740%
01/19 04:02:18 AM | Train: [50/100] Final Prec@1 95.2320%
01/19 04:02:19 AM | Valid: [49/100] Step 000/078 Loss 0.478 Prec@(1,5) (86.7%, 100.0%)
01/19 04:02:19 AM | Train: [50/100] Step 000/390 Loss 0.276 Prec@(1,5) (89.8%, 100.0%)
01/19 04:02:19 AM | Valid: [50/100] Step 000/078 Loss 0.371 Prec@(1,5) (90.6%, 99.2%)
01/19 04:02:21 AM | Valid: [49/100] Step 078/078 Loss 0.447 Prec@(1,5) (85.3%, 99.3%)
01/19 04:02:21 AM | Valid: [49/100] Final Prec@1 85.3400%
01/19 04:02:21 AM | Valid: [50/100] Step 078/078 Loss 0.390 Prec@(1,5) (88.5%, 99.4%)
01/19 04:02:21 AM | Valid: [50/100] Final Prec@1 88.4900%
01/19 04:02:21 AM | Current best Prec@1 = 85.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:02:21 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:02:22 AM | Train: [50/100] Step 000/390 Loss 0.221 Prec@(1,5) (91.4%, 99.2%)
01/19 04:02:22 AM | Train: [51/100] Step 000/390 Loss 0.199 Prec@(1,5) (93.8%, 100.0%)
01/19 04:02:22 AM | Train: [49/100] Step 390/390 Loss 0.140 Prec@(1,5) (95.1%, 100.0%)
01/19 04:02:22 AM | Train: [49/100] Final Prec@1 95.1120%
01/19 04:02:23 AM | Valid: [49/100] Step 000/078 Loss 0.308 Prec@(1,5) (91.4%, 98.4%)
01/19 04:02:24 AM | Valid: [49/100] Step 078/078 Loss 0.340 Prec@(1,5) (89.5%, 99.7%)
01/19 04:02:24 AM | Valid: [49/100] Final Prec@1 89.4700%
01/19 04:02:24 AM | Current best Prec@1 = 89.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:02:25 AM | Train: [50/100] Step 100/390 Loss 0.338 Prec@(1,5) (88.0%, 99.6%)
01/19 04:02:25 AM | Train: [50/100] Step 000/390 Loss 0.083 Prec@(1,5) (98.4%, 100.0%)
01/19 04:02:27 AM | Train: [51/100] Step 100/390 Loss 0.130 Prec@(1,5) (95.5%, 100.0%)
01/19 04:02:28 AM | Train: [50/100] Step 100/390 Loss 0.329 Prec@(1,5) (88.5%, 99.6%)
01/19 04:02:31 AM | Train: [50/100] Step 200/390 Loss 0.327 Prec@(1,5) (88.4%, 99.6%)
01/19 04:02:31 AM | Train: [50/100] Step 100/390 Loss 0.128 Prec@(1,5) (95.7%, 100.0%)
01/19 04:02:33 AM | Train: [51/100] Step 200/390 Loss 0.136 Prec@(1,5) (95.3%, 100.0%)
01/19 04:02:33 AM | Train: [50/100] Step 200/390 Loss 0.323 Prec@(1,5) (88.7%, 99.6%)
01/19 04:02:36 AM | Train: [50/100] Step 300/390 Loss 0.329 Prec@(1,5) (88.3%, 99.7%)
01/19 04:02:37 AM | Train: [50/100] Step 200/390 Loss 0.126 Prec@(1,5) (95.7%, 100.0%)
01/19 04:02:39 AM | Train: [51/100] Step 300/390 Loss 0.138 Prec@(1,5) (95.2%, 100.0%)
01/19 04:02:40 AM | Train: [50/100] Step 300/390 Loss 0.329 Prec@(1,5) (88.5%, 99.6%)
01/19 04:02:42 AM | Train: [50/100] Step 390/390 Loss 0.334 Prec@(1,5) (88.2%, 99.7%)
01/19 04:02:42 AM | Train: [50/100] Final Prec@1 88.2180%
01/19 04:02:43 AM | Valid: [50/100] Step 000/078 Loss 0.332 Prec@(1,5) (89.8%, 98.4%)
01/19 04:02:43 AM | Train: [50/100] Step 300/390 Loss 0.131 Prec@(1,5) (95.6%, 100.0%)
01/19 04:02:44 AM | Valid: [50/100] Step 078/078 Loss 0.439 Prec@(1,5) (85.3%, 99.2%)
01/19 04:02:45 AM | Train: [51/100] Step 390/390 Loss 0.142 Prec@(1,5) (95.1%, 100.0%)
01/19 04:02:45 AM | Valid: [50/100] Final Prec@1 85.3100%
01/19 04:02:45 AM | Current best Prec@1 = 85.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:02:45 AM | Train: [50/100] Step 390/390 Loss 0.333 Prec@(1,5) (88.4%, 99.7%)
01/19 04:02:45 AM | Train: [51/100] Final Prec@1 95.0620%
01/19 04:02:45 AM | Train: [50/100] Final Prec@1 88.3900%
01/19 04:02:45 AM | Train: [51/100] Step 000/390 Loss 0.443 Prec@(1,5) (86.7%, 98.4%)
01/19 04:02:45 AM | Valid: [51/100] Step 000/078 Loss 0.271 Prec@(1,5) (92.2%, 100.0%)
01/19 04:02:46 AM | Valid: [50/100] Step 000/078 Loss 0.351 Prec@(1,5) (88.3%, 100.0%)
01/19 04:02:47 AM | Valid: [51/100] Step 078/078 Loss 0.356 Prec@(1,5) (89.3%, 99.5%)
01/19 04:02:47 AM | Valid: [51/100] Final Prec@1 89.3500%
01/19 04:02:47 AM | Valid: [50/100] Step 078/078 Loss 0.470 Prec@(1,5) (84.8%, 99.3%)
01/19 04:02:47 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:02:47 AM | Valid: [50/100] Final Prec@1 84.7800%
01/19 04:02:47 AM | Current best Prec@1 = 85.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:02:48 AM | Train: [52/100] Step 000/390 Loss 0.170 Prec@(1,5) (94.5%, 99.2%)
01/19 04:02:48 AM | Train: [51/100] Step 000/390 Loss 0.538 Prec@(1,5) (85.2%, 98.4%)
01/19 04:02:48 AM | Train: [50/100] Step 390/390 Loss 0.132 Prec@(1,5) (95.5%, 100.0%)
01/19 04:02:48 AM | Train: [50/100] Final Prec@1 95.5060%
01/19 04:02:49 AM | Valid: [50/100] Step 000/078 Loss 0.323 Prec@(1,5) (88.3%, 99.2%)
01/19 04:02:50 AM | Valid: [50/100] Step 078/078 Loss 0.385 Prec@(1,5) (88.5%, 99.5%)
01/19 04:02:50 AM | Valid: [50/100] Final Prec@1 88.4900%
01/19 04:02:50 AM | Current best Prec@1 = 89.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:02:51 AM | Train: [51/100] Step 000/390 Loss 0.257 Prec@(1,5) (93.0%, 100.0%)
01/19 04:02:51 AM | Train: [51/100] Step 100/390 Loss 0.325 Prec@(1,5) (88.8%, 99.7%)
01/19 04:02:53 AM | Train: [52/100] Step 100/390 Loss 0.127 Prec@(1,5) (95.4%, 100.0%)
01/19 04:02:54 AM | Train: [51/100] Step 100/390 Loss 0.331 Prec@(1,5) (88.5%, 99.7%)
01/19 04:02:57 AM | Train: [51/100] Step 100/390 Loss 0.139 Prec@(1,5) (95.2%, 100.0%)
01/19 04:02:57 AM | Train: [51/100] Step 200/390 Loss 0.324 Prec@(1,5) (88.8%, 99.7%)
01/19 04:02:59 AM | Train: [52/100] Step 200/390 Loss 0.128 Prec@(1,5) (95.5%, 100.0%)
01/19 04:03:00 AM | Train: [51/100] Step 200/390 Loss 0.330 Prec@(1,5) (88.5%, 99.7%)
01/19 04:03:03 AM | Train: [51/100] Step 200/390 Loss 0.134 Prec@(1,5) (95.4%, 100.0%)
01/19 04:03:03 AM | Train: [51/100] Step 300/390 Loss 0.329 Prec@(1,5) (88.6%, 99.7%)
01/19 04:03:05 AM | Train: [52/100] Step 300/390 Loss 0.128 Prec@(1,5) (95.5%, 100.0%)
01/19 04:03:06 AM | Train: [51/100] Step 300/390 Loss 0.328 Prec@(1,5) (88.6%, 99.7%)
01/19 04:03:09 AM | Train: [51/100] Step 390/390 Loss 0.332 Prec@(1,5) (88.5%, 99.7%)
01/19 04:03:09 AM | Train: [51/100] Final Prec@1 88.4740%
01/19 04:03:09 AM | Train: [51/100] Step 300/390 Loss 0.133 Prec@(1,5) (95.5%, 100.0%)
01/19 04:03:09 AM | Valid: [51/100] Step 000/078 Loss 0.300 Prec@(1,5) (92.2%, 99.2%)
01/19 04:03:11 AM | Train: [52/100] Step 390/390 Loss 0.130 Prec@(1,5) (95.5%, 100.0%)
01/19 04:03:11 AM | Train: [52/100] Final Prec@1 95.4840%
01/19 04:03:11 AM | Valid: [51/100] Step 078/078 Loss 0.477 Prec@(1,5) (84.3%, 99.3%)
01/19 04:03:11 AM | Valid: [51/100] Final Prec@1 84.3300%
01/19 04:03:11 AM | Current best Prec@1 = 85.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:03:11 AM | Train: [51/100] Step 390/390 Loss 0.329 Prec@(1,5) (88.6%, 99.7%)
01/19 04:03:11 AM | Valid: [52/100] Step 000/078 Loss 0.233 Prec@(1,5) (89.8%, 100.0%)
01/19 04:03:11 AM | Train: [51/100] Final Prec@1 88.5760%
01/19 04:03:12 AM | Train: [52/100] Step 000/390 Loss 0.319 Prec@(1,5) (87.5%, 99.2%)
01/19 04:03:12 AM | Valid: [51/100] Step 000/078 Loss 0.421 Prec@(1,5) (85.2%, 99.2%)
01/19 04:03:13 AM | Valid: [52/100] Step 078/078 Loss 0.346 Prec@(1,5) (89.3%, 99.6%)
01/19 04:03:13 AM | Valid: [52/100] Final Prec@1 89.2600%
01/19 04:03:13 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:03:13 AM | Valid: [51/100] Step 078/078 Loss 0.462 Prec@(1,5) (84.9%, 99.5%)
01/19 04:03:14 AM | Valid: [51/100] Final Prec@1 84.9200%
01/19 04:03:14 AM | Current best Prec@1 = 85.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:03:14 AM | Train: [53/100] Step 000/390 Loss 0.076 Prec@(1,5) (97.7%, 100.0%)
01/19 04:03:14 AM | Train: [51/100] Step 390/390 Loss 0.133 Prec@(1,5) (95.5%, 100.0%)
01/19 04:03:14 AM | Train: [51/100] Final Prec@1 95.4940%
01/19 04:03:15 AM | Train: [52/100] Step 000/390 Loss 0.241 Prec@(1,5) (90.6%, 100.0%)
01/19 04:03:15 AM | Valid: [51/100] Step 000/078 Loss 0.276 Prec@(1,5) (92.2%, 100.0%)
01/19 04:03:16 AM | Valid: [51/100] Step 078/078 Loss 0.361 Prec@(1,5) (88.9%, 99.7%)
01/19 04:03:17 AM | Valid: [51/100] Final Prec@1 88.9000%
01/19 04:03:17 AM | Current best Prec@1 = 89.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:03:17 AM | Train: [52/100] Step 100/390 Loss 0.328 Prec@(1,5) (88.5%, 99.7%)
01/19 04:03:17 AM | Train: [52/100] Step 000/390 Loss 0.161 Prec@(1,5) (91.4%, 100.0%)
01/19 04:03:19 AM | Train: [53/100] Step 100/390 Loss 0.123 Prec@(1,5) (95.7%, 100.0%)
01/19 04:03:21 AM | Train: [52/100] Step 100/390 Loss 0.323 Prec@(1,5) (88.9%, 99.7%)
01/19 04:03:23 AM | Train: [52/100] Step 200/390 Loss 0.321 Prec@(1,5) (88.8%, 99.7%)
01/19 04:03:23 AM | Train: [52/100] Step 100/390 Loss 0.125 Prec@(1,5) (95.6%, 100.0%)
01/19 04:03:25 AM | Train: [53/100] Step 200/390 Loss 0.126 Prec@(1,5) (95.6%, 100.0%)
01/19 04:03:27 AM | Train: [52/100] Step 200/390 Loss 0.314 Prec@(1,5) (89.1%, 99.7%)
01/19 04:03:29 AM | Train: [52/100] Step 300/390 Loss 0.320 Prec@(1,5) (88.8%, 99.7%)
01/19 04:03:30 AM | Train: [52/100] Step 200/390 Loss 0.132 Prec@(1,5) (95.4%, 100.0%)
01/19 04:03:31 AM | Train: [53/100] Step 300/390 Loss 0.126 Prec@(1,5) (95.6%, 100.0%)
01/19 04:03:33 AM | Train: [52/100] Step 300/390 Loss 0.317 Prec@(1,5) (89.0%, 99.7%)
01/19 04:03:35 AM | Train: [52/100] Step 390/390 Loss 0.323 Prec@(1,5) (88.7%, 99.7%)
01/19 04:03:35 AM | Train: [52/100] Final Prec@1 88.7220%
01/19 04:03:35 AM | Valid: [52/100] Step 000/078 Loss 0.433 Prec@(1,5) (87.5%, 99.2%)
01/19 04:03:36 AM | Train: [52/100] Step 300/390 Loss 0.133 Prec@(1,5) (95.4%, 100.0%)
01/19 04:03:37 AM | Train: [53/100] Step 390/390 Loss 0.128 Prec@(1,5) (95.6%, 100.0%)
01/19 04:03:37 AM | Train: [53/100] Final Prec@1 95.5540%
01/19 04:03:37 AM | Valid: [52/100] Step 078/078 Loss 0.467 Prec@(1,5) (84.6%, 99.4%)
01/19 04:03:37 AM | Valid: [52/100] Final Prec@1 84.6300%
01/19 04:03:37 AM | Current best Prec@1 = 85.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:03:37 AM | Valid: [53/100] Step 000/078 Loss 0.265 Prec@(1,5) (92.2%, 99.2%)
01/19 04:03:38 AM | Train: [53/100] Step 000/390 Loss 0.279 Prec@(1,5) (89.8%, 100.0%)
01/19 04:03:38 AM | Train: [52/100] Step 390/390 Loss 0.320 Prec@(1,5) (88.9%, 99.7%)
01/19 04:03:38 AM | Train: [52/100] Final Prec@1 88.9260%
01/19 04:03:39 AM | Valid: [52/100] Step 000/078 Loss 0.492 Prec@(1,5) (83.6%, 98.4%)
01/19 04:03:39 AM | Valid: [53/100] Step 078/078 Loss 0.361 Prec@(1,5) (89.2%, 99.5%)
01/19 04:03:39 AM | Valid: [53/100] Final Prec@1 89.2000%
01/19 04:03:39 AM | Current best Prec@1 = 89.4800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:03:40 AM | Train: [54/100] Step 000/390 Loss 0.066 Prec@(1,5) (97.7%, 100.0%)
01/19 04:03:40 AM | Valid: [52/100] Step 078/078 Loss 0.473 Prec@(1,5) (84.0%, 99.4%)
01/19 04:03:40 AM | Valid: [52/100] Final Prec@1 83.9800%
01/19 04:03:41 AM | Current best Prec@1 = 85.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:03:41 AM | Train: [52/100] Step 390/390 Loss 0.134 Prec@(1,5) (95.3%, 100.0%)
01/19 04:03:41 AM | Train: [52/100] Final Prec@1 95.2900%
01/19 04:03:41 AM | Train: [53/100] Step 000/390 Loss 0.270 Prec@(1,5) (91.4%, 100.0%)
01/19 04:03:41 AM | Valid: [52/100] Step 000/078 Loss 0.279 Prec@(1,5) (93.8%, 100.0%)
01/19 04:03:43 AM | Valid: [52/100] Step 078/078 Loss 0.364 Prec@(1,5) (89.1%, 99.6%)
01/19 04:03:43 AM | Valid: [52/100] Final Prec@1 89.1100%
01/19 04:03:43 AM | Current best Prec@1 = 89.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:03:43 AM | Train: [53/100] Step 100/390 Loss 0.303 Prec@(1,5) (89.7%, 99.7%)
01/19 04:03:44 AM | Train: [53/100] Step 000/390 Loss 0.083 Prec@(1,5) (96.1%, 100.0%)
01/19 04:03:46 AM | Train: [54/100] Step 100/390 Loss 0.112 Prec@(1,5) (96.1%, 100.0%)
01/19 04:03:47 AM | Train: [53/100] Step 100/390 Loss 0.302 Prec@(1,5) (89.7%, 99.7%)
01/19 04:03:49 AM | Train: [53/100] Step 200/390 Loss 0.319 Prec@(1,5) (89.0%, 99.7%)
01/19 04:03:50 AM | Train: [53/100] Step 100/390 Loss 0.113 Prec@(1,5) (96.2%, 100.0%)
01/19 04:03:52 AM | Train: [54/100] Step 200/390 Loss 0.115 Prec@(1,5) (95.9%, 100.0%)
01/19 04:03:53 AM | Train: [53/100] Step 200/390 Loss 0.318 Prec@(1,5) (88.9%, 99.7%)
01/19 04:03:56 AM | Train: [53/100] Step 300/390 Loss 0.320 Prec@(1,5) (89.0%, 99.7%)
01/19 04:03:56 AM | Train: [53/100] Step 200/390 Loss 0.120 Prec@(1,5) (95.9%, 100.0%)
01/19 04:03:58 AM | Train: [54/100] Step 300/390 Loss 0.119 Prec@(1,5) (95.9%, 100.0%)
01/19 04:04:00 AM | Train: [53/100] Step 300/390 Loss 0.318 Prec@(1,5) (88.9%, 99.7%)
01/19 04:04:01 AM | Train: [53/100] Step 390/390 Loss 0.323 Prec@(1,5) (88.9%, 99.7%)
01/19 04:04:01 AM | Train: [53/100] Final Prec@1 88.8540%
01/19 04:04:02 AM | Valid: [53/100] Step 000/078 Loss 0.369 Prec@(1,5) (86.7%, 99.2%)
01/19 04:04:02 AM | Train: [53/100] Step 300/390 Loss 0.122 Prec@(1,5) (95.8%, 100.0%)
01/19 04:04:03 AM | Valid: [53/100] Step 078/078 Loss 0.447 Prec@(1,5) (85.0%, 99.4%)
01/19 04:04:04 AM | Valid: [53/100] Final Prec@1 85.0500%
01/19 04:04:04 AM | Train: [54/100] Step 390/390 Loss 0.123 Prec@(1,5) (95.7%, 100.0%)
01/19 04:04:04 AM | Train: [54/100] Final Prec@1 95.6800%
01/19 04:04:04 AM | Current best Prec@1 = 85.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:04:04 AM | Valid: [54/100] Step 000/078 Loss 0.241 Prec@(1,5) (89.8%, 100.0%)
01/19 04:04:04 AM | Train: [54/100] Step 000/390 Loss 0.237 Prec@(1,5) (94.5%, 100.0%)
01/19 04:04:05 AM | Train: [53/100] Step 390/390 Loss 0.320 Prec@(1,5) (88.8%, 99.7%)
01/19 04:04:05 AM | Train: [53/100] Final Prec@1 88.8060%
01/19 04:04:05 AM | Valid: [53/100] Step 000/078 Loss 0.471 Prec@(1,5) (85.2%, 98.4%)
01/19 04:04:06 AM | Valid: [54/100] Step 078/078 Loss 0.335 Prec@(1,5) (89.6%, 99.6%)
01/19 04:04:06 AM | Valid: [54/100] Final Prec@1 89.6500%
01/19 04:04:06 AM | Current best Prec@1 = 89.6500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:04:07 AM | Train: [55/100] Step 000/390 Loss 0.156 Prec@(1,5) (93.8%, 100.0%)
01/19 04:04:07 AM | Train: [53/100] Step 390/390 Loss 0.126 Prec@(1,5) (95.7%, 100.0%)
01/19 04:04:07 AM | Train: [53/100] Final Prec@1 95.6520%
01/19 04:04:07 AM | Valid: [53/100] Step 078/078 Loss 0.444 Prec@(1,5) (85.1%, 99.4%)
01/19 04:04:07 AM | Valid: [53/100] Final Prec@1 85.1400%
01/19 04:04:07 AM | Valid: [53/100] Step 000/078 Loss 0.331 Prec@(1,5) (92.2%, 98.4%)
01/19 04:04:08 AM | Current best Prec@1 = 85.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:04:08 AM | Train: [54/100] Step 000/390 Loss 0.207 Prec@(1,5) (91.4%, 100.0%)
01/19 04:04:09 AM | Valid: [53/100] Step 078/078 Loss 0.368 Prec@(1,5) (89.0%, 99.6%)
01/19 04:04:09 AM | Valid: [53/100] Final Prec@1 88.9800%
01/19 04:04:09 AM | Current best Prec@1 = 89.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:04:10 AM | Train: [54/100] Step 100/390 Loss 0.308 Prec@(1,5) (89.3%, 99.7%)
01/19 04:04:10 AM | Train: [54/100] Step 000/390 Loss 0.075 Prec@(1,5) (98.4%, 100.0%)
01/19 04:04:12 AM | Train: [55/100] Step 100/390 Loss 0.122 Prec@(1,5) (95.8%, 100.0%)
01/19 04:04:14 AM | Train: [54/100] Step 100/390 Loss 0.312 Prec@(1,5) (88.9%, 99.7%)
01/19 04:04:16 AM | Train: [54/100] Step 200/390 Loss 0.310 Prec@(1,5) (89.1%, 99.7%)
01/19 04:04:16 AM | Train: [54/100] Step 100/390 Loss 0.124 Prec@(1,5) (95.7%, 100.0%)
01/19 04:04:19 AM | Train: [55/100] Step 200/390 Loss 0.122 Prec@(1,5) (95.8%, 100.0%)
01/19 04:04:21 AM | Train: [54/100] Step 200/390 Loss 0.310 Prec@(1,5) (88.9%, 99.7%)
01/19 04:04:22 AM | Train: [54/100] Step 300/390 Loss 0.315 Prec@(1,5) (89.0%, 99.7%)
01/19 04:04:22 AM | Train: [54/100] Step 200/390 Loss 0.123 Prec@(1,5) (95.7%, 100.0%)
01/19 04:04:25 AM | Train: [55/100] Step 300/390 Loss 0.127 Prec@(1,5) (95.6%, 100.0%)
01/19 04:04:27 AM | Train: [54/100] Step 300/390 Loss 0.311 Prec@(1,5) (89.0%, 99.7%)
01/19 04:04:27 AM | Train: [54/100] Step 390/390 Loss 0.316 Prec@(1,5) (88.9%, 99.7%)
01/19 04:04:28 AM | Train: [54/100] Final Prec@1 88.9240%
01/19 04:04:28 AM | Train: [54/100] Step 300/390 Loss 0.123 Prec@(1,5) (95.7%, 100.0%)
01/19 04:04:28 AM | Valid: [54/100] Step 000/078 Loss 0.374 Prec@(1,5) (90.6%, 99.2%)
01/19 04:04:30 AM | Valid: [54/100] Step 078/078 Loss 0.437 Prec@(1,5) (85.8%, 99.4%)
01/19 04:04:30 AM | Valid: [54/100] Final Prec@1 85.7500%
01/19 04:04:30 AM | Current best Prec@1 = 85.7500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:04:31 AM | Train: [55/100] Step 390/390 Loss 0.128 Prec@(1,5) (95.6%, 100.0%)
01/19 04:04:31 AM | Train: [55/100] Final Prec@1 95.5600%
01/19 04:04:31 AM | Train: [55/100] Step 000/390 Loss 0.385 Prec@(1,5) (85.9%, 100.0%)
01/19 04:04:31 AM | Valid: [55/100] Step 000/078 Loss 0.251 Prec@(1,5) (90.6%, 98.4%)
01/19 04:04:32 AM | Train: [54/100] Step 390/390 Loss 0.312 Prec@(1,5) (89.1%, 99.7%)
01/19 04:04:32 AM | Train: [54/100] Final Prec@1 89.0800%
01/19 04:04:32 AM | Valid: [54/100] Step 000/078 Loss 0.358 Prec@(1,5) (87.5%, 100.0%)
01/19 04:04:33 AM | Valid: [55/100] Step 078/078 Loss 0.341 Prec@(1,5) (89.9%, 99.5%)
01/19 04:04:33 AM | Valid: [55/100] Final Prec@1 89.8700%
01/19 04:04:33 AM | Current best Prec@1 = 89.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:04:33 AM | Train: [54/100] Step 390/390 Loss 0.125 Prec@(1,5) (95.6%, 100.0%)
01/19 04:04:33 AM | Train: [56/100] Step 000/390 Loss 0.123 Prec@(1,5) (95.3%, 100.0%)
01/19 04:04:33 AM | Train: [54/100] Final Prec@1 95.6300%
01/19 04:04:34 AM | Valid: [54/100] Step 000/078 Loss 0.196 Prec@(1,5) (94.5%, 99.2%)
01/19 04:04:34 AM | Valid: [54/100] Step 078/078 Loss 0.411 Prec@(1,5) (86.4%, 99.4%)
01/19 04:04:34 AM | Valid: [54/100] Final Prec@1 86.4400%
01/19 04:04:34 AM | Current best Prec@1 = 86.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:04:35 AM | Train: [55/100] Step 000/390 Loss 0.391 Prec@(1,5) (88.3%, 100.0%)
01/19 04:04:36 AM | Valid: [54/100] Step 078/078 Loss 0.325 Prec@(1,5) (90.0%, 99.6%)
01/19 04:04:36 AM | Valid: [54/100] Final Prec@1 90.0200%
01/19 04:04:36 AM | Current best Prec@1 = 90.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:04:37 AM | Train: [55/100] Step 100/390 Loss 0.300 Prec@(1,5) (89.5%, 99.7%)
01/19 04:04:37 AM | Train: [55/100] Step 000/390 Loss 0.094 Prec@(1,5) (96.1%, 100.0%)
01/19 04:04:39 AM | Train: [56/100] Step 100/390 Loss 0.115 Prec@(1,5) (96.1%, 100.0%)
01/19 04:04:41 AM | Train: [55/100] Step 100/390 Loss 0.298 Prec@(1,5) (89.5%, 99.7%)
01/19 04:04:43 AM | Train: [55/100] Step 100/390 Loss 0.106 Prec@(1,5) (96.3%, 100.0%)
01/19 04:04:43 AM | Train: [55/100] Step 200/390 Loss 0.309 Prec@(1,5) (89.2%, 99.7%)
01/19 04:04:45 AM | Train: [56/100] Step 200/390 Loss 0.114 Prec@(1,5) (96.1%, 100.0%)
01/19 04:04:47 AM | Train: [55/100] Step 200/390 Loss 0.304 Prec@(1,5) (89.2%, 99.7%)
01/19 04:04:49 AM | Train: [55/100] Step 200/390 Loss 0.110 Prec@(1,5) (96.2%, 100.0%)
01/19 04:04:49 AM | Train: [55/100] Step 300/390 Loss 0.313 Prec@(1,5) (89.1%, 99.7%)
01/19 04:04:51 AM | Train: [56/100] Step 300/390 Loss 0.116 Prec@(1,5) (96.0%, 100.0%)
01/19 04:04:53 AM | Train: [55/100] Step 300/390 Loss 0.308 Prec@(1,5) (89.1%, 99.7%)
01/19 04:04:55 AM | Train: [55/100] Step 390/390 Loss 0.316 Prec@(1,5) (89.0%, 99.7%)
01/19 04:04:55 AM | Train: [55/100] Final Prec@1 88.9680%
01/19 04:04:55 AM | Train: [55/100] Step 300/390 Loss 0.114 Prec@(1,5) (96.1%, 100.0%)
01/19 04:04:55 AM | Valid: [55/100] Step 000/078 Loss 0.445 Prec@(1,5) (86.7%, 100.0%)
01/19 04:04:57 AM | Train: [56/100] Step 390/390 Loss 0.116 Prec@(1,5) (95.9%, 100.0%)
01/19 04:04:57 AM | Valid: [55/100] Step 078/078 Loss 0.466 Prec@(1,5) (84.7%, 99.3%)
01/19 04:04:57 AM | Train: [56/100] Final Prec@1 95.9400%
01/19 04:04:57 AM | Valid: [55/100] Final Prec@1 84.7200%
01/19 04:04:57 AM | Current best Prec@1 = 85.7500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:04:58 AM | Valid: [56/100] Step 000/078 Loss 0.263 Prec@(1,5) (92.2%, 99.2%)
01/19 04:04:58 AM | Train: [55/100] Step 390/390 Loss 0.309 Prec@(1,5) (89.1%, 99.7%)
01/19 04:04:58 AM | Train: [55/100] Final Prec@1 89.1440%
01/19 04:04:58 AM | Train: [56/100] Step 000/390 Loss 0.355 Prec@(1,5) (89.1%, 100.0%)
01/19 04:04:58 AM | Valid: [55/100] Step 000/078 Loss 0.454 Prec@(1,5) (83.6%, 99.2%)
01/19 04:04:59 AM | Valid: [56/100] Step 078/078 Loss 0.339 Prec@(1,5) (89.7%, 99.5%)
01/19 04:05:00 AM | Valid: [56/100] Final Prec@1 89.6600%
01/19 04:05:00 AM | Current best Prec@1 = 89.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:05:00 AM | Train: [55/100] Step 390/390 Loss 0.115 Prec@(1,5) (96.0%, 100.0%)
01/19 04:05:00 AM | Train: [55/100] Final Prec@1 96.0400%
01/19 04:05:00 AM | Valid: [55/100] Step 078/078 Loss 0.443 Prec@(1,5) (85.2%, 99.4%)
01/19 04:05:00 AM | Valid: [55/100] Final Prec@1 85.1700%
01/19 04:05:00 AM | Train: [57/100] Step 000/390 Loss 0.159 Prec@(1,5) (92.2%, 100.0%)
01/19 04:05:01 AM | Current best Prec@1 = 86.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:05:01 AM | Valid: [55/100] Step 000/078 Loss 0.321 Prec@(1,5) (92.2%, 98.4%)
01/19 04:05:01 AM | Train: [56/100] Step 000/390 Loss 0.269 Prec@(1,5) (92.2%, 100.0%)
01/19 04:05:02 AM | Valid: [55/100] Step 078/078 Loss 0.382 Prec@(1,5) (88.8%, 99.5%)
01/19 04:05:02 AM | Valid: [55/100] Final Prec@1 88.7500%
01/19 04:05:03 AM | Current best Prec@1 = 90.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:05:03 AM | Train: [56/100] Step 000/390 Loss 0.151 Prec@(1,5) (93.8%, 100.0%)
01/19 04:05:04 AM | Train: [56/100] Step 100/390 Loss 0.296 Prec@(1,5) (89.9%, 99.7%)
01/19 04:05:06 AM | Train: [57/100] Step 100/390 Loss 0.101 Prec@(1,5) (96.7%, 100.0%)
01/19 04:05:07 AM | Train: [56/100] Step 100/390 Loss 0.285 Prec@(1,5) (90.1%, 99.7%)
01/19 04:05:09 AM | Train: [56/100] Step 100/390 Loss 0.109 Prec@(1,5) (96.3%, 100.0%)
01/19 04:05:10 AM | Train: [56/100] Step 200/390 Loss 0.300 Prec@(1,5) (89.6%, 99.7%)
01/19 04:05:12 AM | Train: [57/100] Step 200/390 Loss 0.106 Prec@(1,5) (96.4%, 100.0%)
01/19 04:05:13 AM | Train: [56/100] Step 200/390 Loss 0.292 Prec@(1,5) (89.7%, 99.8%)
01/19 04:05:15 AM | Train: [56/100] Step 200/390 Loss 0.107 Prec@(1,5) (96.4%, 100.0%)
01/19 04:05:16 AM | Train: [56/100] Step 300/390 Loss 0.303 Prec@(1,5) (89.6%, 99.7%)
01/19 04:05:18 AM | Train: [57/100] Step 300/390 Loss 0.109 Prec@(1,5) (96.2%, 100.0%)
01/19 04:05:19 AM | Train: [56/100] Step 300/390 Loss 0.296 Prec@(1,5) (89.6%, 99.7%)
01/19 04:05:21 AM | Train: [56/100] Step 390/390 Loss 0.302 Prec@(1,5) (89.5%, 99.7%)
01/19 04:05:21 AM | Train: [56/100] Step 300/390 Loss 0.110 Prec@(1,5) (96.2%, 100.0%)
01/19 04:05:22 AM | Train: [56/100] Final Prec@1 89.5320%
01/19 04:05:22 AM | Valid: [56/100] Step 000/078 Loss 0.325 Prec@(1,5) (91.4%, 99.2%)
01/19 04:05:24 AM | Valid: [56/100] Step 078/078 Loss 0.429 Prec@(1,5) (86.0%, 99.4%)
01/19 04:05:24 AM | Valid: [56/100] Final Prec@1 85.9900%
01/19 04:05:24 AM | Current best Prec@1 = 85.9900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:05:24 AM | Train: [57/100] Step 390/390 Loss 0.112 Prec@(1,5) (96.1%, 100.0%)
01/19 04:05:24 AM | Train: [57/100] Final Prec@1 96.0980%
01/19 04:05:24 AM | Train: [56/100] Step 390/390 Loss 0.299 Prec@(1,5) (89.5%, 99.7%)
01/19 04:05:24 AM | Train: [56/100] Final Prec@1 89.4860%
01/19 04:05:24 AM | Train: [57/100] Step 000/390 Loss 0.281 Prec@(1,5) (91.4%, 100.0%)
01/19 04:05:25 AM | Valid: [57/100] Step 000/078 Loss 0.194 Prec@(1,5) (93.8%, 100.0%)
01/19 04:05:25 AM | Valid: [56/100] Step 000/078 Loss 0.426 Prec@(1,5) (86.7%, 100.0%)
01/19 04:05:26 AM | Valid: [57/100] Step 078/078 Loss 0.358 Prec@(1,5) (89.2%, 99.4%)
01/19 04:05:26 AM | Valid: [57/100] Final Prec@1 89.1800%
01/19 04:05:26 AM | Train: [56/100] Step 390/390 Loss 0.111 Prec@(1,5) (96.1%, 100.0%)
01/19 04:05:27 AM | Train: [56/100] Final Prec@1 96.1280%
01/19 04:05:27 AM | Current best Prec@1 = 89.8700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:05:27 AM | Valid: [56/100] Step 078/078 Loss 0.437 Prec@(1,5) (85.9%, 99.4%)
01/19 04:05:27 AM | Valid: [56/100] Final Prec@1 85.9200%
01/19 04:05:27 AM | Valid: [56/100] Step 000/078 Loss 0.260 Prec@(1,5) (90.6%, 99.2%)
01/19 04:05:27 AM | Train: [58/100] Step 000/390 Loss 0.102 Prec@(1,5) (94.5%, 100.0%)
01/19 04:05:27 AM | Current best Prec@1 = 86.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:05:28 AM | Train: [57/100] Step 000/390 Loss 0.325 Prec@(1,5) (89.1%, 100.0%)
01/19 04:05:29 AM | Valid: [56/100] Step 078/078 Loss 0.347 Prec@(1,5) (89.7%, 99.6%)
01/19 04:05:29 AM | Valid: [56/100] Final Prec@1 89.6700%
01/19 04:05:29 AM | Current best Prec@1 = 90.0200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:05:30 AM | Train: [57/100] Step 000/390 Loss 0.156 Prec@(1,5) (93.8%, 100.0%)
01/19 04:05:30 AM | Train: [57/100] Step 100/390 Loss 0.290 Prec@(1,5) (89.6%, 99.8%)
01/19 04:05:32 AM | Train: [58/100] Step 100/390 Loss 0.108 Prec@(1,5) (96.3%, 100.0%)
01/19 04:05:34 AM | Train: [57/100] Step 100/390 Loss 0.289 Prec@(1,5) (89.8%, 99.8%)
01/19 04:05:36 AM | Train: [57/100] Step 100/390 Loss 0.108 Prec@(1,5) (96.2%, 100.0%)
01/19 04:05:36 AM | Train: [57/100] Step 200/390 Loss 0.296 Prec@(1,5) (89.6%, 99.8%)
01/19 04:05:39 AM | Train: [58/100] Step 200/390 Loss 0.112 Prec@(1,5) (96.2%, 100.0%)
01/19 04:05:40 AM | Train: [57/100] Step 200/390 Loss 0.297 Prec@(1,5) (89.4%, 99.8%)
01/19 04:05:42 AM | Train: [57/100] Step 300/390 Loss 0.302 Prec@(1,5) (89.5%, 99.8%)
01/19 04:05:42 AM | Train: [57/100] Step 200/390 Loss 0.110 Prec@(1,5) (96.2%, 100.0%)
01/19 04:05:44 AM | Train: [58/100] Step 300/390 Loss 0.111 Prec@(1,5) (96.2%, 100.0%)
01/19 04:05:46 AM | Train: [57/100] Step 300/390 Loss 0.299 Prec@(1,5) (89.5%, 99.8%)
01/19 04:05:47 AM | Train: [57/100] Step 390/390 Loss 0.304 Prec@(1,5) (89.5%, 99.8%)
01/19 04:05:47 AM | Train: [57/100] Final Prec@1 89.4720%
01/19 04:05:48 AM | Valid: [57/100] Step 000/078 Loss 0.339 Prec@(1,5) (89.8%, 99.2%)
01/19 04:05:48 AM | Train: [57/100] Step 300/390 Loss 0.111 Prec@(1,5) (96.2%, 100.0%)
01/19 04:05:49 AM | Valid: [57/100] Step 078/078 Loss 0.424 Prec@(1,5) (86.1%, 99.3%)
01/19 04:05:50 AM | Valid: [57/100] Final Prec@1 86.0500%
01/19 04:05:50 AM | Current best Prec@1 = 86.0500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:05:50 AM | Train: [58/100] Step 390/390 Loss 0.110 Prec@(1,5) (96.2%, 100.0%)
01/19 04:05:50 AM | Train: [58/100] Final Prec@1 96.1820%
01/19 04:05:50 AM | Train: [58/100] Step 000/390 Loss 0.194 Prec@(1,5) (91.4%, 100.0%)
01/19 04:05:51 AM | Valid: [58/100] Step 000/078 Loss 0.238 Prec@(1,5) (93.0%, 100.0%)
01/19 04:05:51 AM | Train: [57/100] Step 390/390 Loss 0.299 Prec@(1,5) (89.5%, 99.8%)
01/19 04:05:51 AM | Train: [57/100] Final Prec@1 89.5000%
01/19 04:05:51 AM | Valid: [57/100] Step 000/078 Loss 0.368 Prec@(1,5) (88.3%, 100.0%)
01/19 04:05:52 AM | Valid: [58/100] Step 078/078 Loss 0.339 Prec@(1,5) (90.2%, 99.6%)
01/19 04:05:52 AM | Valid: [58/100] Final Prec@1 90.2100%
01/19 04:05:52 AM | Current best Prec@1 = 90.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:05:53 AM | Train: [57/100] Step 390/390 Loss 0.112 Prec@(1,5) (96.1%, 100.0%)
01/19 04:05:53 AM | Train: [59/100] Step 000/390 Loss 0.174 Prec@(1,5) (95.3%, 100.0%)
01/19 04:05:53 AM | Train: [57/100] Final Prec@1 96.1080%
01/19 04:05:53 AM | Valid: [57/100] Step 078/078 Loss 0.431 Prec@(1,5) (85.9%, 99.3%)
01/19 04:05:53 AM | Valid: [57/100] Final Prec@1 85.8500%
01/19 04:05:53 AM | Current best Prec@1 = 86.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:05:54 AM | Valid: [57/100] Step 000/078 Loss 0.199 Prec@(1,5) (93.0%, 100.0%)
01/19 04:05:54 AM | Train: [58/100] Step 000/390 Loss 0.169 Prec@(1,5) (96.1%, 100.0%)
01/19 04:05:55 AM | Valid: [57/100] Step 078/078 Loss 0.327 Prec@(1,5) (90.2%, 99.7%)
01/19 04:05:56 AM | Valid: [57/100] Final Prec@1 90.2200%
01/19 04:05:56 AM | Current best Prec@1 = 90.2200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:05:56 AM | Train: [58/100] Step 100/390 Loss 0.297 Prec@(1,5) (89.9%, 99.7%)
01/19 04:05:56 AM | Train: [58/100] Step 000/390 Loss 0.105 Prec@(1,5) (96.9%, 100.0%)
01/19 04:05:59 AM | Train: [59/100] Step 100/390 Loss 0.101 Prec@(1,5) (96.6%, 100.0%)
01/19 04:06:00 AM | Train: [58/100] Step 100/390 Loss 0.290 Prec@(1,5) (90.1%, 99.8%)
01/19 04:06:02 AM | Train: [58/100] Step 200/390 Loss 0.301 Prec@(1,5) (89.7%, 99.7%)
01/19 04:06:03 AM | Train: [58/100] Step 100/390 Loss 0.099 Prec@(1,5) (96.6%, 100.0%)
01/19 04:06:05 AM | Train: [59/100] Step 200/390 Loss 0.100 Prec@(1,5) (96.5%, 100.0%)
01/19 04:06:06 AM | Train: [58/100] Step 200/390 Loss 0.292 Prec@(1,5) (89.9%, 99.8%)
01/19 04:06:08 AM | Train: [58/100] Step 300/390 Loss 0.298 Prec@(1,5) (89.7%, 99.7%)
01/19 04:06:09 AM | Train: [58/100] Step 200/390 Loss 0.101 Prec@(1,5) (96.6%, 100.0%)
01/19 04:06:11 AM | Train: [59/100] Step 300/390 Loss 0.102 Prec@(1,5) (96.5%, 100.0%)
01/19 04:06:12 AM | Train: [58/100] Step 300/390 Loss 0.294 Prec@(1,5) (89.8%, 99.8%)
01/19 04:06:13 AM | Train: [58/100] Step 390/390 Loss 0.298 Prec@(1,5) (89.7%, 99.7%)
01/19 04:06:14 AM | Train: [58/100] Final Prec@1 89.7320%
01/19 04:06:14 AM | Valid: [58/100] Step 000/078 Loss 0.312 Prec@(1,5) (90.6%, 98.4%)
01/19 04:06:15 AM | Train: [58/100] Step 300/390 Loss 0.103 Prec@(1,5) (96.4%, 100.0%)
01/19 04:06:16 AM | Valid: [58/100] Step 078/078 Loss 0.407 Prec@(1,5) (86.5%, 99.4%)
01/19 04:06:16 AM | Valid: [58/100] Final Prec@1 86.5200%
01/19 04:06:16 AM | Current best Prec@1 = 86.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:06:17 AM | Train: [59/100] Step 390/390 Loss 0.104 Prec@(1,5) (96.4%, 100.0%)
01/19 04:06:17 AM | Train: [59/100] Final Prec@1 96.3740%
01/19 04:06:17 AM | Train: [59/100] Step 000/390 Loss 0.367 Prec@(1,5) (85.9%, 100.0%)
01/19 04:06:17 AM | Train: [58/100] Step 390/390 Loss 0.295 Prec@(1,5) (89.7%, 99.8%)
01/19 04:06:17 AM | Valid: [59/100] Step 000/078 Loss 0.305 Prec@(1,5) (91.4%, 98.4%)
01/19 04:06:17 AM | Train: [58/100] Final Prec@1 89.6760%
01/19 04:06:18 AM | Valid: [58/100] Step 000/078 Loss 0.338 Prec@(1,5) (89.1%, 99.2%)
01/19 04:06:19 AM | Valid: [59/100] Step 078/078 Loss 0.338 Prec@(1,5) (90.1%, 99.7%)
01/19 04:06:19 AM | Valid: [59/100] Final Prec@1 90.1000%
01/19 04:06:19 AM | Current best Prec@1 = 90.2100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:06:19 AM | Valid: [58/100] Step 078/078 Loss 0.418 Prec@(1,5) (86.4%, 99.4%)
01/19 04:06:20 AM | Valid: [58/100] Final Prec@1 86.3800%
01/19 04:06:20 AM | Train: [58/100] Step 390/390 Loss 0.104 Prec@(1,5) (96.4%, 100.0%)
01/19 04:06:20 AM | Current best Prec@1 = 86.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:06:20 AM | Train: [58/100] Final Prec@1 96.4120%
01/19 04:06:20 AM | Train: [60/100] Step 000/390 Loss 0.138 Prec@(1,5) (95.3%, 100.0%)
01/19 04:06:20 AM | Valid: [58/100] Step 000/078 Loss 0.305 Prec@(1,5) (89.8%, 98.4%)
01/19 04:06:20 AM | Train: [59/100] Step 000/390 Loss 0.357 Prec@(1,5) (86.7%, 100.0%)
01/19 04:06:22 AM | Valid: [58/100] Step 078/078 Loss 0.318 Prec@(1,5) (90.2%, 99.6%)
01/19 04:06:22 AM | Valid: [58/100] Final Prec@1 90.2400%
01/19 04:06:22 AM | Current best Prec@1 = 90.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:06:22 AM | Train: [59/100] Step 100/390 Loss 0.278 Prec@(1,5) (90.3%, 99.8%)
01/19 04:06:23 AM | Train: [59/100] Step 000/390 Loss 0.209 Prec@(1,5) (93.0%, 100.0%)
01/19 04:06:26 AM | Train: [60/100] Step 100/390 Loss 0.095 Prec@(1,5) (96.8%, 100.0%)
01/19 04:06:26 AM | Train: [59/100] Step 100/390 Loss 0.270 Prec@(1,5) (90.6%, 99.8%)
01/19 04:06:28 AM | Train: [59/100] Step 200/390 Loss 0.283 Prec@(1,5) (90.1%, 99.8%)
01/19 04:06:29 AM | Train: [59/100] Step 100/390 Loss 0.091 Prec@(1,5) (96.8%, 100.0%)
01/19 04:06:32 AM | Train: [60/100] Step 200/390 Loss 0.092 Prec@(1,5) (96.9%, 100.0%)
01/19 04:06:33 AM | Train: [59/100] Step 200/390 Loss 0.278 Prec@(1,5) (90.4%, 99.8%)
01/19 04:06:34 AM | Train: [59/100] Step 300/390 Loss 0.289 Prec@(1,5) (89.9%, 99.8%)
01/19 04:06:35 AM | Train: [59/100] Step 200/390 Loss 0.091 Prec@(1,5) (96.8%, 100.0%)
01/19 04:06:38 AM | Train: [60/100] Step 300/390 Loss 0.092 Prec@(1,5) (96.9%, 100.0%)
01/19 04:06:39 AM | Train: [59/100] Step 300/390 Loss 0.283 Prec@(1,5) (90.1%, 99.8%)
01/19 04:06:40 AM | Train: [59/100] Step 390/390 Loss 0.289 Prec@(1,5) (89.9%, 99.8%)
01/19 04:06:40 AM | Train: [59/100] Final Prec@1 89.8880%
01/19 04:06:40 AM | Valid: [59/100] Step 000/078 Loss 0.325 Prec@(1,5) (89.1%, 100.0%)
01/19 04:06:41 AM | Train: [59/100] Step 300/390 Loss 0.095 Prec@(1,5) (96.7%, 100.0%)
01/19 04:06:42 AM | Valid: [59/100] Step 078/078 Loss 0.413 Prec@(1,5) (86.3%, 99.5%)
01/19 04:06:42 AM | Valid: [59/100] Final Prec@1 86.3000%
01/19 04:06:42 AM | Current best Prec@1 = 86.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:06:43 AM | Train: [60/100] Step 000/390 Loss 0.323 Prec@(1,5) (90.6%, 100.0%)
01/19 04:06:43 AM | Train: [60/100] Step 390/390 Loss 0.096 Prec@(1,5) (96.8%, 100.0%)
01/19 04:06:43 AM | Train: [60/100] Final Prec@1 96.7600%
01/19 04:06:44 AM | Valid: [60/100] Step 000/078 Loss 0.204 Prec@(1,5) (92.2%, 98.4%)
01/19 04:06:44 AM | Train: [59/100] Step 390/390 Loss 0.283 Prec@(1,5) (90.1%, 99.8%)
01/19 04:06:44 AM | Train: [59/100] Final Prec@1 90.0820%
01/19 04:06:45 AM | Valid: [59/100] Step 000/078 Loss 0.363 Prec@(1,5) (88.3%, 100.0%)
01/19 04:06:45 AM | Valid: [60/100] Step 078/078 Loss 0.333 Prec@(1,5) (90.2%, 99.6%)
01/19 04:06:45 AM | Valid: [60/100] Final Prec@1 90.2300%
01/19 04:06:45 AM | Current best Prec@1 = 90.2300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:06:46 AM | Train: [61/100] Step 000/390 Loss 0.097 Prec@(1,5) (98.4%, 100.0%)
01/19 04:06:46 AM | Valid: [59/100] Step 078/078 Loss 0.405 Prec@(1,5) (86.8%, 99.4%)
01/19 04:06:46 AM | Valid: [59/100] Final Prec@1 86.8000%
01/19 04:06:46 AM | Train: [59/100] Step 390/390 Loss 0.098 Prec@(1,5) (96.5%, 100.0%)
01/19 04:06:47 AM | Current best Prec@1 = 86.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:06:47 AM | Train: [59/100] Final Prec@1 96.5480%
01/19 04:06:47 AM | Train: [60/100] Step 000/390 Loss 0.339 Prec@(1,5) (88.3%, 100.0%)
01/19 04:06:47 AM | Valid: [59/100] Step 000/078 Loss 0.334 Prec@(1,5) (89.8%, 99.2%)
01/19 04:06:48 AM | Train: [60/100] Step 100/390 Loss 0.268 Prec@(1,5) (90.7%, 99.8%)
01/19 04:06:49 AM | Valid: [59/100] Step 078/078 Loss 0.338 Prec@(1,5) (90.2%, 99.6%)
01/19 04:06:49 AM | Valid: [59/100] Final Prec@1 90.1900%
01/19 04:06:49 AM | Current best Prec@1 = 90.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:06:50 AM | Train: [60/100] Step 000/390 Loss 0.122 Prec@(1,5) (95.3%, 100.0%)
01/19 04:06:52 AM | Train: [61/100] Step 100/390 Loss 0.092 Prec@(1,5) (96.9%, 100.0%)
01/19 04:06:53 AM | Train: [60/100] Step 100/390 Loss 0.268 Prec@(1,5) (90.7%, 99.8%)
01/19 04:06:54 AM | Train: [60/100] Step 200/390 Loss 0.272 Prec@(1,5) (90.6%, 99.8%)
01/19 04:06:56 AM | Train: [60/100] Step 100/390 Loss 0.092 Prec@(1,5) (97.0%, 100.0%)
01/19 04:06:58 AM | Train: [61/100] Step 200/390 Loss 0.095 Prec@(1,5) (96.8%, 100.0%)
01/19 04:06:59 AM | Train: [60/100] Step 200/390 Loss 0.269 Prec@(1,5) (90.7%, 99.8%)
01/19 04:07:00 AM | Train: [60/100] Step 300/390 Loss 0.276 Prec@(1,5) (90.5%, 99.8%)
01/19 04:07:02 AM | Train: [60/100] Step 200/390 Loss 0.091 Prec@(1,5) (96.9%, 100.0%)
01/19 04:07:04 AM | Train: [61/100] Step 300/390 Loss 0.096 Prec@(1,5) (96.7%, 100.0%)
01/19 04:07:05 AM | Train: [60/100] Step 390/390 Loss 0.282 Prec@(1,5) (90.2%, 99.8%)
01/19 04:07:05 AM | Train: [60/100] Final Prec@1 90.2460%
01/19 04:07:05 AM | Train: [60/100] Step 300/390 Loss 0.274 Prec@(1,5) (90.4%, 99.8%)
01/19 04:07:06 AM | Valid: [60/100] Step 000/078 Loss 0.334 Prec@(1,5) (89.1%, 99.2%)
01/19 04:07:07 AM | Valid: [60/100] Step 078/078 Loss 0.425 Prec@(1,5) (86.0%, 99.5%)
01/19 04:07:07 AM | Valid: [60/100] Final Prec@1 85.9900%
01/19 04:07:08 AM | Current best Prec@1 = 86.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:07:08 AM | Train: [60/100] Step 300/390 Loss 0.092 Prec@(1,5) (96.9%, 100.0%)
01/19 04:07:08 AM | Train: [61/100] Step 000/390 Loss 0.249 Prec@(1,5) (90.6%, 100.0%)
01/19 04:07:09 AM | Train: [61/100] Step 390/390 Loss 0.096 Prec@(1,5) (96.7%, 100.0%)
01/19 04:07:09 AM | Train: [61/100] Final Prec@1 96.7380%
01/19 04:07:10 AM | Valid: [61/100] Step 000/078 Loss 0.203 Prec@(1,5) (92.2%, 100.0%)
01/19 04:07:11 AM | Train: [60/100] Step 390/390 Loss 0.279 Prec@(1,5) (90.3%, 99.8%)
01/19 04:07:11 AM | Train: [60/100] Final Prec@1 90.2660%
01/19 04:07:11 AM | Valid: [60/100] Step 000/078 Loss 0.428 Prec@(1,5) (89.8%, 100.0%)
01/19 04:07:12 AM | Valid: [61/100] Step 078/078 Loss 0.346 Prec@(1,5) (89.9%, 99.6%)
01/19 04:07:12 AM | Valid: [61/100] Final Prec@1 89.9300%
01/19 04:07:12 AM | Current best Prec@1 = 90.2300%
01/19 04:07:12 AM | Perform validation on training dataset. 
01/19 04:07:12 AM | Valid on training dataset: [61/100] Step 000/390 Loss 0.108 Prec@(1,5) (96.9%, 100.0%)
01/19 04:07:13 AM | Valid: [60/100] Step 078/078 Loss 0.433 Prec@(1,5) (85.4%, 99.3%)
01/19 04:07:13 AM | Valid: [60/100] Final Prec@1 85.3900%
01/19 04:07:13 AM | Current best Prec@1 = 86.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:07:13 AM | Train: [60/100] Step 390/390 Loss 0.095 Prec@(1,5) (96.7%, 100.0%)
01/19 04:07:14 AM | Train: [60/100] Final Prec@1 96.7340%
01/19 04:07:14 AM | Train: [61/100] Step 100/390 Loss 0.278 Prec@(1,5) (90.4%, 99.8%)
01/19 04:07:14 AM | Train: [61/100] Step 000/390 Loss 0.336 Prec@(1,5) (85.9%, 99.2%)
01/19 04:07:14 AM | Valid: [60/100] Step 000/078 Loss 0.232 Prec@(1,5) (93.0%, 99.2%)
01/19 04:07:15 AM | Valid on training dataset: [61/100] Step 100/390 Loss 0.092 Prec@(1,5) (96.8%, 100.0%)
01/19 04:07:16 AM | Valid: [60/100] Step 078/078 Loss 0.322 Prec@(1,5) (90.2%, 99.7%)
01/19 04:07:16 AM | Valid: [60/100] Final Prec@1 90.2100%
01/19 04:07:16 AM | Current best Prec@1 = 90.2400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:07:16 AM | Train: [61/100] Step 000/390 Loss 0.109 Prec@(1,5) (97.7%, 100.0%)
01/19 04:07:17 AM | Valid on training dataset: [61/100] Step 200/390 Loss 0.092 Prec@(1,5) (96.8%, 100.0%)
01/19 04:07:19 AM | Valid on training dataset: [61/100] Step 300/390 Loss 0.089 Prec@(1,5) (96.9%, 100.0%)
01/19 04:07:20 AM | Train: [61/100] Step 100/390 Loss 0.276 Prec@(1,5) (90.2%, 99.8%)
01/19 04:07:20 AM | Train: [61/100] Step 200/390 Loss 0.280 Prec@(1,5) (90.5%, 99.8%)
01/19 04:07:21 AM | Valid on training dataset: [61/100] Step 390/390 Loss 0.087 Prec@(1,5) (97.0%, 100.0%)
01/19 04:07:22 AM | Valid on training dataset: [61/100] Final Prec@1 96.9600%
01/19 04:07:22 AM | Final train Prec@1 = 96.9600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:07:22 AM | Train: [61/100] Step 100/390 Loss 0.097 Prec@(1,5) (96.8%, 100.0%)
01/19 04:07:22 AM | Train: [62/100] Step 000/390 Loss 0.059 Prec@(1,5) (98.4%, 100.0%)
01/19 04:07:26 AM | Train: [61/100] Step 200/390 Loss 0.276 Prec@(1,5) (90.3%, 99.8%)
01/19 04:07:27 AM | Train: [61/100] Step 300/390 Loss 0.279 Prec@(1,5) (90.5%, 99.8%)
01/19 04:07:28 AM | Train: [62/100] Step 100/390 Loss 0.085 Prec@(1,5) (97.2%, 100.0%)
01/19 04:07:28 AM | Train: [61/100] Step 200/390 Loss 0.094 Prec@(1,5) (96.9%, 100.0%)
01/19 04:07:32 AM | Train: [61/100] Step 300/390 Loss 0.274 Prec@(1,5) (90.3%, 99.8%)
01/19 04:07:32 AM | Train: [61/100] Step 390/390 Loss 0.280 Prec@(1,5) (90.3%, 99.8%)
01/19 04:07:32 AM | Train: [61/100] Final Prec@1 90.3320%
01/19 04:07:33 AM | Valid: [61/100] Step 000/078 Loss 0.326 Prec@(1,5) (88.3%, 100.0%)
01/19 04:07:34 AM | Train: [62/100] Step 200/390 Loss 0.089 Prec@(1,5) (97.0%, 100.0%)
01/19 04:07:34 AM | Train: [61/100] Step 300/390 Loss 0.096 Prec@(1,5) (96.7%, 100.0%)
01/19 04:07:35 AM | Valid: [61/100] Step 078/078 Loss 0.416 Prec@(1,5) (86.5%, 99.5%)
01/19 04:07:35 AM | Valid: [61/100] Final Prec@1 86.5100%
01/19 04:07:35 AM | Current best Prec@1 = 86.5200%
01/19 04:07:35 AM | Perform validation on training dataset. 
01/19 04:07:35 AM | Valid on training dataset: [61/100] Step 000/390 Loss 0.391 Prec@(1,5) (85.9%, 99.2%)
01/19 04:07:37 AM | Train: [61/100] Step 390/390 Loss 0.273 Prec@(1,5) (90.4%, 99.8%)
01/19 04:07:37 AM | Train: [61/100] Final Prec@1 90.3540%
01/19 04:07:37 AM | Valid: [61/100] Step 000/078 Loss 0.397 Prec@(1,5) (85.2%, 99.2%)
01/19 04:07:37 AM | Valid on training dataset: [61/100] Step 100/390 Loss 0.259 Prec@(1,5) (90.9%, 99.8%)
01/19 04:07:39 AM | Valid: [61/100] Step 078/078 Loss 0.432 Prec@(1,5) (86.1%, 99.4%)
01/19 04:07:39 AM | Valid: [61/100] Final Prec@1 86.0900%
01/19 04:07:39 AM | Current best Prec@1 = 86.8000%
01/19 04:07:39 AM | Perform validation on training dataset. 
01/19 04:07:40 AM | Valid on training dataset: [61/100] Step 200/390 Loss 0.263 Prec@(1,5) (90.9%, 99.8%)
01/19 04:07:40 AM | Valid on training dataset: [61/100] Step 000/390 Loss 0.386 Prec@(1,5) (85.2%, 100.0%)
01/19 04:07:40 AM | Train: [62/100] Step 300/390 Loss 0.090 Prec@(1,5) (97.0%, 100.0%)
01/19 04:07:40 AM | Train: [61/100] Step 390/390 Loss 0.095 Prec@(1,5) (96.7%, 100.0%)
01/19 04:07:40 AM | Train: [61/100] Final Prec@1 96.7240%
01/19 04:07:41 AM | Valid: [61/100] Step 000/078 Loss 0.163 Prec@(1,5) (96.9%, 100.0%)
01/19 04:07:42 AM | Valid on training dataset: [61/100] Step 300/390 Loss 0.261 Prec@(1,5) (90.9%, 99.8%)
01/19 04:07:42 AM | Valid on training dataset: [61/100] Step 100/390 Loss 0.253 Prec@(1,5) (91.1%, 99.8%)
01/19 04:07:43 AM | Valid: [61/100] Step 078/078 Loss 0.319 Prec@(1,5) (90.4%, 99.7%)
01/19 04:07:43 AM | Valid: [61/100] Final Prec@1 90.4300%
01/19 04:07:43 AM | Current best Prec@1 = 90.4300%
01/19 04:07:43 AM | Perform validation on training dataset. 
01/19 04:07:43 AM | Valid on training dataset: [61/100] Step 000/390 Loss 0.115 Prec@(1,5) (96.9%, 100.0%)
01/19 04:07:44 AM | Valid on training dataset: [61/100] Step 390/390 Loss 0.259 Prec@(1,5) (91.0%, 99.8%)
01/19 04:07:44 AM | Valid on training dataset: [61/100] Final Prec@1 90.9500%
01/19 04:07:44 AM | Final train Prec@1 = 90.9500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:07:45 AM | Valid on training dataset: [61/100] Step 200/390 Loss 0.254 Prec@(1,5) (91.1%, 99.8%)
01/19 04:07:45 AM | Train: [62/100] Step 390/390 Loss 0.091 Prec@(1,5) (96.9%, 100.0%)
01/19 04:07:45 AM | Train: [62/100] Final Prec@1 96.9320%
01/19 04:07:45 AM | Train: [62/100] Step 000/390 Loss 0.319 Prec@(1,5) (86.7%, 100.0%)
01/19 04:07:46 AM | Valid: [62/100] Step 000/078 Loss 0.210 Prec@(1,5) (93.0%, 99.2%)
01/19 04:07:46 AM | Valid on training dataset: [61/100] Step 100/390 Loss 0.075 Prec@(1,5) (97.6%, 100.0%)
01/19 04:07:47 AM | Valid on training dataset: [61/100] Step 300/390 Loss 0.252 Prec@(1,5) (91.1%, 99.8%)
01/19 04:07:47 AM | Valid: [62/100] Step 078/078 Loss 0.343 Prec@(1,5) (89.8%, 99.6%)
01/19 04:07:48 AM | Valid: [62/100] Final Prec@1 89.7800%
01/19 04:07:48 AM | Current best Prec@1 = 90.2300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:07:48 AM | Valid on training dataset: [61/100] Step 200/390 Loss 0.078 Prec@(1,5) (97.5%, 100.0%)
01/19 04:07:48 AM | Train: [63/100] Step 000/390 Loss 0.089 Prec@(1,5) (97.7%, 100.0%)
01/19 04:07:50 AM | Valid on training dataset: [61/100] Step 390/390 Loss 0.252 Prec@(1,5) (91.2%, 99.8%)
01/19 04:07:50 AM | Valid on training dataset: [61/100] Final Prec@1 91.1860%
01/19 04:07:50 AM | Final train Prec@1 = 91.1860%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:07:50 AM | Train: [62/100] Step 000/390 Loss 0.226 Prec@(1,5) (93.0%, 100.0%)
01/19 04:07:51 AM | Valid on training dataset: [61/100] Step 300/390 Loss 0.077 Prec@(1,5) (97.5%, 100.0%)
01/19 04:07:51 AM | Train: [62/100] Step 100/390 Loss 0.266 Prec@(1,5) (90.9%, 99.8%)
01/19 04:07:53 AM | Valid on training dataset: [61/100] Step 390/390 Loss 0.076 Prec@(1,5) (97.5%, 100.0%)
01/19 04:07:53 AM | Valid on training dataset: [61/100] Final Prec@1 97.5340%
01/19 04:07:53 AM | Final train Prec@1 = 97.5340%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:07:54 AM | Train: [62/100] Step 000/390 Loss 0.060 Prec@(1,5) (96.1%, 100.0%)
01/19 04:07:54 AM | Train: [63/100] Step 100/390 Loss 0.087 Prec@(1,5) (97.1%, 100.0%)
01/19 04:07:57 AM | Train: [62/100] Step 100/390 Loss 0.263 Prec@(1,5) (90.4%, 99.8%)
01/19 04:07:57 AM | Train: [62/100] Step 200/390 Loss 0.272 Prec@(1,5) (90.6%, 99.8%)
01/19 04:08:00 AM | Train: [62/100] Step 100/390 Loss 0.080 Prec@(1,5) (97.4%, 100.0%)
01/19 04:08:00 AM | Train: [63/100] Step 200/390 Loss 0.090 Prec@(1,5) (97.0%, 100.0%)
01/19 04:08:03 AM | Train: [62/100] Step 200/390 Loss 0.268 Prec@(1,5) (90.5%, 99.8%)
01/19 04:08:03 AM | Train: [62/100] Step 300/390 Loss 0.274 Prec@(1,5) (90.5%, 99.8%)
01/19 04:08:06 AM | Train: [62/100] Step 200/390 Loss 0.086 Prec@(1,5) (97.1%, 100.0%)
01/19 04:08:06 AM | Train: [63/100] Step 300/390 Loss 0.091 Prec@(1,5) (96.9%, 100.0%)
01/19 04:08:09 AM | Train: [62/100] Step 390/390 Loss 0.275 Prec@(1,5) (90.5%, 99.8%)
01/19 04:08:09 AM | Train: [62/100] Final Prec@1 90.5180%
01/19 04:08:09 AM | Train: [62/100] Step 300/390 Loss 0.272 Prec@(1,5) (90.4%, 99.8%)
01/19 04:08:09 AM | Valid: [62/100] Step 000/078 Loss 0.309 Prec@(1,5) (90.6%, 99.2%)
01/19 04:08:11 AM | Valid: [62/100] Step 078/078 Loss 0.400 Prec@(1,5) (86.8%, 99.4%)
01/19 04:08:11 AM | Valid: [62/100] Final Prec@1 86.7900%
01/19 04:08:11 AM | Current best Prec@1 = 86.7900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:08:12 AM | Train: [63/100] Step 000/390 Loss 0.246 Prec@(1,5) (91.4%, 100.0%)
01/19 04:08:12 AM | Train: [62/100] Step 300/390 Loss 0.088 Prec@(1,5) (97.0%, 100.0%)
01/19 04:08:12 AM | Train: [63/100] Step 390/390 Loss 0.089 Prec@(1,5) (97.0%, 100.0%)
01/19 04:08:12 AM | Train: [63/100] Final Prec@1 96.9560%
01/19 04:08:12 AM | Valid: [63/100] Step 000/078 Loss 0.167 Prec@(1,5) (94.5%, 100.0%)
01/19 04:08:14 AM | Valid: [63/100] Step 078/078 Loss 0.311 Prec@(1,5) (90.9%, 99.6%)
01/19 04:08:14 AM | Valid: [63/100] Final Prec@1 90.8600%
01/19 04:08:14 AM | Train: [62/100] Step 390/390 Loss 0.274 Prec@(1,5) (90.4%, 99.8%)
01/19 04:08:15 AM | Current best Prec@1 = 90.8600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:08:15 AM | Train: [62/100] Final Prec@1 90.3680%
01/19 04:08:15 AM | Train: [64/100] Step 000/390 Loss 0.085 Prec@(1,5) (97.7%, 100.0%)
01/19 04:08:15 AM | Valid: [62/100] Step 000/078 Loss 0.372 Prec@(1,5) (86.7%, 99.2%)
01/19 04:08:17 AM | Train: [62/100] Step 390/390 Loss 0.091 Prec@(1,5) (96.9%, 100.0%)
01/19 04:08:17 AM | Valid: [62/100] Step 078/078 Loss 0.427 Prec@(1,5) (86.2%, 99.5%)
01/19 04:08:17 AM | Train: [62/100] Final Prec@1 96.9400%
01/19 04:08:17 AM | Valid: [62/100] Final Prec@1 86.1600%
01/19 04:08:17 AM | Current best Prec@1 = 86.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:08:17 AM | Train: [63/100] Step 100/390 Loss 0.254 Prec@(1,5) (91.3%, 99.8%)
01/19 04:08:17 AM | Valid: [62/100] Step 000/078 Loss 0.272 Prec@(1,5) (93.0%, 99.2%)
01/19 04:08:18 AM | Train: [63/100] Step 000/390 Loss 0.197 Prec@(1,5) (93.8%, 100.0%)
01/19 04:08:19 AM | Valid: [62/100] Step 078/078 Loss 0.347 Prec@(1,5) (89.8%, 99.7%)
01/19 04:08:19 AM | Valid: [62/100] Final Prec@1 89.8000%
01/19 04:08:19 AM | Current best Prec@1 = 90.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:08:20 AM | Train: [63/100] Step 000/390 Loss 0.016 Prec@(1,5) (100.0%, 100.0%)
01/19 04:08:20 AM | Train: [64/100] Step 100/390 Loss 0.082 Prec@(1,5) (97.2%, 100.0%)
01/19 04:08:23 AM | Train: [63/100] Step 200/390 Loss 0.261 Prec@(1,5) (91.2%, 99.8%)
01/19 04:08:24 AM | Train: [63/100] Step 100/390 Loss 0.254 Prec@(1,5) (91.3%, 99.8%)
01/19 04:08:26 AM | Train: [63/100] Step 100/390 Loss 0.084 Prec@(1,5) (97.2%, 100.0%)
01/19 04:08:27 AM | Train: [64/100] Step 200/390 Loss 0.081 Prec@(1,5) (97.3%, 100.0%)
01/19 04:08:29 AM | Train: [63/100] Step 300/390 Loss 0.264 Prec@(1,5) (91.0%, 99.8%)
01/19 04:08:30 AM | Train: [63/100] Step 200/390 Loss 0.258 Prec@(1,5) (91.1%, 99.8%)
01/19 04:08:32 AM | Train: [63/100] Step 200/390 Loss 0.087 Prec@(1,5) (97.0%, 100.0%)
01/19 04:08:33 AM | Train: [64/100] Step 300/390 Loss 0.082 Prec@(1,5) (97.2%, 100.0%)
01/19 04:08:35 AM | Train: [63/100] Step 390/390 Loss 0.268 Prec@(1,5) (90.8%, 99.8%)
01/19 04:08:35 AM | Train: [63/100] Final Prec@1 90.7600%
01/19 04:08:35 AM | Valid: [63/100] Step 000/078 Loss 0.336 Prec@(1,5) (88.3%, 100.0%)
01/19 04:08:36 AM | Train: [63/100] Step 300/390 Loss 0.259 Prec@(1,5) (91.0%, 99.8%)
01/19 04:08:37 AM | Valid: [63/100] Step 078/078 Loss 0.405 Prec@(1,5) (86.9%, 99.5%)
01/19 04:08:37 AM | Valid: [63/100] Final Prec@1 86.9000%
01/19 04:08:37 AM | Current best Prec@1 = 86.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:08:38 AM | Train: [64/100] Step 000/390 Loss 0.167 Prec@(1,5) (94.5%, 100.0%)
01/19 04:08:38 AM | Train: [63/100] Step 300/390 Loss 0.085 Prec@(1,5) (97.1%, 100.0%)
01/19 04:08:38 AM | Train: [64/100] Step 390/390 Loss 0.083 Prec@(1,5) (97.2%, 100.0%)
01/19 04:08:39 AM | Train: [64/100] Final Prec@1 97.1940%
01/19 04:08:39 AM | Valid: [64/100] Step 000/078 Loss 0.200 Prec@(1,5) (93.8%, 100.0%)
01/19 04:08:41 AM | Valid: [64/100] Step 078/078 Loss 0.329 Prec@(1,5) (90.6%, 99.6%)
01/19 04:08:41 AM | Valid: [64/100] Final Prec@1 90.6400%
01/19 04:08:41 AM | Current best Prec@1 = 90.8600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:08:41 AM | Train: [63/100] Step 390/390 Loss 0.262 Prec@(1,5) (90.9%, 99.8%)
01/19 04:08:41 AM | Train: [65/100] Step 000/390 Loss 0.056 Prec@(1,5) (96.9%, 100.0%)
01/19 04:08:41 AM | Train: [63/100] Final Prec@1 90.9260%
01/19 04:08:42 AM | Valid: [63/100] Step 000/078 Loss 0.322 Prec@(1,5) (88.3%, 99.2%)
01/19 04:08:43 AM | Train: [63/100] Step 390/390 Loss 0.085 Prec@(1,5) (97.1%, 100.0%)
01/19 04:08:43 AM | Train: [63/100] Final Prec@1 97.1220%
01/19 04:08:43 AM | Train: [64/100] Step 100/390 Loss 0.261 Prec@(1,5) (90.9%, 99.8%)
01/19 04:08:44 AM | Valid: [63/100] Step 000/078 Loss 0.195 Prec@(1,5) (92.2%, 100.0%)
01/19 04:08:44 AM | Valid: [63/100] Step 078/078 Loss 0.392 Prec@(1,5) (87.2%, 99.4%)
01/19 04:08:44 AM | Valid: [63/100] Final Prec@1 87.2500%
01/19 04:08:44 AM | Current best Prec@1 = 87.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:08:44 AM | Train: [64/100] Step 000/390 Loss 0.164 Prec@(1,5) (93.8%, 100.0%)
01/19 04:08:45 AM | Valid: [63/100] Step 078/078 Loss 0.325 Prec@(1,5) (90.3%, 99.6%)
01/19 04:08:45 AM | Valid: [63/100] Final Prec@1 90.2800%
01/19 04:08:45 AM | Current best Prec@1 = 90.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:08:46 AM | Train: [64/100] Step 000/390 Loss 0.052 Prec@(1,5) (98.4%, 100.0%)
01/19 04:08:47 AM | Train: [65/100] Step 100/390 Loss 0.075 Prec@(1,5) (97.5%, 100.0%)
01/19 04:08:49 AM | Train: [64/100] Step 200/390 Loss 0.257 Prec@(1,5) (90.9%, 99.8%)
01/19 04:08:51 AM | Train: [64/100] Step 100/390 Loss 0.255 Prec@(1,5) (91.2%, 99.8%)
01/19 04:08:52 AM | Train: [64/100] Step 100/390 Loss 0.078 Prec@(1,5) (97.4%, 100.0%)
01/19 04:08:53 AM | Train: [65/100] Step 200/390 Loss 0.081 Prec@(1,5) (97.2%, 100.0%)
01/19 04:08:55 AM | Train: [64/100] Step 300/390 Loss 0.259 Prec@(1,5) (90.9%, 99.8%)
01/19 04:08:57 AM | Train: [64/100] Step 200/390 Loss 0.256 Prec@(1,5) (91.1%, 99.8%)
01/19 04:08:58 AM | Train: [64/100] Step 200/390 Loss 0.079 Prec@(1,5) (97.3%, 100.0%)
01/19 04:08:59 AM | Train: [65/100] Step 300/390 Loss 0.082 Prec@(1,5) (97.2%, 100.0%)
01/19 04:09:01 AM | Train: [64/100] Step 390/390 Loss 0.261 Prec@(1,5) (90.9%, 99.8%)
01/19 04:09:01 AM | Train: [64/100] Final Prec@1 90.9020%
01/19 04:09:01 AM | Valid: [64/100] Step 000/078 Loss 0.390 Prec@(1,5) (89.8%, 99.2%)
01/19 04:09:03 AM | Train: [64/100] Step 300/390 Loss 0.259 Prec@(1,5) (91.0%, 99.8%)
01/19 04:09:03 AM | Valid: [64/100] Step 078/078 Loss 0.415 Prec@(1,5) (86.4%, 99.4%)
01/19 04:09:03 AM | Valid: [64/100] Final Prec@1 86.4100%
01/19 04:09:03 AM | Current best Prec@1 = 86.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:09:04 AM | Train: [65/100] Step 000/390 Loss 0.236 Prec@(1,5) (89.8%, 100.0%)
01/19 04:09:05 AM | Train: [64/100] Step 300/390 Loss 0.079 Prec@(1,5) (97.3%, 100.0%)
01/19 04:09:05 AM | Train: [65/100] Step 390/390 Loss 0.083 Prec@(1,5) (97.1%, 100.0%)
01/19 04:09:05 AM | Train: [65/100] Final Prec@1 97.0800%
01/19 04:09:05 AM | Valid: [65/100] Step 000/078 Loss 0.263 Prec@(1,5) (90.6%, 100.0%)
01/19 04:09:07 AM | Valid: [65/100] Step 078/078 Loss 0.319 Prec@(1,5) (90.5%, 99.7%)
01/19 04:09:08 AM | Valid: [65/100] Final Prec@1 90.4500%
01/19 04:09:08 AM | Current best Prec@1 = 90.8600%
01/19 04:09:08 AM | Train: [64/100] Step 390/390 Loss 0.260 Prec@(1,5) (90.9%, 99.8%)
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:09:08 AM | Train: [64/100] Final Prec@1 90.9320%
01/19 04:09:08 AM | Train: [66/100] Step 000/390 Loss 0.094 Prec@(1,5) (97.7%, 100.0%)
01/19 04:09:08 AM | Valid: [64/100] Step 000/078 Loss 0.456 Prec@(1,5) (85.9%, 99.2%)
01/19 04:09:10 AM | Train: [64/100] Step 390/390 Loss 0.080 Prec@(1,5) (97.3%, 100.0%)
01/19 04:09:10 AM | Train: [65/100] Step 100/390 Loss 0.250 Prec@(1,5) (91.3%, 99.8%)
01/19 04:09:10 AM | Train: [64/100] Final Prec@1 97.2660%
01/19 04:09:10 AM | Valid: [64/100] Step 078/078 Loss 0.400 Prec@(1,5) (87.0%, 99.5%)
01/19 04:09:10 AM | Valid: [64/100] Final Prec@1 86.9800%
01/19 04:09:10 AM | Valid: [64/100] Step 000/078 Loss 0.217 Prec@(1,5) (93.8%, 99.2%)
01/19 04:09:11 AM | Current best Prec@1 = 87.2500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:09:11 AM | Train: [65/100] Step 000/390 Loss 0.157 Prec@(1,5) (95.3%, 100.0%)
01/19 04:09:12 AM | Valid: [64/100] Step 078/078 Loss 0.316 Prec@(1,5) (90.8%, 99.6%)
01/19 04:09:12 AM | Valid: [64/100] Final Prec@1 90.8000%
01/19 04:09:12 AM | Current best Prec@1 = 90.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:09:13 AM | Train: [65/100] Step 000/390 Loss 0.024 Prec@(1,5) (100.0%, 100.0%)
01/19 04:09:13 AM | Train: [66/100] Step 100/390 Loss 0.069 Prec@(1,5) (97.7%, 100.0%)
01/19 04:09:16 AM | Train: [65/100] Step 200/390 Loss 0.252 Prec@(1,5) (91.1%, 99.8%)
01/19 04:09:17 AM | Train: [65/100] Step 100/390 Loss 0.253 Prec@(1,5) (91.4%, 99.8%)
01/19 04:09:19 AM | Train: [65/100] Step 100/390 Loss 0.076 Prec@(1,5) (97.6%, 100.0%)
01/19 04:09:19 AM | Train: [66/100] Step 200/390 Loss 0.071 Prec@(1,5) (97.6%, 100.0%)
01/19 04:09:21 AM | Train: [65/100] Step 300/390 Loss 0.256 Prec@(1,5) (91.0%, 99.8%)
01/19 04:09:23 AM | Train: [65/100] Step 200/390 Loss 0.252 Prec@(1,5) (91.3%, 99.8%)
01/19 04:09:25 AM | Train: [65/100] Step 200/390 Loss 0.079 Prec@(1,5) (97.4%, 100.0%)
01/19 04:09:26 AM | Train: [66/100] Step 300/390 Loss 0.070 Prec@(1,5) (97.7%, 100.0%)
01/19 04:09:27 AM | Train: [65/100] Step 390/390 Loss 0.259 Prec@(1,5) (91.0%, 99.8%)
01/19 04:09:27 AM | Train: [65/100] Final Prec@1 90.9500%
01/19 04:09:28 AM | Valid: [65/100] Step 000/078 Loss 0.329 Prec@(1,5) (93.0%, 100.0%)
01/19 04:09:29 AM | Valid: [65/100] Step 078/078 Loss 0.388 Prec@(1,5) (87.1%, 99.5%)
01/19 04:09:29 AM | Train: [65/100] Step 300/390 Loss 0.255 Prec@(1,5) (91.2%, 99.8%)
01/19 04:09:29 AM | Valid: [65/100] Final Prec@1 87.1300%
01/19 04:09:29 AM | Current best Prec@1 = 87.1300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:09:30 AM | Train: [66/100] Step 000/390 Loss 0.311 Prec@(1,5) (89.1%, 100.0%)
01/19 04:09:31 AM | Train: [66/100] Step 390/390 Loss 0.071 Prec@(1,5) (97.7%, 100.0%)
01/19 04:09:31 AM | Train: [65/100] Step 300/390 Loss 0.078 Prec@(1,5) (97.4%, 100.0%)
01/19 04:09:31 AM | Train: [66/100] Final Prec@1 97.6740%
01/19 04:09:32 AM | Valid: [66/100] Step 000/078 Loss 0.154 Prec@(1,5) (94.5%, 100.0%)
01/19 04:09:33 AM | Valid: [66/100] Step 078/078 Loss 0.313 Prec@(1,5) (90.8%, 99.7%)
01/19 04:09:33 AM | Valid: [66/100] Final Prec@1 90.8100%
01/19 04:09:34 AM | Current best Prec@1 = 90.8600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:09:34 AM | Train: [65/100] Step 390/390 Loss 0.255 Prec@(1,5) (91.1%, 99.8%)
01/19 04:09:34 AM | Train: [67/100] Step 000/390 Loss 0.043 Prec@(1,5) (98.4%, 100.0%)
01/19 04:09:34 AM | Train: [65/100] Final Prec@1 91.1400%
01/19 04:09:35 AM | Valid: [65/100] Step 000/078 Loss 0.336 Prec@(1,5) (88.3%, 99.2%)
01/19 04:09:36 AM | Train: [66/100] Step 100/390 Loss 0.230 Prec@(1,5) (91.9%, 99.9%)
01/19 04:09:36 AM | Train: [65/100] Step 390/390 Loss 0.079 Prec@(1,5) (97.4%, 100.0%)
01/19 04:09:36 AM | Valid: [65/100] Step 078/078 Loss 0.393 Prec@(1,5) (87.4%, 99.5%)
01/19 04:09:36 AM | Train: [65/100] Final Prec@1 97.3640%
01/19 04:09:36 AM | Valid: [65/100] Final Prec@1 87.3600%
01/19 04:09:37 AM | Current best Prec@1 = 87.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:09:37 AM | Valid: [65/100] Step 000/078 Loss 0.199 Prec@(1,5) (93.0%, 100.0%)
01/19 04:09:38 AM | Train: [66/100] Step 000/390 Loss 0.292 Prec@(1,5) (89.1%, 100.0%)
01/19 04:09:39 AM | Valid: [65/100] Step 078/078 Loss 0.329 Prec@(1,5) (90.2%, 99.6%)
01/19 04:09:39 AM | Valid: [65/100] Final Prec@1 90.1700%
01/19 04:09:39 AM | Current best Prec@1 = 90.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:09:39 AM | Train: [66/100] Step 000/390 Loss 0.137 Prec@(1,5) (94.5%, 100.0%)
01/19 04:09:40 AM | Train: [67/100] Step 100/390 Loss 0.065 Prec@(1,5) (97.7%, 100.0%)
01/19 04:09:41 AM | Train: [66/100] Step 200/390 Loss 0.237 Prec@(1,5) (91.8%, 99.9%)
01/19 04:09:44 AM | Train: [66/100] Step 100/390 Loss 0.235 Prec@(1,5) (91.8%, 99.8%)
01/19 04:09:45 AM | Train: [66/100] Step 100/390 Loss 0.069 Prec@(1,5) (97.8%, 100.0%)
01/19 04:09:46 AM | Train: [67/100] Step 200/390 Loss 0.070 Prec@(1,5) (97.6%, 100.0%)
01/19 04:09:48 AM | Train: [66/100] Step 300/390 Loss 0.241 Prec@(1,5) (91.6%, 99.9%)
01/19 04:09:50 AM | Train: [66/100] Step 200/390 Loss 0.235 Prec@(1,5) (91.9%, 99.8%)
01/19 04:09:51 AM | Train: [66/100] Step 200/390 Loss 0.069 Prec@(1,5) (97.7%, 100.0%)
01/19 04:09:52 AM | Train: [67/100] Step 300/390 Loss 0.069 Prec@(1,5) (97.7%, 100.0%)
01/19 04:09:53 AM | Train: [66/100] Step 390/390 Loss 0.246 Prec@(1,5) (91.4%, 99.9%)
01/19 04:09:53 AM | Train: [66/100] Final Prec@1 91.3680%
01/19 04:09:54 AM | Valid: [66/100] Step 000/078 Loss 0.270 Prec@(1,5) (89.8%, 100.0%)
01/19 04:09:55 AM | Valid: [66/100] Step 078/078 Loss 0.398 Prec@(1,5) (87.0%, 99.5%)
01/19 04:09:56 AM | Train: [66/100] Step 300/390 Loss 0.240 Prec@(1,5) (91.7%, 99.9%)
01/19 04:09:56 AM | Valid: [66/100] Final Prec@1 87.0200%
01/19 04:09:56 AM | Current best Prec@1 = 87.1300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:09:56 AM | Train: [67/100] Step 000/390 Loss 0.137 Prec@(1,5) (95.3%, 100.0%)
01/19 04:09:57 AM | Train: [67/100] Step 390/390 Loss 0.070 Prec@(1,5) (97.7%, 100.0%)
01/19 04:09:57 AM | Train: [67/100] Final Prec@1 97.6500%
01/19 04:09:57 AM | Train: [66/100] Step 300/390 Loss 0.067 Prec@(1,5) (97.8%, 100.0%)
01/19 04:09:58 AM | Valid: [67/100] Step 000/078 Loss 0.175 Prec@(1,5) (94.5%, 100.0%)
01/19 04:10:00 AM | Valid: [67/100] Step 078/078 Loss 0.327 Prec@(1,5) (90.5%, 99.7%)
01/19 04:10:00 AM | Valid: [67/100] Final Prec@1 90.4500%
01/19 04:10:00 AM | Current best Prec@1 = 90.8600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:10:01 AM | Train: [68/100] Step 000/390 Loss 0.021 Prec@(1,5) (100.0%, 100.0%)
01/19 04:10:01 AM | Train: [66/100] Step 390/390 Loss 0.242 Prec@(1,5) (91.6%, 99.9%)
01/19 04:10:01 AM | Train: [66/100] Final Prec@1 91.5880%
01/19 04:10:01 AM | Valid: [66/100] Step 000/078 Loss 0.300 Prec@(1,5) (86.7%, 99.2%)
01/19 04:10:02 AM | Train: [67/100] Step 100/390 Loss 0.236 Prec@(1,5) (91.9%, 99.8%)
01/19 04:10:03 AM | Train: [66/100] Step 390/390 Loss 0.069 Prec@(1,5) (97.7%, 100.0%)
01/19 04:10:03 AM | Valid: [66/100] Step 078/078 Loss 0.404 Prec@(1,5) (87.1%, 99.5%)
01/19 04:10:03 AM | Train: [66/100] Final Prec@1 97.7320%
01/19 04:10:03 AM | Valid: [66/100] Final Prec@1 87.1200%
01/19 04:10:03 AM | Current best Prec@1 = 87.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:10:03 AM | Valid: [66/100] Step 000/078 Loss 0.174 Prec@(1,5) (95.3%, 100.0%)
01/19 04:10:04 AM | Train: [67/100] Step 000/390 Loss 0.163 Prec@(1,5) (94.5%, 100.0%)
01/19 04:10:05 AM | Valid: [66/100] Step 078/078 Loss 0.330 Prec@(1,5) (90.4%, 99.7%)
01/19 04:10:05 AM | Valid: [66/100] Final Prec@1 90.4200%
01/19 04:10:05 AM | Current best Prec@1 = 90.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:10:06 AM | Train: [68/100] Step 100/390 Loss 0.064 Prec@(1,5) (98.0%, 100.0%)
01/19 04:10:06 AM | Train: [67/100] Step 000/390 Loss 0.047 Prec@(1,5) (99.2%, 100.0%)
01/19 04:10:08 AM | Train: [67/100] Step 200/390 Loss 0.243 Prec@(1,5) (91.5%, 99.8%)
01/19 04:10:10 AM | Train: [67/100] Step 100/390 Loss 0.227 Prec@(1,5) (91.9%, 99.9%)
01/19 04:10:12 AM | Train: [68/100] Step 200/390 Loss 0.066 Prec@(1,5) (97.9%, 100.0%)
01/19 04:10:12 AM | Train: [67/100] Step 100/390 Loss 0.063 Prec@(1,5) (97.8%, 100.0%)
01/19 04:10:14 AM | Train: [67/100] Step 300/390 Loss 0.246 Prec@(1,5) (91.4%, 99.8%)
01/19 04:10:16 AM | Train: [67/100] Step 200/390 Loss 0.235 Prec@(1,5) (91.7%, 99.8%)
01/19 04:10:18 AM | Train: [68/100] Step 300/390 Loss 0.067 Prec@(1,5) (97.8%, 100.0%)
01/19 04:10:18 AM | Train: [67/100] Step 200/390 Loss 0.063 Prec@(1,5) (97.8%, 100.0%)
01/19 04:10:19 AM | Train: [67/100] Step 390/390 Loss 0.247 Prec@(1,5) (91.4%, 99.8%)
01/19 04:10:20 AM | Train: [67/100] Final Prec@1 91.3540%
01/19 04:10:20 AM | Valid: [67/100] Step 000/078 Loss 0.251 Prec@(1,5) (91.4%, 100.0%)
01/19 04:10:21 AM | Valid: [67/100] Step 078/078 Loss 0.394 Prec@(1,5) (86.9%, 99.5%)
01/19 04:10:22 AM | Valid: [67/100] Final Prec@1 86.9400%
01/19 04:10:22 AM | Train: [67/100] Step 300/390 Loss 0.239 Prec@(1,5) (91.5%, 99.8%)
01/19 04:10:22 AM | Current best Prec@1 = 87.1300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:10:22 AM | Train: [68/100] Step 000/390 Loss 0.232 Prec@(1,5) (89.8%, 100.0%)
01/19 04:10:23 AM | Train: [68/100] Step 390/390 Loss 0.068 Prec@(1,5) (97.8%, 100.0%)
01/19 04:10:24 AM | Train: [68/100] Final Prec@1 97.7560%
01/19 04:10:24 AM | Valid: [68/100] Step 000/078 Loss 0.213 Prec@(1,5) (93.8%, 100.0%)
01/19 04:10:24 AM | Train: [67/100] Step 300/390 Loss 0.065 Prec@(1,5) (97.8%, 100.0%)
01/19 04:10:26 AM | Valid: [68/100] Step 078/078 Loss 0.334 Prec@(1,5) (90.3%, 99.5%)
01/19 04:10:26 AM | Valid: [68/100] Final Prec@1 90.2900%
01/19 04:10:26 AM | Current best Prec@1 = 90.8600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:10:27 AM | Train: [67/100] Step 390/390 Loss 0.241 Prec@(1,5) (91.4%, 99.8%)
01/19 04:10:27 AM | Train: [69/100] Step 000/390 Loss 0.067 Prec@(1,5) (97.7%, 100.0%)
01/19 04:10:27 AM | Train: [67/100] Final Prec@1 91.3880%
01/19 04:10:28 AM | Valid: [67/100] Step 000/078 Loss 0.334 Prec@(1,5) (93.0%, 99.2%)
01/19 04:10:28 AM | Train: [68/100] Step 100/390 Loss 0.229 Prec@(1,5) (92.1%, 99.9%)
01/19 04:10:29 AM | Valid: [67/100] Step 078/078 Loss 0.397 Prec@(1,5) (87.1%, 99.5%)
01/19 04:10:29 AM | Valid: [67/100] Final Prec@1 87.1400%
01/19 04:10:29 AM | Current best Prec@1 = 87.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:10:30 AM | Train: [67/100] Step 390/390 Loss 0.066 Prec@(1,5) (97.8%, 100.0%)
01/19 04:10:30 AM | Train: [67/100] Final Prec@1 97.8020%
01/19 04:10:30 AM | Train: [68/100] Step 000/390 Loss 0.199 Prec@(1,5) (94.5%, 100.0%)
01/19 04:10:30 AM | Valid: [67/100] Step 000/078 Loss 0.205 Prec@(1,5) (93.8%, 100.0%)
01/19 04:10:32 AM | Valid: [67/100] Step 078/078 Loss 0.324 Prec@(1,5) (90.6%, 99.6%)
01/19 04:10:32 AM | Valid: [67/100] Final Prec@1 90.6000%
01/19 04:10:32 AM | Current best Prec@1 = 90.8000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:10:33 AM | Train: [69/100] Step 100/390 Loss 0.064 Prec@(1,5) (97.7%, 100.0%)
01/19 04:10:33 AM | Train: [68/100] Step 000/390 Loss 0.059 Prec@(1,5) (98.4%, 100.0%)
01/19 04:10:34 AM | Train: [68/100] Step 200/390 Loss 0.238 Prec@(1,5) (91.8%, 99.9%)
01/19 04:10:36 AM | Train: [68/100] Step 100/390 Loss 0.228 Prec@(1,5) (91.9%, 99.8%)
01/19 04:10:39 AM | Train: [68/100] Step 100/390 Loss 0.056 Prec@(1,5) (98.2%, 100.0%)
01/19 04:10:39 AM | Train: [69/100] Step 200/390 Loss 0.065 Prec@(1,5) (97.8%, 100.0%)
01/19 04:10:40 AM | Train: [68/100] Step 300/390 Loss 0.239 Prec@(1,5) (91.7%, 99.9%)
01/19 04:10:42 AM | Train: [68/100] Step 200/390 Loss 0.239 Prec@(1,5) (91.6%, 99.8%)
01/19 04:10:45 AM | Train: [68/100] Step 200/390 Loss 0.061 Prec@(1,5) (98.0%, 100.0%)
01/19 04:10:45 AM | Train: [69/100] Step 300/390 Loss 0.063 Prec@(1,5) (97.9%, 100.0%)
01/19 04:10:45 AM | Train: [68/100] Step 390/390 Loss 0.243 Prec@(1,5) (91.6%, 99.8%)
01/19 04:10:46 AM | Train: [68/100] Final Prec@1 91.5540%
01/19 04:10:46 AM | Valid: [68/100] Step 000/078 Loss 0.312 Prec@(1,5) (92.2%, 98.4%)
01/19 04:10:48 AM | Valid: [68/100] Step 078/078 Loss 0.399 Prec@(1,5) (87.1%, 99.5%)
01/19 04:10:48 AM | Train: [68/100] Step 300/390 Loss 0.239 Prec@(1,5) (91.6%, 99.8%)
01/19 04:10:48 AM | Valid: [68/100] Final Prec@1 87.1100%
01/19 04:10:48 AM | Current best Prec@1 = 87.1300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:10:49 AM | Train: [69/100] Step 000/390 Loss 0.260 Prec@(1,5) (90.6%, 99.2%)
01/19 04:10:51 AM | Train: [69/100] Step 390/390 Loss 0.062 Prec@(1,5) (97.9%, 100.0%)
01/19 04:10:51 AM | Train: [68/100] Step 300/390 Loss 0.063 Prec@(1,5) (98.0%, 100.0%)
01/19 04:10:51 AM | Train: [69/100] Final Prec@1 97.9280%
01/19 04:10:51 AM | Valid: [69/100] Step 000/078 Loss 0.196 Prec@(1,5) (95.3%, 100.0%)
01/19 04:10:53 AM | Valid: [69/100] Step 078/078 Loss 0.314 Prec@(1,5) (91.3%, 99.7%)
01/19 04:10:53 AM | Valid: [69/100] Final Prec@1 91.2800%
01/19 04:10:53 AM | Train: [68/100] Step 390/390 Loss 0.238 Prec@(1,5) (91.7%, 99.8%)
01/19 04:10:53 AM | Current best Prec@1 = 91.2800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:10:53 AM | Train: [68/100] Final Prec@1 91.6900%
01/19 04:10:54 AM | Train: [70/100] Step 000/390 Loss 0.022 Prec@(1,5) (99.2%, 100.0%)
01/19 04:10:54 AM | Valid: [68/100] Step 000/078 Loss 0.379 Prec@(1,5) (89.8%, 99.2%)
01/19 04:10:54 AM | Train: [69/100] Step 100/390 Loss 0.226 Prec@(1,5) (92.2%, 99.9%)
01/19 04:10:55 AM | Valid: [68/100] Step 078/078 Loss 0.404 Prec@(1,5) (87.0%, 99.5%)
01/19 04:10:56 AM | Train: [68/100] Step 390/390 Loss 0.063 Prec@(1,5) (97.9%, 100.0%)
01/19 04:10:56 AM | Valid: [68/100] Final Prec@1 87.0100%
01/19 04:10:56 AM | Train: [68/100] Final Prec@1 97.9140%
01/19 04:10:56 AM | Current best Prec@1 = 87.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:10:56 AM | Valid: [68/100] Step 000/078 Loss 0.276 Prec@(1,5) (93.0%, 100.0%)
01/19 04:10:57 AM | Train: [69/100] Step 000/390 Loss 0.232 Prec@(1,5) (93.0%, 100.0%)
01/19 04:10:58 AM | Valid: [68/100] Step 078/078 Loss 0.318 Prec@(1,5) (91.0%, 99.7%)
01/19 04:10:58 AM | Valid: [68/100] Final Prec@1 90.9700%
01/19 04:10:58 AM | Current best Prec@1 = 90.9700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:10:59 AM | Train: [69/100] Step 000/390 Loss 0.045 Prec@(1,5) (98.4%, 100.0%)
01/19 04:11:00 AM | Train: [70/100] Step 100/390 Loss 0.057 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:00 AM | Train: [69/100] Step 200/390 Loss 0.230 Prec@(1,5) (92.1%, 99.9%)
01/19 04:11:03 AM | Train: [69/100] Step 100/390 Loss 0.218 Prec@(1,5) (92.4%, 99.9%)
01/19 04:11:05 AM | Train: [69/100] Step 100/390 Loss 0.061 Prec@(1,5) (98.0%, 100.0%)
01/19 04:11:06 AM | Train: [70/100] Step 200/390 Loss 0.057 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:06 AM | Train: [69/100] Step 300/390 Loss 0.234 Prec@(1,5) (91.9%, 99.9%)
01/19 04:11:09 AM | Train: [69/100] Step 200/390 Loss 0.221 Prec@(1,5) (92.3%, 99.9%)
01/19 04:11:11 AM | Train: [69/100] Step 200/390 Loss 0.057 Prec@(1,5) (98.1%, 100.0%)
01/19 04:11:12 AM | Train: [70/100] Step 300/390 Loss 0.056 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:12 AM | Train: [69/100] Step 390/390 Loss 0.235 Prec@(1,5) (91.8%, 99.9%)
01/19 04:11:12 AM | Train: [69/100] Final Prec@1 91.8280%
01/19 04:11:12 AM | Valid: [69/100] Step 000/078 Loss 0.290 Prec@(1,5) (91.4%, 100.0%)
01/19 04:11:14 AM | Valid: [69/100] Step 078/078 Loss 0.408 Prec@(1,5) (87.0%, 99.5%)
01/19 04:11:14 AM | Valid: [69/100] Final Prec@1 86.9900%
01/19 04:11:14 AM | Current best Prec@1 = 87.1300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:11:14 AM | Train: [69/100] Step 300/390 Loss 0.224 Prec@(1,5) (92.2%, 99.9%)
01/19 04:11:15 AM | Train: [70/100] Step 000/390 Loss 0.227 Prec@(1,5) (93.8%, 99.2%)
01/19 04:11:17 AM | Train: [70/100] Step 390/390 Loss 0.057 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:17 AM | Train: [70/100] Final Prec@1 98.1640%
01/19 04:11:17 AM | Train: [69/100] Step 300/390 Loss 0.057 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:18 AM | Valid: [70/100] Step 000/078 Loss 0.160 Prec@(1,5) (93.0%, 100.0%)
01/19 04:11:19 AM | Valid: [70/100] Step 078/078 Loss 0.309 Prec@(1,5) (91.0%, 99.7%)
01/19 04:11:19 AM | Valid: [70/100] Final Prec@1 91.0400%
01/19 04:11:20 AM | Current best Prec@1 = 91.2800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:11:20 AM | Train: [69/100] Step 390/390 Loss 0.227 Prec@(1,5) (92.1%, 99.9%)
01/19 04:11:20 AM | Train: [69/100] Final Prec@1 92.0800%
01/19 04:11:20 AM | Train: [71/100] Step 000/390 Loss 0.052 Prec@(1,5) (98.4%, 100.0%)
01/19 04:11:20 AM | Train: [70/100] Step 100/390 Loss 0.229 Prec@(1,5) (92.2%, 99.8%)
01/19 04:11:20 AM | Valid: [69/100] Step 000/078 Loss 0.308 Prec@(1,5) (89.8%, 100.0%)
01/19 04:11:22 AM | Valid: [69/100] Step 078/078 Loss 0.407 Prec@(1,5) (87.3%, 99.4%)
01/19 04:11:22 AM | Valid: [69/100] Final Prec@1 87.2700%
01/19 04:11:22 AM | Train: [69/100] Step 390/390 Loss 0.056 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:22 AM | Current best Prec@1 = 87.3600%
01/19 04:11:22 AM | Train: [69/100] Final Prec@1 98.1960%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:11:23 AM | Valid: [69/100] Step 000/078 Loss 0.284 Prec@(1,5) (93.8%, 99.2%)
01/19 04:11:23 AM | Train: [70/100] Step 000/390 Loss 0.175 Prec@(1,5) (93.0%, 100.0%)
01/19 04:11:24 AM | Valid: [69/100] Step 078/078 Loss 0.308 Prec@(1,5) (91.2%, 99.6%)
01/19 04:11:24 AM | Valid: [69/100] Final Prec@1 91.2000%
01/19 04:11:25 AM | Current best Prec@1 = 91.2000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:11:25 AM | Train: [70/100] Step 000/390 Loss 0.030 Prec@(1,5) (99.2%, 100.0%)
01/19 04:11:26 AM | Train: [71/100] Step 100/390 Loss 0.053 Prec@(1,5) (98.3%, 100.0%)
01/19 04:11:26 AM | Train: [70/100] Step 200/390 Loss 0.224 Prec@(1,5) (92.5%, 99.8%)
01/19 04:11:29 AM | Train: [70/100] Step 100/390 Loss 0.223 Prec@(1,5) (92.3%, 99.9%)
01/19 04:11:31 AM | Train: [70/100] Step 100/390 Loss 0.053 Prec@(1,5) (98.3%, 100.0%)
01/19 04:11:32 AM | Train: [70/100] Step 300/390 Loss 0.225 Prec@(1,5) (92.4%, 99.8%)
01/19 04:11:32 AM | Train: [71/100] Step 200/390 Loss 0.055 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:35 AM | Train: [70/100] Step 200/390 Loss 0.220 Prec@(1,5) (92.4%, 99.9%)
01/19 04:11:37 AM | Train: [70/100] Step 200/390 Loss 0.055 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:37 AM | Train: [70/100] Step 390/390 Loss 0.228 Prec@(1,5) (92.2%, 99.8%)
01/19 04:11:38 AM | Train: [70/100] Final Prec@1 92.2220%
01/19 04:11:38 AM | Train: [71/100] Step 300/390 Loss 0.054 Prec@(1,5) (98.3%, 100.0%)
01/19 04:11:38 AM | Valid: [70/100] Step 000/078 Loss 0.350 Prec@(1,5) (90.6%, 99.2%)
01/19 04:11:40 AM | Valid: [70/100] Step 078/078 Loss 0.391 Prec@(1,5) (87.5%, 99.5%)
01/19 04:11:40 AM | Valid: [70/100] Final Prec@1 87.5200%
01/19 04:11:40 AM | Current best Prec@1 = 87.5200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:11:41 AM | Train: [70/100] Step 300/390 Loss 0.223 Prec@(1,5) (92.3%, 99.9%)
01/19 04:11:41 AM | Train: [71/100] Step 000/390 Loss 0.169 Prec@(1,5) (93.8%, 100.0%)
01/19 04:11:43 AM | Train: [71/100] Step 390/390 Loss 0.054 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:43 AM | Train: [70/100] Step 300/390 Loss 0.055 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:43 AM | Train: [71/100] Final Prec@1 98.2300%
01/19 04:11:44 AM | Valid: [71/100] Step 000/078 Loss 0.142 Prec@(1,5) (94.5%, 100.0%)
01/19 04:11:46 AM | Valid: [71/100] Step 078/078 Loss 0.308 Prec@(1,5) (91.1%, 99.7%)
01/19 04:11:46 AM | Valid: [71/100] Final Prec@1 91.1000%
01/19 04:11:46 AM | Current best Prec@1 = 91.2800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:11:46 AM | Train: [70/100] Step 390/390 Loss 0.223 Prec@(1,5) (92.3%, 99.9%)
01/19 04:11:46 AM | Train: [70/100] Final Prec@1 92.2540%
01/19 04:11:47 AM | Train: [72/100] Step 000/390 Loss 0.082 Prec@(1,5) (97.7%, 100.0%)
01/19 04:11:47 AM | Train: [71/100] Step 100/390 Loss 0.216 Prec@(1,5) (92.4%, 99.9%)
01/19 04:11:47 AM | Valid: [70/100] Step 000/078 Loss 0.320 Prec@(1,5) (91.4%, 100.0%)
01/19 04:11:49 AM | Valid: [70/100] Step 078/078 Loss 0.418 Prec@(1,5) (86.9%, 99.5%)
01/19 04:11:49 AM | Train: [70/100] Step 390/390 Loss 0.056 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:49 AM | Valid: [70/100] Final Prec@1 86.9000%
01/19 04:11:49 AM | Current best Prec@1 = 87.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:11:49 AM | Train: [70/100] Final Prec@1 98.1560%
01/19 04:11:49 AM | Train: [71/100] Step 000/390 Loss 0.160 Prec@(1,5) (95.3%, 100.0%)
01/19 04:11:49 AM | Valid: [70/100] Step 000/078 Loss 0.280 Prec@(1,5) (93.8%, 99.2%)
01/19 04:11:51 AM | Valid: [70/100] Step 078/078 Loss 0.322 Prec@(1,5) (91.3%, 99.6%)
01/19 04:11:51 AM | Valid: [70/100] Final Prec@1 91.3200%
01/19 04:11:51 AM | Current best Prec@1 = 91.3200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:11:52 AM | Train: [71/100] Step 000/390 Loss 0.061 Prec@(1,5) (97.7%, 100.0%)
01/19 04:11:52 AM | Train: [72/100] Step 100/390 Loss 0.056 Prec@(1,5) (98.1%, 100.0%)
01/19 04:11:52 AM | Train: [71/100] Step 200/390 Loss 0.217 Prec@(1,5) (92.5%, 99.9%)
01/19 04:11:56 AM | Train: [71/100] Step 100/390 Loss 0.219 Prec@(1,5) (92.2%, 99.9%)
01/19 04:11:58 AM | Train: [72/100] Step 200/390 Loss 0.053 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:58 AM | Train: [71/100] Step 100/390 Loss 0.053 Prec@(1,5) (98.2%, 100.0%)
01/19 04:11:59 AM | Train: [71/100] Step 300/390 Loss 0.216 Prec@(1,5) (92.6%, 99.9%)
01/19 04:12:01 AM | Train: [71/100] Step 200/390 Loss 0.215 Prec@(1,5) (92.4%, 99.9%)
01/19 04:12:04 AM | Train: [72/100] Step 300/390 Loss 0.052 Prec@(1,5) (98.3%, 100.0%)
01/19 04:12:04 AM | Train: [71/100] Step 200/390 Loss 0.056 Prec@(1,5) (98.1%, 100.0%)
01/19 04:12:04 AM | Train: [71/100] Step 390/390 Loss 0.217 Prec@(1,5) (92.5%, 99.9%)
01/19 04:12:04 AM | Train: [71/100] Final Prec@1 92.5000%
01/19 04:12:05 AM | Valid: [71/100] Step 000/078 Loss 0.303 Prec@(1,5) (88.3%, 99.2%)
01/19 04:12:06 AM | Valid: [71/100] Step 078/078 Loss 0.381 Prec@(1,5) (88.0%, 99.6%)
01/19 04:12:07 AM | Valid: [71/100] Final Prec@1 87.9600%
01/19 04:12:07 AM | Train: [71/100] Step 300/390 Loss 0.213 Prec@(1,5) (92.5%, 99.9%)
01/19 04:12:07 AM | Current best Prec@1 = 87.9600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:12:08 AM | Train: [72/100] Step 000/390 Loss 0.185 Prec@(1,5) (90.6%, 100.0%)
01/19 04:12:10 AM | Train: [72/100] Step 390/390 Loss 0.052 Prec@(1,5) (98.3%, 100.0%)
01/19 04:12:10 AM | Train: [72/100] Final Prec@1 98.3240%
01/19 04:12:10 AM | Train: [71/100] Step 300/390 Loss 0.055 Prec@(1,5) (98.2%, 100.0%)
01/19 04:12:11 AM | Valid: [72/100] Step 000/078 Loss 0.187 Prec@(1,5) (93.0%, 100.0%)
01/19 04:12:12 AM | Valid: [72/100] Step 078/078 Loss 0.317 Prec@(1,5) (91.2%, 99.7%)
01/19 04:12:12 AM | Valid: [72/100] Final Prec@1 91.1900%
01/19 04:12:12 AM | Train: [71/100] Step 390/390 Loss 0.215 Prec@(1,5) (92.4%, 99.9%)
01/19 04:12:12 AM | Current best Prec@1 = 91.2800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:12:12 AM | Train: [71/100] Final Prec@1 92.4180%
01/19 04:12:13 AM | Train: [73/100] Step 000/390 Loss 0.080 Prec@(1,5) (97.7%, 100.0%)
01/19 04:12:13 AM | Valid: [71/100] Step 000/078 Loss 0.346 Prec@(1,5) (89.8%, 99.2%)
01/19 04:12:13 AM | Train: [72/100] Step 100/390 Loss 0.198 Prec@(1,5) (93.1%, 99.9%)
01/19 04:12:15 AM | Valid: [71/100] Step 078/078 Loss 0.388 Prec@(1,5) (87.7%, 99.5%)
01/19 04:12:15 AM | Valid: [71/100] Final Prec@1 87.6800%
01/19 04:12:15 AM | Train: [71/100] Step 390/390 Loss 0.056 Prec@(1,5) (98.1%, 100.0%)
01/19 04:12:15 AM | Current best Prec@1 = 87.6800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:12:15 AM | Train: [71/100] Final Prec@1 98.1480%
01/19 04:12:16 AM | Valid: [71/100] Step 000/078 Loss 0.232 Prec@(1,5) (93.8%, 99.2%)
01/19 04:12:16 AM | Train: [72/100] Step 000/390 Loss 0.183 Prec@(1,5) (94.5%, 100.0%)
01/19 04:12:17 AM | Valid: [71/100] Step 078/078 Loss 0.315 Prec@(1,5) (91.0%, 99.6%)
01/19 04:12:17 AM | Valid: [71/100] Final Prec@1 91.0500%
01/19 04:12:17 AM | Current best Prec@1 = 91.3200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:12:18 AM | Train: [72/100] Step 000/390 Loss 0.064 Prec@(1,5) (96.9%, 100.0%)
01/19 04:12:19 AM | Train: [73/100] Step 100/390 Loss 0.048 Prec@(1,5) (98.4%, 100.0%)
01/19 04:12:19 AM | Train: [72/100] Step 200/390 Loss 0.201 Prec@(1,5) (93.0%, 99.9%)
01/19 04:12:22 AM | Train: [72/100] Step 100/390 Loss 0.193 Prec@(1,5) (93.4%, 99.9%)
01/19 04:12:24 AM | Train: [72/100] Step 100/390 Loss 0.051 Prec@(1,5) (98.3%, 100.0%)
01/19 04:12:25 AM | Train: [73/100] Step 200/390 Loss 0.043 Prec@(1,5) (98.7%, 100.0%)
01/19 04:12:25 AM | Train: [72/100] Step 300/390 Loss 0.209 Prec@(1,5) (92.7%, 99.9%)
01/19 04:12:28 AM | Train: [72/100] Step 200/390 Loss 0.192 Prec@(1,5) (93.4%, 99.9%)
01/19 04:12:30 AM | Train: [72/100] Step 200/390 Loss 0.049 Prec@(1,5) (98.4%, 100.0%)
01/19 04:12:30 AM | Train: [72/100] Step 390/390 Loss 0.212 Prec@(1,5) (92.6%, 99.9%)
01/19 04:12:31 AM | Train: [72/100] Final Prec@1 92.6060%
01/19 04:12:31 AM | Train: [73/100] Step 300/390 Loss 0.044 Prec@(1,5) (98.6%, 100.0%)
01/19 04:12:31 AM | Valid: [72/100] Step 000/078 Loss 0.268 Prec@(1,5) (92.2%, 100.0%)
01/19 04:12:32 AM | Valid: [72/100] Step 078/078 Loss 0.386 Prec@(1,5) (87.4%, 99.5%)
01/19 04:12:32 AM | Valid: [72/100] Final Prec@1 87.3700%
01/19 04:12:33 AM | Current best Prec@1 = 87.9600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:12:33 AM | Train: [73/100] Step 000/390 Loss 0.232 Prec@(1,5) (93.8%, 100.0%)
01/19 04:12:33 AM | Train: [72/100] Step 300/390 Loss 0.201 Prec@(1,5) (93.1%, 99.9%)
01/19 04:12:36 AM | Train: [72/100] Step 300/390 Loss 0.050 Prec@(1,5) (98.3%, 100.0%)
01/19 04:12:36 AM | Train: [73/100] Step 390/390 Loss 0.044 Prec@(1,5) (98.6%, 100.0%)
01/19 04:12:36 AM | Train: [73/100] Final Prec@1 98.5840%
01/19 04:12:37 AM | Valid: [73/100] Step 000/078 Loss 0.165 Prec@(1,5) (95.3%, 100.0%)
01/19 04:12:38 AM | Valid: [73/100] Step 078/078 Loss 0.315 Prec@(1,5) (91.6%, 99.7%)
01/19 04:12:38 AM | Valid: [73/100] Final Prec@1 91.6300%
01/19 04:12:39 AM | Current best Prec@1 = 91.6300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:12:39 AM | Train: [72/100] Step 390/390 Loss 0.205 Prec@(1,5) (92.9%, 99.9%)
01/19 04:12:39 AM | Train: [72/100] Final Prec@1 92.9480%
01/19 04:12:39 AM | Train: [73/100] Step 100/390 Loss 0.211 Prec@(1,5) (92.9%, 99.9%)
01/19 04:12:39 AM | Train: [74/100] Step 000/390 Loss 0.063 Prec@(1,5) (98.4%, 100.0%)
01/19 04:12:40 AM | Valid: [72/100] Step 000/078 Loss 0.303 Prec@(1,5) (89.8%, 100.0%)
01/19 04:12:41 AM | Train: [72/100] Step 390/390 Loss 0.049 Prec@(1,5) (98.4%, 100.0%)
01/19 04:12:41 AM | Valid: [72/100] Step 078/078 Loss 0.396 Prec@(1,5) (87.4%, 99.5%)
01/19 04:12:41 AM | Train: [72/100] Final Prec@1 98.4180%
01/19 04:12:41 AM | Valid: [72/100] Final Prec@1 87.4200%
01/19 04:12:41 AM | Current best Prec@1 = 87.6800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:12:42 AM | Valid: [72/100] Step 000/078 Loss 0.235 Prec@(1,5) (93.8%, 99.2%)
01/19 04:12:42 AM | Train: [73/100] Step 000/390 Loss 0.211 Prec@(1,5) (93.8%, 100.0%)
01/19 04:12:43 AM | Valid: [72/100] Step 078/078 Loss 0.301 Prec@(1,5) (91.5%, 99.7%)
01/19 04:12:43 AM | Valid: [72/100] Final Prec@1 91.5300%
01/19 04:12:44 AM | Current best Prec@1 = 91.5300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:12:44 AM | Train: [73/100] Step 000/390 Loss 0.082 Prec@(1,5) (96.9%, 100.0%)
01/19 04:12:44 AM | Train: [74/100] Step 100/390 Loss 0.039 Prec@(1,5) (98.8%, 100.0%)
01/19 04:12:45 AM | Train: [73/100] Step 200/390 Loss 0.202 Prec@(1,5) (93.1%, 99.9%)
01/19 04:12:48 AM | Train: [73/100] Step 100/390 Loss 0.205 Prec@(1,5) (93.0%, 99.9%)
01/19 04:12:50 AM | Train: [73/100] Step 100/390 Loss 0.044 Prec@(1,5) (98.6%, 100.0%)
01/19 04:12:50 AM | Train: [74/100] Step 200/390 Loss 0.039 Prec@(1,5) (98.9%, 100.0%)
01/19 04:12:51 AM | Train: [73/100] Step 300/390 Loss 0.205 Prec@(1,5) (93.0%, 99.9%)
01/19 04:12:54 AM | Train: [73/100] Step 200/390 Loss 0.195 Prec@(1,5) (93.3%, 99.9%)
01/19 04:12:56 AM | Train: [73/100] Step 390/390 Loss 0.205 Prec@(1,5) (92.9%, 99.9%)
01/19 04:12:56 AM | Train: [73/100] Final Prec@1 92.9500%
01/19 04:12:56 AM | Train: [73/100] Step 200/390 Loss 0.042 Prec@(1,5) (98.7%, 100.0%)
01/19 04:12:56 AM | Train: [74/100] Step 300/390 Loss 0.039 Prec@(1,5) (98.8%, 100.0%)
01/19 04:12:57 AM | Valid: [73/100] Step 000/078 Loss 0.317 Prec@(1,5) (89.8%, 100.0%)
01/19 04:12:58 AM | Valid: [73/100] Step 078/078 Loss 0.371 Prec@(1,5) (88.3%, 99.6%)
01/19 04:12:58 AM | Valid: [73/100] Final Prec@1 88.2700%
01/19 04:12:58 AM | Current best Prec@1 = 88.2700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:12:59 AM | Train: [74/100] Step 000/390 Loss 0.190 Prec@(1,5) (91.4%, 100.0%)
01/19 04:13:00 AM | Train: [73/100] Step 300/390 Loss 0.201 Prec@(1,5) (93.0%, 99.9%)
01/19 04:13:02 AM | Train: [74/100] Step 390/390 Loss 0.040 Prec@(1,5) (98.8%, 100.0%)
01/19 04:13:02 AM | Train: [74/100] Final Prec@1 98.7980%
01/19 04:13:02 AM | Train: [73/100] Step 300/390 Loss 0.042 Prec@(1,5) (98.6%, 100.0%)
01/19 04:13:03 AM | Valid: [74/100] Step 000/078 Loss 0.167 Prec@(1,5) (95.3%, 100.0%)
01/19 04:13:04 AM | Valid: [74/100] Step 078/078 Loss 0.314 Prec@(1,5) (91.1%, 99.7%)
01/19 04:13:05 AM | Valid: [74/100] Final Prec@1 91.1400%
01/19 04:13:05 AM | Current best Prec@1 = 91.6300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:13:05 AM | Train: [74/100] Step 100/390 Loss 0.187 Prec@(1,5) (93.4%, 99.9%)
01/19 04:13:05 AM | Train: [73/100] Step 390/390 Loss 0.200 Prec@(1,5) (93.1%, 99.9%)
01/19 04:13:05 AM | Train: [75/100] Step 000/390 Loss 0.034 Prec@(1,5) (99.2%, 100.0%)
01/19 04:13:05 AM | Train: [73/100] Final Prec@1 93.1460%
01/19 04:13:06 AM | Valid: [73/100] Step 000/078 Loss 0.289 Prec@(1,5) (89.1%, 100.0%)
01/19 04:13:07 AM | Train: [73/100] Step 390/390 Loss 0.043 Prec@(1,5) (98.6%, 100.0%)
01/19 04:13:07 AM | Train: [73/100] Final Prec@1 98.5820%
01/19 04:13:08 AM | Valid: [73/100] Step 078/078 Loss 0.387 Prec@(1,5) (87.8%, 99.5%)
01/19 04:13:08 AM | Valid: [73/100] Final Prec@1 87.7800%
01/19 04:13:08 AM | Current best Prec@1 = 87.7800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:13:08 AM | Valid: [73/100] Step 000/078 Loss 0.229 Prec@(1,5) (95.3%, 100.0%)
01/19 04:13:08 AM | Train: [74/100] Step 000/390 Loss 0.228 Prec@(1,5) (93.0%, 100.0%)
01/19 04:13:09 AM | Valid: [73/100] Step 078/078 Loss 0.304 Prec@(1,5) (91.9%, 99.7%)
01/19 04:13:10 AM | Valid: [73/100] Final Prec@1 91.8800%
01/19 04:13:10 AM | Current best Prec@1 = 91.8800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:13:10 AM | Train: [74/100] Step 000/390 Loss 0.091 Prec@(1,5) (97.7%, 100.0%)
01/19 04:13:10 AM | Train: [74/100] Step 200/390 Loss 0.192 Prec@(1,5) (93.2%, 99.9%)
01/19 04:13:11 AM | Train: [75/100] Step 100/390 Loss 0.039 Prec@(1,5) (98.7%, 100.0%)
01/19 04:13:14 AM | Train: [74/100] Step 100/390 Loss 0.181 Prec@(1,5) (93.8%, 99.9%)
01/19 04:13:16 AM | Train: [74/100] Step 100/390 Loss 0.039 Prec@(1,5) (98.8%, 100.0%)
01/19 04:13:16 AM | Train: [75/100] Step 200/390 Loss 0.041 Prec@(1,5) (98.7%, 100.0%)
01/19 04:13:17 AM | Train: [74/100] Step 300/390 Loss 0.196 Prec@(1,5) (93.1%, 99.9%)
01/19 04:13:20 AM | Train: [74/100] Step 200/390 Loss 0.185 Prec@(1,5) (93.6%, 99.9%)
01/19 04:13:22 AM | Train: [74/100] Step 390/390 Loss 0.198 Prec@(1,5) (93.0%, 99.9%)
01/19 04:13:22 AM | Train: [74/100] Final Prec@1 93.0160%
01/19 04:13:22 AM | Valid: [74/100] Step 000/078 Loss 0.255 Prec@(1,5) (90.6%, 100.0%)
01/19 04:13:22 AM | Train: [75/100] Step 300/390 Loss 0.041 Prec@(1,5) (98.7%, 100.0%)
01/19 04:13:23 AM | Train: [74/100] Step 200/390 Loss 0.041 Prec@(1,5) (98.8%, 100.0%)
01/19 04:13:24 AM | Valid: [74/100] Step 078/078 Loss 0.386 Prec@(1,5) (88.0%, 99.6%)
01/19 04:13:24 AM | Valid: [74/100] Final Prec@1 88.0200%
01/19 04:13:24 AM | Current best Prec@1 = 88.2700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:13:25 AM | Train: [75/100] Step 000/390 Loss 0.160 Prec@(1,5) (96.1%, 99.2%)
01/19 04:13:26 AM | Train: [74/100] Step 300/390 Loss 0.188 Prec@(1,5) (93.5%, 99.9%)
01/19 04:13:28 AM | Train: [75/100] Step 390/390 Loss 0.041 Prec@(1,5) (98.7%, 100.0%)
01/19 04:13:28 AM | Train: [75/100] Final Prec@1 98.6920%
01/19 04:13:28 AM | Valid: [75/100] Step 000/078 Loss 0.143 Prec@(1,5) (95.3%, 100.0%)
01/19 04:13:29 AM | Train: [74/100] Step 300/390 Loss 0.039 Prec@(1,5) (98.8%, 100.0%)
01/19 04:13:30 AM | Valid: [75/100] Step 078/078 Loss 0.316 Prec@(1,5) (91.0%, 99.6%)
01/19 04:13:30 AM | Valid: [75/100] Final Prec@1 91.0500%
01/19 04:13:30 AM | Current best Prec@1 = 91.6300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:13:31 AM | Train: [75/100] Step 100/390 Loss 0.193 Prec@(1,5) (93.3%, 99.9%)
01/19 04:13:31 AM | Train: [76/100] Step 000/390 Loss 0.045 Prec@(1,5) (98.4%, 100.0%)
01/19 04:13:31 AM | Train: [74/100] Step 390/390 Loss 0.192 Prec@(1,5) (93.4%, 99.9%)
01/19 04:13:31 AM | Train: [74/100] Final Prec@1 93.3960%
01/19 04:13:32 AM | Valid: [74/100] Step 000/078 Loss 0.312 Prec@(1,5) (90.6%, 99.2%)
01/19 04:13:33 AM | Valid: [74/100] Step 078/078 Loss 0.387 Prec@(1,5) (87.7%, 99.6%)
01/19 04:13:34 AM | Valid: [74/100] Final Prec@1 87.7000%
01/19 04:13:34 AM | Current best Prec@1 = 87.7800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:13:34 AM | Train: [74/100] Step 390/390 Loss 0.040 Prec@(1,5) (98.7%, 100.0%)
01/19 04:13:34 AM | Train: [74/100] Final Prec@1 98.7440%
01/19 04:13:35 AM | Train: [75/100] Step 000/390 Loss 0.127 Prec@(1,5) (96.1%, 100.0%)
01/19 04:13:35 AM | Valid: [74/100] Step 000/078 Loss 0.218 Prec@(1,5) (95.3%, 99.2%)
01/19 04:13:36 AM | Train: [75/100] Step 200/390 Loss 0.190 Prec@(1,5) (93.4%, 99.9%)
01/19 04:13:36 AM | Valid: [74/100] Step 078/078 Loss 0.302 Prec@(1,5) (91.5%, 99.7%)
01/19 04:13:36 AM | Valid: [74/100] Final Prec@1 91.5200%
01/19 04:13:36 AM | Current best Prec@1 = 91.8800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:13:37 AM | Train: [76/100] Step 100/390 Loss 0.037 Prec@(1,5) (98.9%, 100.0%)
01/19 04:13:37 AM | Train: [75/100] Step 000/390 Loss 0.018 Prec@(1,5) (100.0%, 100.0%)
01/19 04:13:40 AM | Train: [75/100] Step 100/390 Loss 0.182 Prec@(1,5) (93.9%, 99.9%)
01/19 04:13:42 AM | Train: [75/100] Step 300/390 Loss 0.195 Prec@(1,5) (93.2%, 99.9%)
01/19 04:13:43 AM | Train: [76/100] Step 200/390 Loss 0.037 Prec@(1,5) (98.9%, 100.0%)
01/19 04:13:43 AM | Train: [75/100] Step 100/390 Loss 0.038 Prec@(1,5) (98.9%, 100.0%)
01/19 04:13:46 AM | Train: [75/100] Step 200/390 Loss 0.185 Prec@(1,5) (93.7%, 99.9%)
01/19 04:13:48 AM | Train: [75/100] Step 390/390 Loss 0.195 Prec@(1,5) (93.2%, 99.9%)
01/19 04:13:48 AM | Train: [75/100] Final Prec@1 93.1840%
01/19 04:13:49 AM | Valid: [75/100] Step 000/078 Loss 0.254 Prec@(1,5) (91.4%, 99.2%)
01/19 04:13:49 AM | Train: [76/100] Step 300/390 Loss 0.037 Prec@(1,5) (98.8%, 100.0%)
01/19 04:13:49 AM | Train: [75/100] Step 200/390 Loss 0.038 Prec@(1,5) (98.9%, 100.0%)
01/19 04:13:50 AM | Valid: [75/100] Step 078/078 Loss 0.393 Prec@(1,5) (87.5%, 99.5%)
01/19 04:13:50 AM | Valid: [75/100] Final Prec@1 87.5300%
01/19 04:13:50 AM | Current best Prec@1 = 88.2700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:13:51 AM | Train: [76/100] Step 000/390 Loss 0.192 Prec@(1,5) (93.0%, 100.0%)
01/19 04:13:52 AM | Train: [75/100] Step 300/390 Loss 0.190 Prec@(1,5) (93.4%, 99.9%)
01/19 04:13:54 AM | Train: [76/100] Step 390/390 Loss 0.037 Prec@(1,5) (98.8%, 100.0%)
01/19 04:13:54 AM | Train: [76/100] Final Prec@1 98.8400%
01/19 04:13:55 AM | Valid: [76/100] Step 000/078 Loss 0.143 Prec@(1,5) (94.5%, 100.0%)
01/19 04:13:55 AM | Train: [75/100] Step 300/390 Loss 0.037 Prec@(1,5) (98.9%, 100.0%)
01/19 04:13:57 AM | Valid: [76/100] Step 078/078 Loss 0.306 Prec@(1,5) (91.6%, 99.7%)
01/19 04:13:57 AM | Valid: [76/100] Final Prec@1 91.6200%
01/19 04:13:57 AM | Train: [76/100] Step 100/390 Loss 0.178 Prec@(1,5) (93.9%, 99.9%)
01/19 04:13:57 AM | Current best Prec@1 = 91.6300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:13:57 AM | Train: [75/100] Step 390/390 Loss 0.190 Prec@(1,5) (93.4%, 99.9%)
01/19 04:13:58 AM | Train: [75/100] Final Prec@1 93.4260%
01/19 04:13:58 AM | Train: [77/100] Step 000/390 Loss 0.021 Prec@(1,5) (99.2%, 100.0%)
01/19 04:13:58 AM | Valid: [75/100] Step 000/078 Loss 0.254 Prec@(1,5) (90.6%, 100.0%)
01/19 04:14:00 AM | Valid: [75/100] Step 078/078 Loss 0.380 Prec@(1,5) (87.8%, 99.6%)
01/19 04:14:00 AM | Valid: [75/100] Final Prec@1 87.7500%
01/19 04:14:00 AM | Current best Prec@1 = 87.7800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:14:00 AM | Train: [75/100] Step 390/390 Loss 0.037 Prec@(1,5) (98.9%, 100.0%)
01/19 04:14:01 AM | Train: [75/100] Final Prec@1 98.8980%
01/19 04:14:01 AM | Train: [76/100] Step 000/390 Loss 0.175 Prec@(1,5) (93.8%, 100.0%)
01/19 04:14:01 AM | Valid: [75/100] Step 000/078 Loss 0.180 Prec@(1,5) (96.1%, 99.2%)
01/19 04:14:02 AM | Train: [76/100] Step 200/390 Loss 0.181 Prec@(1,5) (93.8%, 99.9%)
01/19 04:14:03 AM | Valid: [75/100] Step 078/078 Loss 0.311 Prec@(1,5) (91.4%, 99.6%)
01/19 04:14:03 AM | Valid: [75/100] Final Prec@1 91.4100%
01/19 04:14:03 AM | Current best Prec@1 = 91.8800%
01/19 04:14:03 AM | Train: [77/100] Step 100/390 Loss 0.033 Prec@(1,5) (99.0%, 100.0%)
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:14:04 AM | Train: [76/100] Step 000/390 Loss 0.049 Prec@(1,5) (99.2%, 100.0%)
01/19 04:14:07 AM | Train: [76/100] Step 100/390 Loss 0.176 Prec@(1,5) (93.8%, 100.0%)
01/19 04:14:08 AM | Train: [76/100] Step 300/390 Loss 0.184 Prec@(1,5) (93.7%, 99.9%)
01/19 04:14:09 AM | Train: [77/100] Step 200/390 Loss 0.034 Prec@(1,5) (98.9%, 100.0%)
01/19 04:14:10 AM | Train: [76/100] Step 100/390 Loss 0.032 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:13 AM | Train: [76/100] Step 200/390 Loss 0.180 Prec@(1,5) (93.8%, 99.9%)
01/19 04:14:14 AM | Train: [76/100] Step 390/390 Loss 0.185 Prec@(1,5) (93.6%, 99.9%)
01/19 04:14:14 AM | Train: [76/100] Final Prec@1 93.6400%
01/19 04:14:14 AM | Valid: [76/100] Step 000/078 Loss 0.289 Prec@(1,5) (89.8%, 99.2%)
01/19 04:14:15 AM | Train: [77/100] Step 300/390 Loss 0.032 Prec@(1,5) (99.0%, 100.0%)
01/19 04:14:16 AM | Valid: [76/100] Step 078/078 Loss 0.375 Prec@(1,5) (88.4%, 99.5%)
01/19 04:14:16 AM | Valid: [76/100] Final Prec@1 88.3600%
01/19 04:14:16 AM | Current best Prec@1 = 88.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:14:16 AM | Train: [76/100] Step 200/390 Loss 0.034 Prec@(1,5) (99.0%, 100.0%)
01/19 04:14:17 AM | Train: [77/100] Step 000/390 Loss 0.147 Prec@(1,5) (94.5%, 100.0%)
01/19 04:14:19 AM | Train: [76/100] Step 300/390 Loss 0.180 Prec@(1,5) (93.8%, 99.9%)
01/19 04:14:21 AM | Train: [77/100] Step 390/390 Loss 0.032 Prec@(1,5) (99.0%, 100.0%)
01/19 04:14:21 AM | Train: [77/100] Final Prec@1 99.0080%
01/19 04:14:21 AM | Valid: [77/100] Step 000/078 Loss 0.167 Prec@(1,5) (92.2%, 100.0%)
01/19 04:14:22 AM | Train: [76/100] Step 300/390 Loss 0.034 Prec@(1,5) (99.0%, 100.0%)
01/19 04:14:23 AM | Valid: [77/100] Step 078/078 Loss 0.300 Prec@(1,5) (91.7%, 99.6%)
01/19 04:14:23 AM | Valid: [77/100] Final Prec@1 91.7200%
01/19 04:14:23 AM | Train: [77/100] Step 100/390 Loss 0.175 Prec@(1,5) (93.9%, 99.9%)
01/19 04:14:23 AM | Current best Prec@1 = 91.7200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:14:24 AM | Train: [78/100] Step 000/390 Loss 0.056 Prec@(1,5) (98.4%, 100.0%)
01/19 04:14:24 AM | Train: [76/100] Step 390/390 Loss 0.181 Prec@(1,5) (93.8%, 99.9%)
01/19 04:14:24 AM | Train: [76/100] Final Prec@1 93.7880%
01/19 04:14:25 AM | Valid: [76/100] Step 000/078 Loss 0.267 Prec@(1,5) (92.2%, 100.0%)
01/19 04:14:26 AM | Valid: [76/100] Step 078/078 Loss 0.375 Prec@(1,5) (88.3%, 99.5%)
01/19 04:14:26 AM | Valid: [76/100] Final Prec@1 88.3400%
01/19 04:14:27 AM | Current best Prec@1 = 88.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:14:27 AM | Train: [76/100] Step 390/390 Loss 0.034 Prec@(1,5) (99.0%, 100.0%)
01/19 04:14:27 AM | Train: [77/100] Step 000/390 Loss 0.130 Prec@(1,5) (94.5%, 100.0%)
01/19 04:14:27 AM | Train: [76/100] Final Prec@1 98.9880%
01/19 04:14:28 AM | Valid: [76/100] Step 000/078 Loss 0.164 Prec@(1,5) (96.1%, 100.0%)
01/19 04:14:28 AM | Train: [77/100] Step 200/390 Loss 0.177 Prec@(1,5) (93.8%, 99.9%)
01/19 04:14:29 AM | Train: [78/100] Step 100/390 Loss 0.030 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:29 AM | Valid: [76/100] Step 078/078 Loss 0.306 Prec@(1,5) (91.5%, 99.6%)
01/19 04:14:29 AM | Valid: [76/100] Final Prec@1 91.5200%
01/19 04:14:30 AM | Current best Prec@1 = 91.8800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:14:30 AM | Train: [77/100] Step 000/390 Loss 0.042 Prec@(1,5) (98.4%, 100.0%)
01/19 04:14:34 AM | Train: [77/100] Step 100/390 Loss 0.167 Prec@(1,5) (94.0%, 100.0%)
01/19 04:14:35 AM | Train: [77/100] Step 300/390 Loss 0.176 Prec@(1,5) (93.9%, 99.9%)
01/19 04:14:35 AM | Train: [78/100] Step 200/390 Loss 0.031 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:36 AM | Train: [77/100] Step 100/390 Loss 0.031 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:40 AM | Train: [77/100] Step 200/390 Loss 0.169 Prec@(1,5) (94.1%, 100.0%)
01/19 04:14:40 AM | Train: [77/100] Step 390/390 Loss 0.177 Prec@(1,5) (93.9%, 99.9%)
01/19 04:14:40 AM | Train: [77/100] Final Prec@1 93.8900%
01/19 04:14:41 AM | Valid: [77/100] Step 000/078 Loss 0.256 Prec@(1,5) (92.2%, 100.0%)
01/19 04:14:41 AM | Train: [78/100] Step 300/390 Loss 0.031 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:42 AM | Valid: [77/100] Step 078/078 Loss 0.375 Prec@(1,5) (88.3%, 99.5%)
01/19 04:14:42 AM | Valid: [77/100] Final Prec@1 88.2900%
01/19 04:14:42 AM | Train: [77/100] Step 200/390 Loss 0.031 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:42 AM | Current best Prec@1 = 88.3600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:14:43 AM | Train: [78/100] Step 000/390 Loss 0.190 Prec@(1,5) (93.8%, 100.0%)
01/19 04:14:46 AM | Train: [77/100] Step 300/390 Loss 0.167 Prec@(1,5) (94.2%, 100.0%)
01/19 04:14:47 AM | Train: [78/100] Step 390/390 Loss 0.030 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:47 AM | Train: [78/100] Final Prec@1 99.1440%
01/19 04:14:47 AM | Valid: [78/100] Step 000/078 Loss 0.128 Prec@(1,5) (93.8%, 100.0%)
01/19 04:14:48 AM | Train: [77/100] Step 300/390 Loss 0.031 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:49 AM | Valid: [78/100] Step 078/078 Loss 0.301 Prec@(1,5) (91.9%, 99.6%)
01/19 04:14:49 AM | Valid: [78/100] Final Prec@1 91.9100%
01/19 04:14:49 AM | Current best Prec@1 = 91.9100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:14:49 AM | Train: [78/100] Step 100/390 Loss 0.170 Prec@(1,5) (94.1%, 99.9%)
01/19 04:14:50 AM | Train: [79/100] Step 000/390 Loss 0.020 Prec@(1,5) (99.2%, 100.0%)
01/19 04:14:51 AM | Train: [77/100] Step 390/390 Loss 0.170 Prec@(1,5) (94.1%, 100.0%)
01/19 04:14:51 AM | Train: [77/100] Final Prec@1 94.0560%
01/19 04:14:52 AM | Valid: [77/100] Step 000/078 Loss 0.250 Prec@(1,5) (91.4%, 100.0%)
01/19 04:14:53 AM | Valid: [77/100] Step 078/078 Loss 0.389 Prec@(1,5) (87.8%, 99.6%)
01/19 04:14:53 AM | Valid: [77/100] Final Prec@1 87.7600%
01/19 04:14:54 AM | Current best Prec@1 = 88.3400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:14:54 AM | Train: [77/100] Step 390/390 Loss 0.031 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:54 AM | Train: [77/100] Final Prec@1 99.0900%
01/19 04:14:54 AM | Train: [78/100] Step 000/390 Loss 0.265 Prec@(1,5) (92.2%, 99.2%)
01/19 04:14:55 AM | Valid: [77/100] Step 000/078 Loss 0.209 Prec@(1,5) (93.0%, 100.0%)
01/19 04:14:55 AM | Train: [78/100] Step 200/390 Loss 0.171 Prec@(1,5) (94.1%, 99.9%)
01/19 04:14:56 AM | Train: [79/100] Step 100/390 Loss 0.029 Prec@(1,5) (99.1%, 100.0%)
01/19 04:14:56 AM | Valid: [77/100] Step 078/078 Loss 0.292 Prec@(1,5) (91.9%, 99.7%)
01/19 04:14:56 AM | Valid: [77/100] Final Prec@1 91.9000%
01/19 04:14:56 AM | Current best Prec@1 = 91.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:14:57 AM | Train: [78/100] Step 000/390 Loss 0.036 Prec@(1,5) (98.4%, 100.0%)
01/19 04:15:00 AM | Train: [78/100] Step 100/390 Loss 0.169 Prec@(1,5) (94.1%, 99.9%)
01/19 04:15:01 AM | Train: [78/100] Step 300/390 Loss 0.171 Prec@(1,5) (94.1%, 99.9%)
01/19 04:15:01 AM | Train: [79/100] Step 200/390 Loss 0.029 Prec@(1,5) (99.1%, 100.0%)
01/19 04:15:03 AM | Train: [78/100] Step 100/390 Loss 0.028 Prec@(1,5) (99.2%, 100.0%)
01/19 04:15:06 AM | Train: [78/100] Step 200/390 Loss 0.169 Prec@(1,5) (94.2%, 99.9%)
01/19 04:15:07 AM | Train: [78/100] Step 390/390 Loss 0.169 Prec@(1,5) (94.2%, 99.9%)
01/19 04:15:07 AM | Train: [78/100] Final Prec@1 94.2160%
01/19 04:15:07 AM | Valid: [78/100] Step 000/078 Loss 0.276 Prec@(1,5) (92.2%, 100.0%)
01/19 04:15:08 AM | Train: [79/100] Step 300/390 Loss 0.029 Prec@(1,5) (99.1%, 100.0%)
01/19 04:15:09 AM | Valid: [78/100] Step 078/078 Loss 0.363 Prec@(1,5) (88.7%, 99.5%)
01/19 04:15:09 AM | Train: [78/100] Step 200/390 Loss 0.028 Prec@(1,5) (99.2%, 100.0%)
01/19 04:15:09 AM | Valid: [78/100] Final Prec@1 88.7400%
01/19 04:15:09 AM | Current best Prec@1 = 88.7400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:15:09 AM | Train: [79/100] Step 000/390 Loss 0.172 Prec@(1,5) (92.2%, 100.0%)
01/19 04:15:12 AM | Train: [78/100] Step 300/390 Loss 0.170 Prec@(1,5) (94.2%, 99.9%)
01/19 04:15:13 AM | Train: [79/100] Step 390/390 Loss 0.029 Prec@(1,5) (99.1%, 100.0%)
01/19 04:15:13 AM | Train: [79/100] Final Prec@1 99.1140%
01/19 04:15:14 AM | Valid: [79/100] Step 000/078 Loss 0.133 Prec@(1,5) (93.8%, 100.0%)
01/19 04:15:15 AM | Train: [78/100] Step 300/390 Loss 0.029 Prec@(1,5) (99.2%, 100.0%)
01/19 04:15:15 AM | Valid: [79/100] Step 078/078 Loss 0.301 Prec@(1,5) (91.8%, 99.7%)
01/19 04:15:15 AM | Valid: [79/100] Final Prec@1 91.8300%
01/19 04:15:16 AM | Current best Prec@1 = 91.9100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:15:16 AM | Train: [79/100] Step 100/390 Loss 0.162 Prec@(1,5) (94.4%, 100.0%)
01/19 04:15:16 AM | Train: [80/100] Step 000/390 Loss 0.031 Prec@(1,5) (98.4%, 100.0%)
01/19 04:15:17 AM | Train: [78/100] Step 390/390 Loss 0.166 Prec@(1,5) (94.3%, 99.9%)
01/19 04:15:17 AM | Train: [78/100] Final Prec@1 94.3200%
01/19 04:15:18 AM | Valid: [78/100] Step 000/078 Loss 0.278 Prec@(1,5) (92.2%, 100.0%)
01/19 04:15:19 AM | Valid: [78/100] Step 078/078 Loss 0.374 Prec@(1,5) (88.4%, 99.6%)
01/19 04:15:20 AM | Valid: [78/100] Final Prec@1 88.3900%
01/19 04:15:20 AM | Current best Prec@1 = 88.3900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:15:20 AM | Train: [78/100] Step 390/390 Loss 0.027 Prec@(1,5) (99.2%, 100.0%)
01/19 04:15:20 AM | Train: [78/100] Final Prec@1 99.2100%
01/19 04:15:20 AM | Train: [79/100] Step 000/390 Loss 0.134 Prec@(1,5) (96.9%, 100.0%)
01/19 04:15:21 AM | Valid: [78/100] Step 000/078 Loss 0.152 Prec@(1,5) (96.1%, 100.0%)
01/19 04:15:21 AM | Train: [79/100] Step 200/390 Loss 0.165 Prec@(1,5) (94.3%, 100.0%)
01/19 04:15:22 AM | Train: [80/100] Step 100/390 Loss 0.024 Prec@(1,5) (99.4%, 100.0%)
01/19 04:15:22 AM | Valid: [78/100] Step 078/078 Loss 0.290 Prec@(1,5) (92.1%, 99.6%)
01/19 04:15:22 AM | Valid: [78/100] Final Prec@1 92.1100%
01/19 04:15:22 AM | Current best Prec@1 = 92.1100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:15:23 AM | Train: [79/100] Step 000/390 Loss 0.018 Prec@(1,5) (99.2%, 100.0%)
01/19 04:15:26 AM | Train: [79/100] Step 100/390 Loss 0.156 Prec@(1,5) (94.8%, 99.9%)
01/19 04:15:27 AM | Train: [79/100] Step 300/390 Loss 0.165 Prec@(1,5) (94.4%, 100.0%)
01/19 04:15:28 AM | Train: [80/100] Step 200/390 Loss 0.025 Prec@(1,5) (99.4%, 100.0%)
01/19 04:15:29 AM | Train: [79/100] Step 100/390 Loss 0.023 Prec@(1,5) (99.4%, 100.0%)
01/19 04:15:32 AM | Train: [79/100] Step 390/390 Loss 0.166 Prec@(1,5) (94.4%, 99.9%)
01/19 04:15:32 AM | Train: [79/100] Final Prec@1 94.3680%
01/19 04:15:32 AM | Train: [79/100] Step 200/390 Loss 0.160 Prec@(1,5) (94.5%, 100.0%)
01/19 04:15:33 AM | Valid: [79/100] Step 000/078 Loss 0.262 Prec@(1,5) (91.4%, 100.0%)
01/19 04:15:34 AM | Train: [80/100] Step 300/390 Loss 0.025 Prec@(1,5) (99.3%, 100.0%)
01/19 04:15:34 AM | Valid: [79/100] Step 078/078 Loss 0.372 Prec@(1,5) (88.2%, 99.6%)
01/19 04:15:35 AM | Valid: [79/100] Final Prec@1 88.2000%
01/19 04:15:35 AM | Current best Prec@1 = 88.7400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:15:35 AM | Train: [80/100] Step 000/390 Loss 0.125 Prec@(1,5) (95.3%, 100.0%)
01/19 04:15:35 AM | Train: [79/100] Step 200/390 Loss 0.024 Prec@(1,5) (99.4%, 100.0%)
01/19 04:15:38 AM | Train: [79/100] Step 300/390 Loss 0.158 Prec@(1,5) (94.6%, 100.0%)
01/19 04:15:39 AM | Train: [80/100] Step 390/390 Loss 0.025 Prec@(1,5) (99.3%, 100.0%)
01/19 04:15:39 AM | Train: [80/100] Final Prec@1 99.3460%
01/19 04:15:40 AM | Valid: [80/100] Step 000/078 Loss 0.119 Prec@(1,5) (95.3%, 100.0%)
01/19 04:15:41 AM | Train: [79/100] Step 300/390 Loss 0.025 Prec@(1,5) (99.3%, 100.0%)
01/19 04:15:41 AM | Train: [80/100] Step 100/390 Loss 0.157 Prec@(1,5) (94.7%, 99.9%)
01/19 04:15:41 AM | Valid: [80/100] Step 078/078 Loss 0.294 Prec@(1,5) (92.0%, 99.6%)
01/19 04:15:41 AM | Valid: [80/100] Final Prec@1 91.9800%
01/19 04:15:42 AM | Current best Prec@1 = 91.9800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:15:42 AM | Train: [81/100] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
01/19 04:15:44 AM | Train: [79/100] Step 390/390 Loss 0.160 Prec@(1,5) (94.4%, 100.0%)
01/19 04:15:44 AM | Train: [79/100] Final Prec@1 94.4140%
01/19 04:15:44 AM | Valid: [79/100] Step 000/078 Loss 0.257 Prec@(1,5) (91.4%, 100.0%)
01/19 04:15:45 AM | Valid: [79/100] Step 078/078 Loss 0.385 Prec@(1,5) (87.9%, 99.6%)
01/19 04:15:46 AM | Valid: [79/100] Final Prec@1 87.8800%
01/19 04:15:46 AM | Current best Prec@1 = 88.3900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:15:46 AM | Train: [79/100] Step 390/390 Loss 0.025 Prec@(1,5) (99.3%, 100.0%)
01/19 04:15:46 AM | Train: [80/100] Step 000/390 Loss 0.112 Prec@(1,5) (96.9%, 100.0%)
01/19 04:15:46 AM | Train: [79/100] Final Prec@1 99.2900%
01/19 04:15:47 AM | Train: [80/100] Step 200/390 Loss 0.155 Prec@(1,5) (94.7%, 99.9%)
01/19 04:15:47 AM | Valid: [79/100] Step 000/078 Loss 0.146 Prec@(1,5) (95.3%, 99.2%)
01/19 04:15:48 AM | Train: [81/100] Step 100/390 Loss 0.024 Prec@(1,5) (99.3%, 100.0%)
01/19 04:15:49 AM | Valid: [79/100] Step 078/078 Loss 0.292 Prec@(1,5) (92.2%, 99.6%)
01/19 04:15:49 AM | Valid: [79/100] Final Prec@1 92.1700%
01/19 04:15:49 AM | Current best Prec@1 = 92.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:15:50 AM | Train: [80/100] Step 000/390 Loss 0.015 Prec@(1,5) (100.0%, 100.0%)
01/19 04:15:53 AM | Train: [80/100] Step 100/390 Loss 0.160 Prec@(1,5) (94.6%, 100.0%)
01/19 04:15:53 AM | Train: [80/100] Step 300/390 Loss 0.155 Prec@(1,5) (94.7%, 99.9%)
01/19 04:15:54 AM | Train: [81/100] Step 200/390 Loss 0.024 Prec@(1,5) (99.4%, 100.0%)
01/19 04:15:56 AM | Train: [80/100] Step 100/390 Loss 0.023 Prec@(1,5) (99.4%, 100.0%)
01/19 04:15:58 AM | Train: [80/100] Step 390/390 Loss 0.157 Prec@(1,5) (94.6%, 99.9%)
01/19 04:15:58 AM | Train: [80/100] Final Prec@1 94.6380%
01/19 04:15:59 AM | Train: [80/100] Step 200/390 Loss 0.158 Prec@(1,5) (94.7%, 99.9%)
01/19 04:15:59 AM | Valid: [80/100] Step 000/078 Loss 0.279 Prec@(1,5) (89.1%, 100.0%)
01/19 04:16:00 AM | Train: [81/100] Step 300/390 Loss 0.024 Prec@(1,5) (99.3%, 100.0%)
01/19 04:16:00 AM | Valid: [80/100] Step 078/078 Loss 0.375 Prec@(1,5) (88.3%, 99.6%)
01/19 04:16:00 AM | Valid: [80/100] Final Prec@1 88.3000%
01/19 04:16:00 AM | Current best Prec@1 = 88.7400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:16:01 AM | Train: [81/100] Step 000/390 Loss 0.085 Prec@(1,5) (98.4%, 100.0%)
01/19 04:16:02 AM | Train: [80/100] Step 200/390 Loss 0.023 Prec@(1,5) (99.4%, 100.0%)
01/19 04:16:05 AM | Train: [80/100] Step 300/390 Loss 0.158 Prec@(1,5) (94.7%, 99.9%)
01/19 04:16:05 AM | Train: [81/100] Step 390/390 Loss 0.025 Prec@(1,5) (99.3%, 100.0%)
01/19 04:16:06 AM | Train: [81/100] Final Prec@1 99.3160%
01/19 04:16:06 AM | Valid: [81/100] Step 000/078 Loss 0.121 Prec@(1,5) (96.1%, 100.0%)
01/19 04:16:07 AM | Train: [81/100] Step 100/390 Loss 0.149 Prec@(1,5) (94.9%, 99.9%)
01/19 04:16:08 AM | Train: [80/100] Step 300/390 Loss 0.024 Prec@(1,5) (99.3%, 100.0%)
01/19 04:16:08 AM | Valid: [81/100] Step 078/078 Loss 0.306 Prec@(1,5) (91.9%, 99.6%)
01/19 04:16:08 AM | Valid: [81/100] Final Prec@1 91.9300%
01/19 04:16:08 AM | Current best Prec@1 = 91.9800%
01/19 04:16:08 AM | Perform validation on training dataset. 
01/19 04:16:09 AM | Valid on training dataset: [81/100] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
01/19 04:16:10 AM | Train: [80/100] Step 390/390 Loss 0.158 Prec@(1,5) (94.6%, 99.9%)
01/19 04:16:10 AM | Train: [80/100] Final Prec@1 94.6180%
01/19 04:16:11 AM | Valid: [80/100] Step 000/078 Loss 0.284 Prec@(1,5) (92.2%, 100.0%)
01/19 04:16:11 AM | Valid on training dataset: [81/100] Step 100/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:16:12 AM | Valid: [80/100] Step 078/078 Loss 0.380 Prec@(1,5) (88.4%, 99.6%)
01/19 04:16:12 AM | Valid: [80/100] Final Prec@1 88.4500%
01/19 04:16:13 AM | Train: [80/100] Step 390/390 Loss 0.025 Prec@(1,5) (99.3%, 100.0%)
01/19 04:16:13 AM | Current best Prec@1 = 88.4500%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:16:13 AM | Train: [80/100] Final Prec@1 99.2920%
01/19 04:16:13 AM | Train: [81/100] Step 200/390 Loss 0.151 Prec@(1,5) (94.9%, 100.0%)
01/19 04:16:13 AM | Train: [81/100] Step 000/390 Loss 0.085 Prec@(1,5) (98.4%, 100.0%)
01/19 04:16:13 AM | Valid on training dataset: [81/100] Step 200/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:16:13 AM | Valid: [80/100] Step 000/078 Loss 0.176 Prec@(1,5) (95.3%, 100.0%)
01/19 04:16:15 AM | Valid: [80/100] Step 078/078 Loss 0.301 Prec@(1,5) (91.9%, 99.6%)
01/19 04:16:15 AM | Valid: [80/100] Final Prec@1 91.9300%
01/19 04:16:15 AM | Current best Prec@1 = 92.1700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:16:16 AM | Valid on training dataset: [81/100] Step 300/390 Loss 0.016 Prec@(1,5) (99.6%, 100.0%)
01/19 04:16:16 AM | Train: [81/100] Step 000/390 Loss 0.045 Prec@(1,5) (98.4%, 100.0%)
01/19 04:16:17 AM | Valid on training dataset: [81/100] Step 390/390 Loss 0.016 Prec@(1,5) (99.6%, 100.0%)
01/19 04:16:18 AM | Valid on training dataset: [81/100] Final Prec@1 99.6020%
01/19 04:16:18 AM | Final train Prec@1 = 99.6020%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:16:18 AM | Train: [82/100] Step 000/390 Loss 0.040 Prec@(1,5) (99.2%, 100.0%)
01/19 04:16:19 AM | Train: [81/100] Step 300/390 Loss 0.153 Prec@(1,5) (94.8%, 100.0%)
01/19 04:16:19 AM | Train: [81/100] Step 100/390 Loss 0.146 Prec@(1,5) (95.2%, 99.9%)
01/19 04:16:22 AM | Train: [81/100] Step 100/390 Loss 0.023 Prec@(1,5) (99.4%, 100.0%)
01/19 04:16:24 AM | Train: [82/100] Step 100/390 Loss 0.022 Prec@(1,5) (99.5%, 100.0%)
01/19 04:16:24 AM | Train: [81/100] Step 390/390 Loss 0.153 Prec@(1,5) (94.8%, 100.0%)
01/19 04:16:24 AM | Train: [81/100] Final Prec@1 94.7660%
01/19 04:16:25 AM | Valid: [81/100] Step 000/078 Loss 0.233 Prec@(1,5) (91.4%, 100.0%)
01/19 04:16:25 AM | Train: [81/100] Step 200/390 Loss 0.148 Prec@(1,5) (95.0%, 99.9%)
01/19 04:16:27 AM | Valid: [81/100] Step 078/078 Loss 0.364 Prec@(1,5) (88.6%, 99.6%)
01/19 04:16:27 AM | Valid: [81/100] Final Prec@1 88.5800%
01/19 04:16:27 AM | Current best Prec@1 = 88.7400%
01/19 04:16:27 AM | Perform validation on training dataset. 
01/19 04:16:27 AM | Valid on training dataset: [81/100] Step 000/390 Loss 0.141 Prec@(1,5) (96.1%, 100.0%)
01/19 04:16:28 AM | Train: [81/100] Step 200/390 Loss 0.022 Prec@(1,5) (99.4%, 100.0%)
01/19 04:16:30 AM | Valid on training dataset: [81/100] Step 100/390 Loss 0.129 Prec@(1,5) (95.8%, 100.0%)
01/19 04:16:30 AM | Train: [82/100] Step 200/390 Loss 0.021 Prec@(1,5) (99.5%, 100.0%)
01/19 04:16:31 AM | Train: [81/100] Step 300/390 Loss 0.150 Prec@(1,5) (94.8%, 99.9%)
01/19 04:16:32 AM | Valid on training dataset: [81/100] Step 200/390 Loss 0.127 Prec@(1,5) (95.8%, 100.0%)
01/19 04:16:34 AM | Train: [81/100] Step 300/390 Loss 0.022 Prec@(1,5) (99.4%, 100.0%)
01/19 04:16:34 AM | Valid on training dataset: [81/100] Step 300/390 Loss 0.124 Prec@(1,5) (95.9%, 100.0%)
01/19 04:16:36 AM | Valid on training dataset: [81/100] Step 390/390 Loss 0.124 Prec@(1,5) (95.9%, 100.0%)
01/19 04:16:36 AM | Valid on training dataset: [81/100] Final Prec@1 95.9340%
01/19 04:16:36 AM | Final train Prec@1 = 95.9340%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:16:36 AM | Train: [81/100] Step 390/390 Loss 0.150 Prec@(1,5) (94.8%, 99.9%)
01/19 04:16:37 AM | Train: [81/100] Final Prec@1 94.7760%
01/19 04:16:37 AM | Train: [82/100] Step 300/390 Loss 0.021 Prec@(1,5) (99.5%, 100.0%)
01/19 04:16:37 AM | Valid: [81/100] Step 000/078 Loss 0.287 Prec@(1,5) (91.4%, 99.2%)
01/19 04:16:37 AM | Train: [82/100] Step 000/390 Loss 0.210 Prec@(1,5) (93.8%, 100.0%)
01/19 04:16:39 AM | Valid: [81/100] Step 078/078 Loss 0.373 Prec@(1,5) (88.4%, 99.6%)
01/19 04:16:39 AM | Valid: [81/100] Final Prec@1 88.3700%
01/19 04:16:39 AM | Current best Prec@1 = 88.4500%
01/19 04:16:39 AM | Perform validation on training dataset. 
01/19 04:16:40 AM | Train: [81/100] Step 390/390 Loss 0.022 Prec@(1,5) (99.4%, 100.0%)
01/19 04:16:40 AM | Valid on training dataset: [81/100] Step 000/390 Loss 0.200 Prec@(1,5) (93.8%, 99.2%)
01/19 04:16:40 AM | Train: [81/100] Final Prec@1 99.4220%
01/19 04:16:40 AM | Valid: [81/100] Step 000/078 Loss 0.150 Prec@(1,5) (96.1%, 100.0%)
01/19 04:16:42 AM | Valid on training dataset: [81/100] Step 100/390 Loss 0.133 Prec@(1,5) (95.7%, 99.9%)
01/19 04:16:42 AM | Valid: [81/100] Step 078/078 Loss 0.294 Prec@(1,5) (92.1%, 99.6%)
01/19 04:16:42 AM | Train: [82/100] Step 390/390 Loss 0.022 Prec@(1,5) (99.5%, 100.0%)
01/19 04:16:42 AM | Valid: [81/100] Final Prec@1 92.1000%
01/19 04:16:42 AM | Train: [82/100] Final Prec@1 99.4680%
01/19 04:16:42 AM | Current best Prec@1 = 92.1700%
01/19 04:16:42 AM | Perform validation on training dataset. 
01/19 04:16:42 AM | Train: [82/100] Step 100/390 Loss 0.145 Prec@(1,5) (95.2%, 100.0%)
01/19 04:16:43 AM | Valid: [82/100] Step 000/078 Loss 0.132 Prec@(1,5) (95.3%, 100.0%)
01/19 04:16:43 AM | Valid on training dataset: [81/100] Step 000/390 Loss 0.012 Prec@(1,5) (99.2%, 100.0%)
01/19 04:16:44 AM | Valid on training dataset: [81/100] Step 200/390 Loss 0.128 Prec@(1,5) (95.8%, 100.0%)
01/19 04:16:44 AM | Valid: [82/100] Step 078/078 Loss 0.300 Prec@(1,5) (92.0%, 99.6%)
01/19 04:16:44 AM | Valid: [82/100] Final Prec@1 91.9800%
01/19 04:16:45 AM | Current best Prec@1 = 91.9800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:16:45 AM | Valid on training dataset: [81/100] Step 100/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:16:45 AM | Train: [83/100] Step 000/390 Loss 0.025 Prec@(1,5) (99.2%, 100.0%)
01/19 04:16:47 AM | Valid on training dataset: [81/100] Step 300/390 Loss 0.125 Prec@(1,5) (95.9%, 100.0%)
01/19 04:16:47 AM | Valid on training dataset: [81/100] Step 200/390 Loss 0.016 Prec@(1,5) (99.6%, 100.0%)
01/19 04:16:48 AM | Train: [82/100] Step 200/390 Loss 0.144 Prec@(1,5) (95.2%, 100.0%)
01/19 04:16:49 AM | Valid on training dataset: [81/100] Step 390/390 Loss 0.125 Prec@(1,5) (95.8%, 100.0%)
01/19 04:16:49 AM | Valid on training dataset: [81/100] Final Prec@1 95.8460%
01/19 04:16:49 AM | Final train Prec@1 = 95.8460%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:16:49 AM | Train: [82/100] Step 000/390 Loss 0.212 Prec@(1,5) (92.2%, 100.0%)
01/19 04:16:50 AM | Valid on training dataset: [81/100] Step 300/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:16:51 AM | Train: [83/100] Step 100/390 Loss 0.020 Prec@(1,5) (99.5%, 100.0%)
01/19 04:16:52 AM | Valid on training dataset: [81/100] Step 390/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:16:52 AM | Valid on training dataset: [81/100] Final Prec@1 99.6240%
01/19 04:16:52 AM | Final train Prec@1 = 99.6240%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:16:52 AM | Train: [82/100] Step 000/390 Loss 0.028 Prec@(1,5) (99.2%, 100.0%)
01/19 04:16:54 AM | Train: [82/100] Step 300/390 Loss 0.144 Prec@(1,5) (95.2%, 100.0%)
01/19 04:16:56 AM | Train: [82/100] Step 100/390 Loss 0.149 Prec@(1,5) (94.9%, 100.0%)
01/19 04:16:56 AM | Train: [83/100] Step 200/390 Loss 0.021 Prec@(1,5) (99.5%, 100.0%)
01/19 04:16:59 AM | Train: [82/100] Step 100/390 Loss 0.022 Prec@(1,5) (99.4%, 100.0%)
01/19 04:16:59 AM | Train: [82/100] Step 390/390 Loss 0.145 Prec@(1,5) (95.1%, 100.0%)
01/19 04:17:00 AM | Train: [82/100] Final Prec@1 95.1400%
01/19 04:17:00 AM | Valid: [82/100] Step 000/078 Loss 0.277 Prec@(1,5) (92.2%, 100.0%)
01/19 04:17:02 AM | Valid: [82/100] Step 078/078 Loss 0.369 Prec@(1,5) (88.7%, 99.5%)
01/19 04:17:02 AM | Valid: [82/100] Final Prec@1 88.6800%
01/19 04:17:02 AM | Train: [82/100] Step 200/390 Loss 0.147 Prec@(1,5) (94.9%, 100.0%)
01/19 04:17:02 AM | Current best Prec@1 = 88.7400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:17:02 AM | Train: [83/100] Step 300/390 Loss 0.021 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:03 AM | Train: [83/100] Step 000/390 Loss 0.113 Prec@(1,5) (96.1%, 100.0%)
01/19 04:17:05 AM | Train: [82/100] Step 200/390 Loss 0.022 Prec@(1,5) (99.4%, 100.0%)
01/19 04:17:08 AM | Train: [82/100] Step 300/390 Loss 0.144 Prec@(1,5) (95.0%, 100.0%)
01/19 04:17:08 AM | Train: [83/100] Step 390/390 Loss 0.021 Prec@(1,5) (99.4%, 100.0%)
01/19 04:17:08 AM | Train: [83/100] Final Prec@1 99.4420%
01/19 04:17:08 AM | Train: [83/100] Step 100/390 Loss 0.140 Prec@(1,5) (95.1%, 99.9%)
01/19 04:17:09 AM | Valid: [83/100] Step 000/078 Loss 0.093 Prec@(1,5) (96.1%, 100.0%)
01/19 04:17:10 AM | Valid: [83/100] Step 078/078 Loss 0.296 Prec@(1,5) (92.1%, 99.7%)
01/19 04:17:10 AM | Valid: [83/100] Final Prec@1 92.0900%
01/19 04:17:10 AM | Current best Prec@1 = 92.0900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:17:10 AM | Train: [82/100] Step 300/390 Loss 0.021 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:11 AM | Train: [84/100] Step 000/390 Loss 0.013 Prec@(1,5) (100.0%, 100.0%)
01/19 04:17:13 AM | Train: [82/100] Step 390/390 Loss 0.143 Prec@(1,5) (95.1%, 100.0%)
01/19 04:17:13 AM | Train: [82/100] Final Prec@1 95.1260%
01/19 04:17:14 AM | Valid: [82/100] Step 000/078 Loss 0.291 Prec@(1,5) (91.4%, 99.2%)
01/19 04:17:14 AM | Train: [83/100] Step 200/390 Loss 0.140 Prec@(1,5) (95.2%, 99.9%)
01/19 04:17:15 AM | Valid: [82/100] Step 078/078 Loss 0.377 Prec@(1,5) (88.5%, 99.5%)
01/19 04:17:16 AM | Valid: [82/100] Final Prec@1 88.4700%
01/19 04:17:16 AM | Train: [82/100] Step 390/390 Loss 0.021 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:16 AM | Current best Prec@1 = 88.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:17:16 AM | Train: [82/100] Final Prec@1 99.4700%
01/19 04:17:16 AM | Valid: [82/100] Step 000/078 Loss 0.190 Prec@(1,5) (95.3%, 100.0%)
01/19 04:17:16 AM | Train: [83/100] Step 000/390 Loss 0.093 Prec@(1,5) (96.9%, 100.0%)
01/19 04:17:17 AM | Train: [84/100] Step 100/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:17:18 AM | Valid: [82/100] Step 078/078 Loss 0.297 Prec@(1,5) (92.4%, 99.7%)
01/19 04:17:18 AM | Valid: [82/100] Final Prec@1 92.3800%
01/19 04:17:18 AM | Current best Prec@1 = 92.3800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:17:19 AM | Train: [83/100] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
01/19 04:17:20 AM | Train: [83/100] Step 300/390 Loss 0.140 Prec@(1,5) (95.2%, 99.9%)
01/19 04:17:22 AM | Train: [83/100] Step 100/390 Loss 0.143 Prec@(1,5) (95.3%, 99.9%)
01/19 04:17:23 AM | Train: [84/100] Step 200/390 Loss 0.018 Prec@(1,5) (99.6%, 100.0%)
01/19 04:17:25 AM | Train: [83/100] Step 100/390 Loss 0.019 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:26 AM | Train: [83/100] Step 390/390 Loss 0.142 Prec@(1,5) (95.1%, 99.9%)
01/19 04:17:26 AM | Train: [83/100] Final Prec@1 95.1360%
01/19 04:17:27 AM | Valid: [83/100] Step 000/078 Loss 0.238 Prec@(1,5) (92.2%, 100.0%)
01/19 04:17:28 AM | Valid: [83/100] Step 078/078 Loss 0.360 Prec@(1,5) (88.9%, 99.5%)
01/19 04:17:28 AM | Train: [83/100] Step 200/390 Loss 0.141 Prec@(1,5) (95.2%, 99.9%)
01/19 04:17:28 AM | Valid: [83/100] Final Prec@1 88.9000%
01/19 04:17:28 AM | Current best Prec@1 = 88.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:17:29 AM | Train: [84/100] Step 000/390 Loss 0.097 Prec@(1,5) (98.4%, 100.0%)
01/19 04:17:29 AM | Train: [84/100] Step 300/390 Loss 0.018 Prec@(1,5) (99.6%, 100.0%)
01/19 04:17:31 AM | Train: [83/100] Step 200/390 Loss 0.019 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:34 AM | Train: [83/100] Step 300/390 Loss 0.140 Prec@(1,5) (95.3%, 100.0%)
01/19 04:17:34 AM | Train: [84/100] Step 390/390 Loss 0.018 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:35 AM | Train: [84/100] Final Prec@1 99.5480%
01/19 04:17:35 AM | Train: [84/100] Step 100/390 Loss 0.122 Prec@(1,5) (96.0%, 100.0%)
01/19 04:17:35 AM | Valid: [84/100] Step 000/078 Loss 0.118 Prec@(1,5) (95.3%, 100.0%)
01/19 04:17:37 AM | Valid: [84/100] Step 078/078 Loss 0.291 Prec@(1,5) (92.2%, 99.6%)
01/19 04:17:37 AM | Valid: [84/100] Final Prec@1 92.1600%
01/19 04:17:37 AM | Train: [83/100] Step 300/390 Loss 0.019 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:37 AM | Current best Prec@1 = 92.1600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:17:38 AM | Train: [85/100] Step 000/390 Loss 0.011 Prec@(1,5) (99.2%, 100.0%)
01/19 04:17:39 AM | Train: [83/100] Step 390/390 Loss 0.139 Prec@(1,5) (95.3%, 100.0%)
01/19 04:17:40 AM | Train: [83/100] Final Prec@1 95.2800%
01/19 04:17:40 AM | Valid: [83/100] Step 000/078 Loss 0.255 Prec@(1,5) (93.0%, 100.0%)
01/19 04:17:41 AM | Train: [84/100] Step 200/390 Loss 0.129 Prec@(1,5) (95.8%, 100.0%)
01/19 04:17:42 AM | Valid: [83/100] Step 078/078 Loss 0.369 Prec@(1,5) (88.8%, 99.5%)
01/19 04:17:42 AM | Valid: [83/100] Final Prec@1 88.8100%
01/19 04:17:42 AM | Current best Prec@1 = 88.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:17:43 AM | Train: [84/100] Step 000/390 Loss 0.099 Prec@(1,5) (95.3%, 100.0%)
01/19 04:17:43 AM | Train: [83/100] Step 390/390 Loss 0.019 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:43 AM | Train: [83/100] Final Prec@1 99.4880%
01/19 04:17:43 AM | Valid: [83/100] Step 000/078 Loss 0.165 Prec@(1,5) (95.3%, 100.0%)
01/19 04:17:44 AM | Train: [85/100] Step 100/390 Loss 0.019 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:45 AM | Valid: [83/100] Step 078/078 Loss 0.288 Prec@(1,5) (92.4%, 99.6%)
01/19 04:17:45 AM | Valid: [83/100] Final Prec@1 92.4300%
01/19 04:17:45 AM | Current best Prec@1 = 92.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:17:46 AM | Train: [84/100] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
01/19 04:17:47 AM | Train: [84/100] Step 300/390 Loss 0.131 Prec@(1,5) (95.7%, 99.9%)
01/19 04:17:49 AM | Train: [84/100] Step 100/390 Loss 0.127 Prec@(1,5) (95.8%, 100.0%)
01/19 04:17:49 AM | Train: [85/100] Step 200/390 Loss 0.018 Prec@(1,5) (99.5%, 100.0%)
01/19 04:17:52 AM | Train: [84/100] Step 100/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:17:53 AM | Train: [84/100] Step 390/390 Loss 0.131 Prec@(1,5) (95.7%, 99.9%)
01/19 04:17:53 AM | Train: [84/100] Final Prec@1 95.7200%
01/19 04:17:53 AM | Valid: [84/100] Step 000/078 Loss 0.301 Prec@(1,5) (89.8%, 99.2%)
01/19 04:17:55 AM | Train: [84/100] Step 200/390 Loss 0.128 Prec@(1,5) (95.7%, 100.0%)
01/19 04:17:55 AM | Valid: [84/100] Step 078/078 Loss 0.359 Prec@(1,5) (89.2%, 99.5%)
01/19 04:17:55 AM | Valid: [84/100] Final Prec@1 89.1800%
01/19 04:17:55 AM | Current best Prec@1 = 89.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:17:55 AM | Train: [85/100] Step 300/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:17:56 AM | Train: [85/100] Step 000/390 Loss 0.087 Prec@(1,5) (96.9%, 100.0%)
01/19 04:17:58 AM | Train: [84/100] Step 200/390 Loss 0.018 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:00 AM | Train: [85/100] Step 390/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:00 AM | Train: [84/100] Step 300/390 Loss 0.130 Prec@(1,5) (95.6%, 100.0%)
01/19 04:18:00 AM | Train: [85/100] Final Prec@1 99.5800%
01/19 04:18:01 AM | Valid: [85/100] Step 000/078 Loss 0.103 Prec@(1,5) (96.9%, 100.0%)
01/19 04:18:02 AM | Train: [85/100] Step 100/390 Loss 0.129 Prec@(1,5) (95.9%, 99.9%)
01/19 04:18:03 AM | Valid: [85/100] Step 078/078 Loss 0.302 Prec@(1,5) (92.4%, 99.6%)
01/19 04:18:03 AM | Valid: [85/100] Final Prec@1 92.3700%
01/19 04:18:03 AM | Current best Prec@1 = 92.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:18:03 AM | Train: [86/100] Step 000/390 Loss 0.020 Prec@(1,5) (99.2%, 100.0%)
01/19 04:18:04 AM | Train: [84/100] Step 300/390 Loss 0.018 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:06 AM | Train: [84/100] Step 390/390 Loss 0.130 Prec@(1,5) (95.6%, 100.0%)
01/19 04:18:06 AM | Train: [84/100] Final Prec@1 95.6140%
01/19 04:18:07 AM | Valid: [84/100] Step 000/078 Loss 0.257 Prec@(1,5) (91.4%, 100.0%)
01/19 04:18:07 AM | Train: [85/100] Step 200/390 Loss 0.129 Prec@(1,5) (95.8%, 100.0%)
01/19 04:18:08 AM | Valid: [84/100] Step 078/078 Loss 0.369 Prec@(1,5) (88.7%, 99.5%)
01/19 04:18:08 AM | Valid: [84/100] Final Prec@1 88.7300%
01/19 04:18:09 AM | Current best Prec@1 = 88.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:18:09 AM | Train: [85/100] Step 000/390 Loss 0.093 Prec@(1,5) (96.1%, 100.0%)
01/19 04:18:09 AM | Train: [84/100] Step 390/390 Loss 0.018 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:10 AM | Train: [84/100] Final Prec@1 99.5720%
01/19 04:18:10 AM | Train: [86/100] Step 100/390 Loss 0.018 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:10 AM | Valid: [84/100] Step 000/078 Loss 0.182 Prec@(1,5) (95.3%, 100.0%)
01/19 04:18:11 AM | Valid: [84/100] Step 078/078 Loss 0.287 Prec@(1,5) (92.3%, 99.6%)
01/19 04:18:12 AM | Valid: [84/100] Final Prec@1 92.3400%
01/19 04:18:12 AM | Current best Prec@1 = 92.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:18:12 AM | Train: [85/100] Step 000/390 Loss 0.024 Prec@(1,5) (99.2%, 100.0%)
01/19 04:18:13 AM | Train: [85/100] Step 300/390 Loss 0.131 Prec@(1,5) (95.7%, 100.0%)
01/19 04:18:15 AM | Train: [85/100] Step 100/390 Loss 0.128 Prec@(1,5) (95.6%, 100.0%)
01/19 04:18:15 AM | Train: [86/100] Step 200/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:18 AM | Train: [85/100] Step 100/390 Loss 0.018 Prec@(1,5) (99.5%, 100.0%)
01/19 04:18:19 AM | Train: [85/100] Step 390/390 Loss 0.130 Prec@(1,5) (95.7%, 100.0%)
01/19 04:18:19 AM | Train: [85/100] Final Prec@1 95.7160%
01/19 04:18:20 AM | Valid: [85/100] Step 000/078 Loss 0.272 Prec@(1,5) (91.4%, 98.4%)
01/19 04:18:21 AM | Valid: [85/100] Step 078/078 Loss 0.352 Prec@(1,5) (89.1%, 99.6%)
01/19 04:18:21 AM | Valid: [85/100] Final Prec@1 89.1100%
01/19 04:18:21 AM | Train: [85/100] Step 200/390 Loss 0.124 Prec@(1,5) (95.8%, 100.0%)
01/19 04:18:21 AM | Current best Prec@1 = 89.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:18:21 AM | Train: [86/100] Step 300/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:22 AM | Train: [86/100] Step 000/390 Loss 0.106 Prec@(1,5) (97.7%, 100.0%)
01/19 04:18:24 AM | Train: [85/100] Step 200/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:27 AM | Train: [86/100] Step 390/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:27 AM | Train: [86/100] Final Prec@1 99.5740%
01/19 04:18:27 AM | Valid: [86/100] Step 000/078 Loss 0.112 Prec@(1,5) (95.3%, 100.0%)
01/19 04:18:27 AM | Train: [85/100] Step 300/390 Loss 0.126 Prec@(1,5) (95.7%, 100.0%)
01/19 04:18:28 AM | Train: [86/100] Step 100/390 Loss 0.121 Prec@(1,5) (96.0%, 100.0%)
01/19 04:18:29 AM | Valid: [86/100] Step 078/078 Loss 0.290 Prec@(1,5) (92.3%, 99.6%)
01/19 04:18:29 AM | Valid: [86/100] Final Prec@1 92.3500%
01/19 04:18:29 AM | Current best Prec@1 = 92.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:18:30 AM | Train: [87/100] Step 000/390 Loss 0.014 Prec@(1,5) (100.0%, 100.0%)
01/19 04:18:30 AM | Train: [85/100] Step 300/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:33 AM | Train: [85/100] Step 390/390 Loss 0.127 Prec@(1,5) (95.6%, 100.0%)
01/19 04:18:33 AM | Train: [85/100] Final Prec@1 95.6480%
01/19 04:18:33 AM | Valid: [85/100] Step 000/078 Loss 0.290 Prec@(1,5) (90.6%, 100.0%)
01/19 04:18:34 AM | Train: [86/100] Step 200/390 Loss 0.120 Prec@(1,5) (96.0%, 100.0%)
01/19 04:18:35 AM | Valid: [85/100] Step 078/078 Loss 0.378 Prec@(1,5) (88.7%, 99.5%)
01/19 04:18:35 AM | Valid: [85/100] Final Prec@1 88.7000%
01/19 04:18:35 AM | Current best Prec@1 = 88.8100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:18:36 AM | Train: [85/100] Step 390/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:36 AM | Train: [85/100] Final Prec@1 99.5780%
01/19 04:18:36 AM | Train: [86/100] Step 000/390 Loss 0.116 Prec@(1,5) (97.7%, 100.0%)
01/19 04:18:36 AM | Train: [87/100] Step 100/390 Loss 0.017 Prec@(1,5) (99.7%, 100.0%)
01/19 04:18:36 AM | Valid: [85/100] Step 000/078 Loss 0.175 Prec@(1,5) (94.5%, 100.0%)
01/19 04:18:38 AM | Valid: [85/100] Step 078/078 Loss 0.285 Prec@(1,5) (92.4%, 99.7%)
01/19 04:18:38 AM | Valid: [85/100] Final Prec@1 92.3700%
01/19 04:18:38 AM | Current best Prec@1 = 92.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:18:39 AM | Train: [86/100] Step 000/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
01/19 04:18:39 AM | Train: [86/100] Step 300/390 Loss 0.120 Prec@(1,5) (96.0%, 100.0%)
01/19 04:18:42 AM | Train: [87/100] Step 200/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:42 AM | Train: [86/100] Step 100/390 Loss 0.122 Prec@(1,5) (96.1%, 100.0%)
01/19 04:18:44 AM | Train: [86/100] Step 390/390 Loss 0.119 Prec@(1,5) (96.1%, 100.0%)
01/19 04:18:45 AM | Train: [86/100] Final Prec@1 96.0620%
01/19 04:18:45 AM | Train: [86/100] Step 100/390 Loss 0.014 Prec@(1,5) (99.8%, 100.0%)
01/19 04:18:45 AM | Valid: [86/100] Step 000/078 Loss 0.255 Prec@(1,5) (91.4%, 99.2%)
01/19 04:18:47 AM | Valid: [86/100] Step 078/078 Loss 0.348 Prec@(1,5) (89.2%, 99.6%)
01/19 04:18:47 AM | Valid: [86/100] Final Prec@1 89.1900%
01/19 04:18:47 AM | Current best Prec@1 = 89.1900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:18:47 AM | Train: [87/100] Step 000/390 Loss 0.083 Prec@(1,5) (98.4%, 100.0%)
01/19 04:18:48 AM | Train: [87/100] Step 300/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:48 AM | Train: [86/100] Step 200/390 Loss 0.121 Prec@(1,5) (96.1%, 100.0%)
01/19 04:18:51 AM | Train: [86/100] Step 200/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:18:53 AM | Train: [87/100] Step 390/390 Loss 0.017 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:53 AM | Train: [87/100] Step 100/390 Loss 0.119 Prec@(1,5) (96.1%, 99.9%)
01/19 04:18:53 AM | Train: [87/100] Final Prec@1 99.6100%
01/19 04:18:54 AM | Valid: [87/100] Step 000/078 Loss 0.099 Prec@(1,5) (95.3%, 100.0%)
01/19 04:18:54 AM | Train: [86/100] Step 300/390 Loss 0.120 Prec@(1,5) (96.1%, 100.0%)
01/19 04:18:56 AM | Valid: [87/100] Step 078/078 Loss 0.289 Prec@(1,5) (92.4%, 99.7%)
01/19 04:18:56 AM | Valid: [87/100] Final Prec@1 92.3600%
01/19 04:18:56 AM | Current best Prec@1 = 92.3700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:18:56 AM | Train: [88/100] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
01/19 04:18:56 AM | Train: [86/100] Step 300/390 Loss 0.016 Prec@(1,5) (99.6%, 100.0%)
01/19 04:18:59 AM | Train: [86/100] Step 390/390 Loss 0.122 Prec@(1,5) (96.1%, 100.0%)
01/19 04:18:59 AM | Train: [87/100] Step 200/390 Loss 0.117 Prec@(1,5) (96.3%, 99.9%)
01/19 04:18:59 AM | Train: [86/100] Final Prec@1 96.0700%
01/19 04:19:00 AM | Valid: [86/100] Step 000/078 Loss 0.269 Prec@(1,5) (92.2%, 100.0%)
01/19 04:19:01 AM | Valid: [86/100] Step 078/078 Loss 0.362 Prec@(1,5) (89.1%, 99.6%)
01/19 04:19:02 AM | Valid: [86/100] Final Prec@1 89.1200%
01/19 04:19:02 AM | Current best Prec@1 = 89.1200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:19:02 AM | Train: [86/100] Step 390/390 Loss 0.016 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:02 AM | Train: [86/100] Final Prec@1 99.6560%
01/19 04:19:02 AM | Train: [88/100] Step 100/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:02 AM | Train: [87/100] Step 000/390 Loss 0.118 Prec@(1,5) (97.7%, 100.0%)
01/19 04:19:03 AM | Valid: [86/100] Step 000/078 Loss 0.160 Prec@(1,5) (96.1%, 100.0%)
01/19 04:19:04 AM | Valid: [86/100] Step 078/078 Loss 0.284 Prec@(1,5) (92.3%, 99.6%)
01/19 04:19:04 AM | Valid: [86/100] Final Prec@1 92.3500%
01/19 04:19:04 AM | Current best Prec@1 = 92.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:19:04 AM | Train: [87/100] Step 300/390 Loss 0.116 Prec@(1,5) (96.3%, 100.0%)
01/19 04:19:05 AM | Train: [87/100] Step 000/390 Loss 0.022 Prec@(1,5) (99.2%, 100.0%)
01/19 04:19:08 AM | Train: [88/100] Step 200/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:08 AM | Train: [87/100] Step 100/390 Loss 0.120 Prec@(1,5) (96.1%, 100.0%)
01/19 04:19:10 AM | Train: [87/100] Step 390/390 Loss 0.116 Prec@(1,5) (96.3%, 100.0%)
01/19 04:19:10 AM | Train: [87/100] Final Prec@1 96.2700%
01/19 04:19:10 AM | Valid: [87/100] Step 000/078 Loss 0.278 Prec@(1,5) (92.2%, 100.0%)
01/19 04:19:11 AM | Train: [87/100] Step 100/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:12 AM | Valid: [87/100] Step 078/078 Loss 0.354 Prec@(1,5) (89.5%, 99.6%)
01/19 04:19:12 AM | Valid: [87/100] Final Prec@1 89.4700%
01/19 04:19:13 AM | Current best Prec@1 = 89.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:19:13 AM | Train: [88/100] Step 000/390 Loss 0.085 Prec@(1,5) (97.7%, 100.0%)
01/19 04:19:14 AM | Train: [87/100] Step 200/390 Loss 0.116 Prec@(1,5) (96.2%, 100.0%)
01/19 04:19:14 AM | Train: [88/100] Step 300/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:17 AM | Train: [87/100] Step 200/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:19 AM | Train: [88/100] Step 100/390 Loss 0.114 Prec@(1,5) (96.3%, 100.0%)
01/19 04:19:20 AM | Train: [88/100] Step 390/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:20 AM | Train: [88/100] Final Prec@1 99.6640%
01/19 04:19:20 AM | Train: [87/100] Step 300/390 Loss 0.115 Prec@(1,5) (96.3%, 100.0%)
01/19 04:19:21 AM | Valid: [88/100] Step 000/078 Loss 0.126 Prec@(1,5) (95.3%, 100.0%)
01/19 04:19:23 AM | Valid: [88/100] Step 078/078 Loss 0.284 Prec@(1,5) (92.4%, 99.6%)
01/19 04:19:23 AM | Train: [87/100] Step 300/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:23 AM | Valid: [88/100] Final Prec@1 92.4000%
01/19 04:19:23 AM | Current best Prec@1 = 92.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:19:23 AM | Train: [89/100] Step 000/390 Loss 0.018 Prec@(1,5) (100.0%, 100.0%)
01/19 04:19:25 AM | Train: [88/100] Step 200/390 Loss 0.116 Prec@(1,5) (96.1%, 100.0%)
01/19 04:19:26 AM | Train: [87/100] Step 390/390 Loss 0.116 Prec@(1,5) (96.3%, 100.0%)
01/19 04:19:26 AM | Train: [87/100] Final Prec@1 96.2560%
01/19 04:19:26 AM | Valid: [87/100] Step 000/078 Loss 0.282 Prec@(1,5) (90.6%, 100.0%)
01/19 04:19:28 AM | Valid: [87/100] Step 078/078 Loss 0.358 Prec@(1,5) (89.1%, 99.6%)
01/19 04:19:28 AM | Train: [87/100] Step 390/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:28 AM | Valid: [87/100] Final Prec@1 89.1000%
01/19 04:19:28 AM | Train: [87/100] Final Prec@1 99.6920%
01/19 04:19:28 AM | Current best Prec@1 = 89.1200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:19:29 AM | Valid: [87/100] Step 000/078 Loss 0.172 Prec@(1,5) (94.5%, 100.0%)
01/19 04:19:29 AM | Train: [88/100] Step 000/390 Loss 0.081 Prec@(1,5) (96.9%, 100.0%)
01/19 04:19:29 AM | Train: [89/100] Step 100/390 Loss 0.016 Prec@(1,5) (99.6%, 100.0%)
01/19 04:19:30 AM | Valid: [87/100] Step 078/078 Loss 0.285 Prec@(1,5) (92.5%, 99.6%)
01/19 04:19:30 AM | Valid: [87/100] Final Prec@1 92.5000%
01/19 04:19:30 AM | Current best Prec@1 = 92.5000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:19:30 AM | Train: [88/100] Step 300/390 Loss 0.115 Prec@(1,5) (96.1%, 100.0%)
01/19 04:19:31 AM | Train: [88/100] Step 000/390 Loss 0.008 Prec@(1,5) (100.0%, 100.0%)
01/19 04:19:35 AM | Train: [88/100] Step 100/390 Loss 0.108 Prec@(1,5) (96.6%, 100.0%)
01/19 04:19:35 AM | Train: [89/100] Step 200/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:19:36 AM | Train: [88/100] Step 390/390 Loss 0.115 Prec@(1,5) (96.1%, 100.0%)
01/19 04:19:36 AM | Train: [88/100] Final Prec@1 96.1060%
01/19 04:19:36 AM | Valid: [88/100] Step 000/078 Loss 0.282 Prec@(1,5) (89.8%, 99.2%)
01/19 04:19:37 AM | Train: [88/100] Step 100/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:19:38 AM | Valid: [88/100] Step 078/078 Loss 0.342 Prec@(1,5) (89.5%, 99.6%)
01/19 04:19:38 AM | Valid: [88/100] Final Prec@1 89.4600%
01/19 04:19:38 AM | Current best Prec@1 = 89.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:19:39 AM | Train: [89/100] Step 000/390 Loss 0.077 Prec@(1,5) (96.9%, 100.0%)
01/19 04:19:41 AM | Train: [88/100] Step 200/390 Loss 0.112 Prec@(1,5) (96.4%, 100.0%)
01/19 04:19:41 AM | Train: [89/100] Step 300/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:19:43 AM | Train: [88/100] Step 200/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:19:45 AM | Train: [89/100] Step 100/390 Loss 0.117 Prec@(1,5) (96.1%, 100.0%)
01/19 04:19:46 AM | Train: [88/100] Step 300/390 Loss 0.110 Prec@(1,5) (96.5%, 100.0%)
01/19 04:19:47 AM | Train: [89/100] Step 390/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:19:47 AM | Train: [89/100] Final Prec@1 99.6440%
01/19 04:19:48 AM | Valid: [89/100] Step 000/078 Loss 0.119 Prec@(1,5) (95.3%, 100.0%)
01/19 04:19:49 AM | Train: [88/100] Step 300/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:19:49 AM | Valid: [89/100] Step 078/078 Loss 0.282 Prec@(1,5) (92.3%, 99.7%)
01/19 04:19:49 AM | Valid: [89/100] Final Prec@1 92.3300%
01/19 04:19:50 AM | Current best Prec@1 = 92.4000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:19:50 AM | Train: [90/100] Step 000/390 Loss 0.034 Prec@(1,5) (97.7%, 100.0%)
01/19 04:19:51 AM | Train: [89/100] Step 200/390 Loss 0.111 Prec@(1,5) (96.3%, 100.0%)
01/19 04:19:52 AM | Train: [88/100] Step 390/390 Loss 0.111 Prec@(1,5) (96.4%, 100.0%)
01/19 04:19:52 AM | Train: [88/100] Final Prec@1 96.4260%
01/19 04:19:53 AM | Valid: [88/100] Step 000/078 Loss 0.266 Prec@(1,5) (91.4%, 100.0%)
01/19 04:19:54 AM | Valid: [88/100] Step 078/078 Loss 0.355 Prec@(1,5) (89.1%, 99.5%)
01/19 04:19:54 AM | Train: [88/100] Step 390/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:54 AM | Valid: [88/100] Final Prec@1 89.1100%
01/19 04:19:54 AM | Train: [88/100] Final Prec@1 99.6640%
01/19 04:19:54 AM | Current best Prec@1 = 89.1200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:19:55 AM | Train: [89/100] Step 000/390 Loss 0.087 Prec@(1,5) (97.7%, 100.0%)
01/19 04:19:55 AM | Valid: [88/100] Step 000/078 Loss 0.161 Prec@(1,5) (95.3%, 100.0%)
01/19 04:19:56 AM | Train: [90/100] Step 100/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:19:56 AM | Valid: [88/100] Step 078/078 Loss 0.281 Prec@(1,5) (92.6%, 99.6%)
01/19 04:19:57 AM | Valid: [88/100] Final Prec@1 92.5600%
01/19 04:19:57 AM | Current best Prec@1 = 92.5600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:19:57 AM | Train: [89/100] Step 300/390 Loss 0.111 Prec@(1,5) (96.3%, 100.0%)
01/19 04:19:57 AM | Train: [89/100] Step 000/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
01/19 04:20:01 AM | Train: [89/100] Step 100/390 Loss 0.108 Prec@(1,5) (96.3%, 100.0%)
01/19 04:20:02 AM | Train: [90/100] Step 200/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:03 AM | Train: [89/100] Step 390/390 Loss 0.111 Prec@(1,5) (96.4%, 100.0%)
01/19 04:20:03 AM | Train: [89/100] Final Prec@1 96.3800%
01/19 04:20:03 AM | Valid: [89/100] Step 000/078 Loss 0.266 Prec@(1,5) (93.8%, 100.0%)
01/19 04:20:03 AM | Train: [89/100] Step 100/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:05 AM | Valid: [89/100] Step 078/078 Loss 0.349 Prec@(1,5) (89.2%, 99.7%)
01/19 04:20:05 AM | Valid: [89/100] Final Prec@1 89.2200%
01/19 04:20:05 AM | Current best Prec@1 = 89.4700%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:20:06 AM | Train: [90/100] Step 000/390 Loss 0.229 Prec@(1,5) (93.0%, 99.2%)
01/19 04:20:07 AM | Train: [89/100] Step 200/390 Loss 0.104 Prec@(1,5) (96.6%, 100.0%)
01/19 04:20:08 AM | Train: [90/100] Step 300/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:09 AM | Train: [89/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:12 AM | Train: [90/100] Step 100/390 Loss 0.118 Prec@(1,5) (96.2%, 100.0%)
01/19 04:20:13 AM | Train: [89/100] Step 300/390 Loss 0.106 Prec@(1,5) (96.5%, 100.0%)
01/19 04:20:13 AM | Train: [90/100] Step 390/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:14 AM | Train: [90/100] Final Prec@1 99.6740%
01/19 04:20:14 AM | Valid: [90/100] Step 000/078 Loss 0.099 Prec@(1,5) (96.9%, 100.0%)
01/19 04:20:15 AM | Train: [89/100] Step 300/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:16 AM | Valid: [90/100] Step 078/078 Loss 0.280 Prec@(1,5) (92.5%, 99.7%)
01/19 04:20:16 AM | Valid: [90/100] Final Prec@1 92.5400%
01/19 04:20:16 AM | Current best Prec@1 = 92.5400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:20:17 AM | Train: [91/100] Step 000/390 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)
01/19 04:20:18 AM | Train: [90/100] Step 200/390 Loss 0.111 Prec@(1,5) (96.3%, 100.0%)
01/19 04:20:18 AM | Train: [89/100] Step 390/390 Loss 0.108 Prec@(1,5) (96.4%, 100.0%)
01/19 04:20:18 AM | Train: [89/100] Final Prec@1 96.4100%
01/19 04:20:19 AM | Valid: [89/100] Step 000/078 Loss 0.297 Prec@(1,5) (93.0%, 100.0%)
01/19 04:20:20 AM | Train: [89/100] Step 390/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:20 AM | Valid: [89/100] Step 078/078 Loss 0.366 Prec@(1,5) (89.2%, 99.5%)
01/19 04:20:20 AM | Train: [89/100] Final Prec@1 99.6680%
01/19 04:20:20 AM | Valid: [89/100] Final Prec@1 89.1800%
01/19 04:20:21 AM | Current best Prec@1 = 89.1800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:20:21 AM | Valid: [89/100] Step 000/078 Loss 0.159 Prec@(1,5) (96.1%, 100.0%)
01/19 04:20:21 AM | Train: [90/100] Step 000/390 Loss 0.185 Prec@(1,5) (93.8%, 100.0%)
01/19 04:20:22 AM | Train: [91/100] Step 100/390 Loss 0.016 Prec@(1,5) (99.6%, 100.0%)
01/19 04:20:23 AM | Valid: [89/100] Step 078/078 Loss 0.277 Prec@(1,5) (92.4%, 99.7%)
01/19 04:20:23 AM | Valid: [89/100] Final Prec@1 92.4300%
01/19 04:20:23 AM | Current best Prec@1 = 92.5600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:20:23 AM | Train: [90/100] Step 000/390 Loss 0.045 Prec@(1,5) (98.4%, 100.0%)
01/19 04:20:24 AM | Train: [90/100] Step 300/390 Loss 0.111 Prec@(1,5) (96.3%, 100.0%)
01/19 04:20:27 AM | Train: [90/100] Step 100/390 Loss 0.113 Prec@(1,5) (96.2%, 100.0%)
01/19 04:20:29 AM | Train: [91/100] Step 200/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:20:29 AM | Train: [90/100] Step 390/390 Loss 0.110 Prec@(1,5) (96.4%, 100.0%)
01/19 04:20:29 AM | Train: [90/100] Final Prec@1 96.3600%
01/19 04:20:30 AM | Valid: [90/100] Step 000/078 Loss 0.266 Prec@(1,5) (94.5%, 100.0%)
01/19 04:20:30 AM | Train: [90/100] Step 100/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:32 AM | Valid: [90/100] Step 078/078 Loss 0.350 Prec@(1,5) (89.7%, 99.6%)
01/19 04:20:32 AM | Valid: [90/100] Final Prec@1 89.7100%
01/19 04:20:32 AM | Current best Prec@1 = 89.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:20:32 AM | Train: [91/100] Step 000/390 Loss 0.135 Prec@(1,5) (96.1%, 99.2%)
01/19 04:20:33 AM | Train: [90/100] Step 200/390 Loss 0.105 Prec@(1,5) (96.6%, 100.0%)
01/19 04:20:35 AM | Train: [91/100] Step 300/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:36 AM | Train: [90/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:39 AM | Train: [91/100] Step 100/390 Loss 0.108 Prec@(1,5) (96.6%, 100.0%)
01/19 04:20:39 AM | Train: [90/100] Step 300/390 Loss 0.106 Prec@(1,5) (96.6%, 100.0%)
01/19 04:20:40 AM | Train: [91/100] Step 390/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:40 AM | Train: [91/100] Final Prec@1 99.6780%
01/19 04:20:41 AM | Valid: [91/100] Step 000/078 Loss 0.105 Prec@(1,5) (96.1%, 100.0%)
01/19 04:20:42 AM | Train: [90/100] Step 300/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:42 AM | Valid: [91/100] Step 078/078 Loss 0.281 Prec@(1,5) (92.6%, 99.7%)
01/19 04:20:43 AM | Valid: [91/100] Final Prec@1 92.6000%
01/19 04:20:43 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:20:43 AM | Train: [92/100] Step 000/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
01/19 04:20:45 AM | Train: [90/100] Step 390/390 Loss 0.106 Prec@(1,5) (96.6%, 100.0%)
01/19 04:20:45 AM | Train: [91/100] Step 200/390 Loss 0.105 Prec@(1,5) (96.6%, 100.0%)
01/19 04:20:45 AM | Train: [90/100] Final Prec@1 96.6300%
01/19 04:20:45 AM | Valid: [90/100] Step 000/078 Loss 0.278 Prec@(1,5) (91.4%, 100.0%)
01/19 04:20:47 AM | Valid: [90/100] Step 078/078 Loss 0.354 Prec@(1,5) (89.4%, 99.6%)
01/19 04:20:47 AM | Valid: [90/100] Final Prec@1 89.3800%
01/19 04:20:47 AM | Train: [90/100] Step 390/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:47 AM | Current best Prec@1 = 89.3800%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:20:47 AM | Train: [90/100] Final Prec@1 99.7160%
01/19 04:20:48 AM | Valid: [90/100] Step 000/078 Loss 0.159 Prec@(1,5) (96.1%, 100.0%)
01/19 04:20:48 AM | Train: [91/100] Step 000/390 Loss 0.155 Prec@(1,5) (91.4%, 100.0%)
01/19 04:20:49 AM | Train: [92/100] Step 100/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:49 AM | Valid: [90/100] Step 078/078 Loss 0.280 Prec@(1,5) (92.5%, 99.6%)
01/19 04:20:49 AM | Valid: [90/100] Final Prec@1 92.4500%
01/19 04:20:50 AM | Current best Prec@1 = 92.5600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:20:50 AM | Train: [91/100] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
01/19 04:20:50 AM | Train: [91/100] Step 300/390 Loss 0.104 Prec@(1,5) (96.7%, 100.0%)
01/19 04:20:54 AM | Train: [91/100] Step 100/390 Loss 0.103 Prec@(1,5) (96.7%, 100.0%)
01/19 04:20:55 AM | Train: [92/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:56 AM | Train: [91/100] Step 390/390 Loss 0.103 Prec@(1,5) (96.8%, 100.0%)
01/19 04:20:56 AM | Train: [91/100] Final Prec@1 96.7620%
01/19 04:20:56 AM | Valid: [91/100] Step 000/078 Loss 0.254 Prec@(1,5) (95.3%, 100.0%)
01/19 04:20:56 AM | Train: [91/100] Step 100/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:20:58 AM | Valid: [91/100] Step 078/078 Loss 0.346 Prec@(1,5) (89.6%, 99.6%)
01/19 04:20:58 AM | Valid: [91/100] Final Prec@1 89.5900%
01/19 04:20:58 AM | Current best Prec@1 = 89.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:20:59 AM | Train: [92/100] Step 000/390 Loss 0.051 Prec@(1,5) (99.2%, 100.0%)
01/19 04:20:59 AM | Train: [91/100] Step 200/390 Loss 0.101 Prec@(1,5) (96.7%, 100.0%)
01/19 04:21:01 AM | Train: [92/100] Step 300/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:03 AM | Train: [91/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:05 AM | Train: [92/100] Step 100/390 Loss 0.107 Prec@(1,5) (96.6%, 100.0%)
01/19 04:21:05 AM | Train: [91/100] Step 300/390 Loss 0.102 Prec@(1,5) (96.7%, 100.0%)
01/19 04:21:07 AM | Train: [92/100] Step 390/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:07 AM | Train: [92/100] Final Prec@1 99.7300%
01/19 04:21:08 AM | Valid: [92/100] Step 000/078 Loss 0.112 Prec@(1,5) (94.5%, 100.0%)
01/19 04:21:09 AM | Train: [91/100] Step 300/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:09 AM | Valid: [92/100] Step 078/078 Loss 0.283 Prec@(1,5) (92.6%, 99.7%)
01/19 04:21:10 AM | Valid: [92/100] Final Prec@1 92.5900%
01/19 04:21:10 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:21:10 AM | Train: [93/100] Step 000/390 Loss 0.013 Prec@(1,5) (100.0%, 100.0%)
01/19 04:21:11 AM | Train: [91/100] Step 390/390 Loss 0.101 Prec@(1,5) (96.7%, 100.0%)
01/19 04:21:11 AM | Train: [91/100] Final Prec@1 96.7420%
01/19 04:21:11 AM | Train: [92/100] Step 200/390 Loss 0.106 Prec@(1,5) (96.6%, 100.0%)
01/19 04:21:12 AM | Valid: [91/100] Step 000/078 Loss 0.292 Prec@(1,5) (90.6%, 100.0%)
01/19 04:21:13 AM | Valid: [91/100] Step 078/078 Loss 0.359 Prec@(1,5) (89.4%, 99.6%)
01/19 04:21:13 AM | Valid: [91/100] Final Prec@1 89.4300%
01/19 04:21:13 AM | Current best Prec@1 = 89.4300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:21:13 AM | Train: [91/100] Step 390/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:14 AM | Train: [91/100] Final Prec@1 99.7200%
01/19 04:21:14 AM | Train: [92/100] Step 000/390 Loss 0.077 Prec@(1,5) (97.7%, 100.0%)
01/19 04:21:14 AM | Valid: [91/100] Step 000/078 Loss 0.160 Prec@(1,5) (96.1%, 100.0%)
01/19 04:21:16 AM | Train: [93/100] Step 100/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:16 AM | Valid: [91/100] Step 078/078 Loss 0.280 Prec@(1,5) (92.5%, 99.7%)
01/19 04:21:16 AM | Valid: [91/100] Final Prec@1 92.4600%
01/19 04:21:16 AM | Current best Prec@1 = 92.5600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:21:17 AM | Train: [92/100] Step 000/390 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)
01/19 04:21:17 AM | Train: [92/100] Step 300/390 Loss 0.105 Prec@(1,5) (96.6%, 100.0%)
01/19 04:21:20 AM | Train: [92/100] Step 100/390 Loss 0.099 Prec@(1,5) (96.9%, 100.0%)
01/19 04:21:21 AM | Train: [93/100] Step 200/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:22 AM | Train: [92/100] Step 390/390 Loss 0.104 Prec@(1,5) (96.6%, 100.0%)
01/19 04:21:22 AM | Train: [92/100] Final Prec@1 96.6060%
01/19 04:21:23 AM | Train: [92/100] Step 100/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:23 AM | Valid: [92/100] Step 000/078 Loss 0.255 Prec@(1,5) (93.8%, 100.0%)
01/19 04:21:24 AM | Valid: [92/100] Step 078/078 Loss 0.339 Prec@(1,5) (89.9%, 99.6%)
01/19 04:21:25 AM | Valid: [92/100] Final Prec@1 89.9000%
01/19 04:21:25 AM | Current best Prec@1 = 89.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:21:25 AM | Train: [93/100] Step 000/390 Loss 0.102 Prec@(1,5) (96.9%, 100.0%)
01/19 04:21:26 AM | Train: [92/100] Step 200/390 Loss 0.100 Prec@(1,5) (96.8%, 100.0%)
01/19 04:21:27 AM | Train: [93/100] Step 300/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:29 AM | Train: [92/100] Step 200/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:21:31 AM | Train: [93/100] Step 100/390 Loss 0.100 Prec@(1,5) (96.8%, 100.0%)
01/19 04:21:32 AM | Train: [92/100] Step 300/390 Loss 0.099 Prec@(1,5) (96.9%, 100.0%)
01/19 04:21:33 AM | Train: [93/100] Step 390/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:33 AM | Train: [93/100] Final Prec@1 99.6960%
01/19 04:21:34 AM | Valid: [93/100] Step 000/078 Loss 0.113 Prec@(1,5) (95.3%, 100.0%)
01/19 04:21:35 AM | Train: [92/100] Step 300/390 Loss 0.013 Prec@(1,5) (99.8%, 100.0%)
01/19 04:21:36 AM | Valid: [93/100] Step 078/078 Loss 0.279 Prec@(1,5) (92.4%, 99.7%)
01/19 04:21:36 AM | Valid: [93/100] Final Prec@1 92.4300%
01/19 04:21:36 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:21:36 AM | Train: [94/100] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
01/19 04:21:37 AM | Train: [93/100] Step 200/390 Loss 0.101 Prec@(1,5) (96.8%, 100.0%)
01/19 04:21:38 AM | Train: [92/100] Step 390/390 Loss 0.099 Prec@(1,5) (96.9%, 100.0%)
01/19 04:21:38 AM | Train: [92/100] Final Prec@1 96.8560%
01/19 04:21:39 AM | Valid: [92/100] Step 000/078 Loss 0.275 Prec@(1,5) (92.2%, 100.0%)
01/19 04:21:40 AM | Train: [92/100] Step 390/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:40 AM | Train: [92/100] Final Prec@1 99.7320%
01/19 04:21:40 AM | Valid: [92/100] Step 078/078 Loss 0.351 Prec@(1,5) (89.4%, 99.6%)
01/19 04:21:40 AM | Valid: [92/100] Step 000/078 Loss 0.169 Prec@(1,5) (95.3%, 100.0%)
01/19 04:21:40 AM | Valid: [92/100] Final Prec@1 89.4400%
01/19 04:21:41 AM | Current best Prec@1 = 89.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:21:41 AM | Train: [93/100] Step 000/390 Loss 0.101 Prec@(1,5) (96.9%, 100.0%)
01/19 04:21:42 AM | Valid: [92/100] Step 078/078 Loss 0.283 Prec@(1,5) (92.6%, 99.6%)
01/19 04:21:42 AM | Train: [94/100] Step 100/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:42 AM | Valid: [92/100] Final Prec@1 92.5900%
01/19 04:21:42 AM | Current best Prec@1 = 92.5900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:21:43 AM | Train: [93/100] Step 300/390 Loss 0.101 Prec@(1,5) (96.7%, 100.0%)
01/19 04:21:43 AM | Train: [93/100] Step 000/390 Loss 0.012 Prec@(1,5) (100.0%, 100.0%)
01/19 04:21:47 AM | Train: [93/100] Step 100/390 Loss 0.094 Prec@(1,5) (97.0%, 100.0%)
01/19 04:21:48 AM | Train: [94/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:48 AM | Train: [93/100] Step 390/390 Loss 0.101 Prec@(1,5) (96.7%, 100.0%)
01/19 04:21:49 AM | Train: [93/100] Final Prec@1 96.7440%
01/19 04:21:49 AM | Train: [93/100] Step 100/390 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)
01/19 04:21:49 AM | Valid: [93/100] Step 000/078 Loss 0.239 Prec@(1,5) (92.2%, 100.0%)
01/19 04:21:51 AM | Valid: [93/100] Step 078/078 Loss 0.345 Prec@(1,5) (89.6%, 99.6%)
01/19 04:21:51 AM | Valid: [93/100] Final Prec@1 89.6200%
01/19 04:21:51 AM | Current best Prec@1 = 89.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:21:51 AM | Train: [94/100] Step 000/390 Loss 0.083 Prec@(1,5) (96.9%, 100.0%)
01/19 04:21:53 AM | Train: [93/100] Step 200/390 Loss 0.097 Prec@(1,5) (96.8%, 100.0%)
01/19 04:21:54 AM | Train: [94/100] Step 300/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:55 AM | Train: [93/100] Step 200/390 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)
01/19 04:21:57 AM | Train: [94/100] Step 100/390 Loss 0.095 Prec@(1,5) (96.9%, 100.0%)
01/19 04:21:59 AM | Train: [94/100] Step 390/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:21:59 AM | Train: [93/100] Step 300/390 Loss 0.097 Prec@(1,5) (96.9%, 100.0%)
01/19 04:21:59 AM | Train: [94/100] Final Prec@1 99.6980%
01/19 04:22:00 AM | Valid: [94/100] Step 000/078 Loss 0.115 Prec@(1,5) (94.5%, 100.0%)
01/19 04:22:01 AM | Train: [93/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:02 AM | Valid: [94/100] Step 078/078 Loss 0.280 Prec@(1,5) (92.4%, 99.7%)
01/19 04:22:02 AM | Valid: [94/100] Final Prec@1 92.4300%
01/19 04:22:02 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:22:03 AM | Train: [95/100] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
01/19 04:22:03 AM | Train: [94/100] Step 200/390 Loss 0.096 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:05 AM | Train: [93/100] Step 390/390 Loss 0.098 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:05 AM | Train: [93/100] Final Prec@1 96.8620%
01/19 04:22:05 AM | Valid: [93/100] Step 000/078 Loss 0.241 Prec@(1,5) (93.8%, 100.0%)
01/19 04:22:06 AM | Train: [93/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:22:06 AM | Train: [93/100] Final Prec@1 99.7580%
01/19 04:22:07 AM | Valid: [93/100] Step 000/078 Loss 0.159 Prec@(1,5) (96.1%, 100.0%)
01/19 04:22:07 AM | Valid: [93/100] Step 078/078 Loss 0.353 Prec@(1,5) (89.4%, 99.5%)
01/19 04:22:07 AM | Valid: [93/100] Final Prec@1 89.4100%
01/19 04:22:07 AM | Current best Prec@1 = 89.4400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:22:08 AM | Train: [94/100] Step 000/390 Loss 0.056 Prec@(1,5) (98.4%, 100.0%)
01/19 04:22:08 AM | Train: [95/100] Step 100/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:08 AM | Valid: [93/100] Step 078/078 Loss 0.283 Prec@(1,5) (92.5%, 99.6%)
01/19 04:22:09 AM | Valid: [93/100] Final Prec@1 92.4600%
01/19 04:22:09 AM | Current best Prec@1 = 92.5900%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:22:09 AM | Train: [94/100] Step 300/390 Loss 0.098 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:10 AM | Train: [94/100] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
01/19 04:22:14 AM | Train: [95/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:14 AM | Train: [94/100] Step 100/390 Loss 0.097 Prec@(1,5) (96.8%, 100.0%)
01/19 04:22:14 AM | Train: [94/100] Step 390/390 Loss 0.099 Prec@(1,5) (96.8%, 100.0%)
01/19 04:22:14 AM | Train: [94/100] Final Prec@1 96.8160%
01/19 04:22:15 AM | Valid: [94/100] Step 000/078 Loss 0.250 Prec@(1,5) (93.0%, 100.0%)
01/19 04:22:16 AM | Train: [94/100] Step 100/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:16 AM | Valid: [94/100] Step 078/078 Loss 0.346 Prec@(1,5) (89.6%, 99.6%)
01/19 04:22:16 AM | Valid: [94/100] Final Prec@1 89.6000%
01/19 04:22:17 AM | Current best Prec@1 = 89.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:22:17 AM | Train: [95/100] Step 000/390 Loss 0.071 Prec@(1,5) (97.7%, 100.0%)
01/19 04:22:20 AM | Train: [94/100] Step 200/390 Loss 0.098 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:20 AM | Train: [95/100] Step 300/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:22 AM | Train: [94/100] Step 200/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:23 AM | Train: [95/100] Step 100/390 Loss 0.097 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:25 AM | Train: [95/100] Step 390/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:26 AM | Train: [95/100] Final Prec@1 99.7380%
01/19 04:22:26 AM | Train: [94/100] Step 300/390 Loss 0.097 Prec@(1,5) (97.0%, 100.0%)
01/19 04:22:26 AM | Valid: [95/100] Step 000/078 Loss 0.112 Prec@(1,5) (95.3%, 100.0%)
01/19 04:22:28 AM | Train: [94/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:22:28 AM | Valid: [95/100] Step 078/078 Loss 0.281 Prec@(1,5) (92.5%, 99.7%)
01/19 04:22:28 AM | Valid: [95/100] Final Prec@1 92.4900%
01/19 04:22:28 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:22:29 AM | Train: [96/100] Step 000/390 Loss 0.025 Prec@(1,5) (99.2%, 100.0%)
01/19 04:22:30 AM | Train: [95/100] Step 200/390 Loss 0.096 Prec@(1,5) (97.0%, 100.0%)
01/19 04:22:31 AM | Train: [94/100] Step 390/390 Loss 0.098 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:32 AM | Train: [94/100] Final Prec@1 96.8800%
01/19 04:22:32 AM | Valid: [94/100] Step 000/078 Loss 0.248 Prec@(1,5) (93.0%, 100.0%)
01/19 04:22:33 AM | Train: [94/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:33 AM | Train: [94/100] Final Prec@1 99.7440%
01/19 04:22:34 AM | Valid: [94/100] Step 078/078 Loss 0.352 Prec@(1,5) (89.5%, 99.6%)
01/19 04:22:34 AM | Valid: [94/100] Step 000/078 Loss 0.169 Prec@(1,5) (96.1%, 100.0%)
01/19 04:22:34 AM | Valid: [94/100] Final Prec@1 89.4600%
01/19 04:22:34 AM | Current best Prec@1 = 89.4600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:22:35 AM | Train: [95/100] Step 000/390 Loss 0.077 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:35 AM | Train: [96/100] Step 100/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:35 AM | Train: [95/100] Step 300/390 Loss 0.096 Prec@(1,5) (97.0%, 100.0%)
01/19 04:22:36 AM | Valid: [94/100] Step 078/078 Loss 0.282 Prec@(1,5) (92.6%, 99.6%)
01/19 04:22:36 AM | Valid: [94/100] Final Prec@1 92.6200%
01/19 04:22:36 AM | Current best Prec@1 = 92.6200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:22:36 AM | Train: [95/100] Step 000/390 Loss 0.006 Prec@(1,5) (100.0%, 100.0%)
01/19 04:22:40 AM | Train: [95/100] Step 100/390 Loss 0.095 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:40 AM | Train: [96/100] Step 200/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:22:41 AM | Train: [95/100] Step 390/390 Loss 0.095 Prec@(1,5) (97.0%, 100.0%)
01/19 04:22:41 AM | Train: [95/100] Final Prec@1 96.9700%
01/19 04:22:41 AM | Valid: [95/100] Step 000/078 Loss 0.242 Prec@(1,5) (93.0%, 100.0%)
01/19 04:22:43 AM | Valid: [95/100] Step 078/078 Loss 0.342 Prec@(1,5) (89.6%, 99.5%)
01/19 04:22:43 AM | Train: [95/100] Step 100/390 Loss 0.013 Prec@(1,5) (99.8%, 100.0%)
01/19 04:22:43 AM | Valid: [95/100] Final Prec@1 89.6500%
01/19 04:22:43 AM | Current best Prec@1 = 89.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:22:44 AM | Train: [96/100] Step 000/390 Loss 0.155 Prec@(1,5) (93.8%, 100.0%)
01/19 04:22:46 AM | Train: [95/100] Step 200/390 Loss 0.092 Prec@(1,5) (97.1%, 100.0%)
01/19 04:22:46 AM | Train: [96/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:22:49 AM | Train: [95/100] Step 200/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:50 AM | Train: [96/100] Step 100/390 Loss 0.102 Prec@(1,5) (96.7%, 100.0%)
01/19 04:22:52 AM | Train: [96/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:22:52 AM | Train: [96/100] Final Prec@1 99.7640%
01/19 04:22:52 AM | Train: [95/100] Step 300/390 Loss 0.093 Prec@(1,5) (97.0%, 100.0%)
01/19 04:22:53 AM | Valid: [96/100] Step 000/078 Loss 0.104 Prec@(1,5) (96.1%, 100.0%)
01/19 04:22:54 AM | Valid: [96/100] Step 078/078 Loss 0.280 Prec@(1,5) (92.4%, 99.7%)
01/19 04:22:54 AM | Valid: [96/100] Final Prec@1 92.4400%
01/19 04:22:54 AM | Train: [95/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:22:55 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:22:55 AM | Train: [97/100] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
01/19 04:22:57 AM | Train: [96/100] Step 200/390 Loss 0.099 Prec@(1,5) (96.9%, 100.0%)
01/19 04:22:57 AM | Train: [95/100] Step 390/390 Loss 0.093 Prec@(1,5) (97.0%, 100.0%)
01/19 04:22:58 AM | Train: [95/100] Final Prec@1 97.0360%
01/19 04:22:58 AM | Valid: [95/100] Step 000/078 Loss 0.239 Prec@(1,5) (93.8%, 100.0%)
01/19 04:23:00 AM | Valid: [95/100] Step 078/078 Loss 0.349 Prec@(1,5) (89.7%, 99.5%)
01/19 04:23:00 AM | Valid: [95/100] Final Prec@1 89.6600%
01/19 04:23:00 AM | Current best Prec@1 = 89.6600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:23:00 AM | Train: [95/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:00 AM | Train: [95/100] Final Prec@1 99.7620%
01/19 04:23:00 AM | Train: [96/100] Step 000/390 Loss 0.166 Prec@(1,5) (93.8%, 100.0%)
01/19 04:23:00 AM | Valid: [95/100] Step 000/078 Loss 0.170 Prec@(1,5) (96.1%, 100.0%)
01/19 04:23:01 AM | Train: [97/100] Step 100/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:23:02 AM | Train: [96/100] Step 300/390 Loss 0.098 Prec@(1,5) (96.9%, 100.0%)
01/19 04:23:02 AM | Valid: [95/100] Step 078/078 Loss 0.284 Prec@(1,5) (92.5%, 99.6%)
01/19 04:23:03 AM | Valid: [95/100] Final Prec@1 92.5300%
01/19 04:23:03 AM | Current best Prec@1 = 92.6200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:23:03 AM | Train: [96/100] Step 000/390 Loss 0.028 Prec@(1,5) (99.2%, 100.0%)
01/19 04:23:07 AM | Train: [96/100] Step 100/390 Loss 0.094 Prec@(1,5) (96.9%, 100.0%)
01/19 04:23:07 AM | Train: [97/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:23:08 AM | Train: [96/100] Step 390/390 Loss 0.096 Prec@(1,5) (96.9%, 100.0%)
01/19 04:23:08 AM | Train: [96/100] Final Prec@1 96.9140%
01/19 04:23:09 AM | Valid: [96/100] Step 000/078 Loss 0.229 Prec@(1,5) (93.8%, 100.0%)
01/19 04:23:10 AM | Train: [96/100] Step 100/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:10 AM | Valid: [96/100] Step 078/078 Loss 0.343 Prec@(1,5) (89.8%, 99.6%)
01/19 04:23:10 AM | Valid: [96/100] Final Prec@1 89.7700%
01/19 04:23:11 AM | Current best Prec@1 = 89.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:23:11 AM | Train: [97/100] Step 000/390 Loss 0.132 Prec@(1,5) (97.7%, 100.0%)
01/19 04:23:12 AM | Train: [96/100] Step 200/390 Loss 0.093 Prec@(1,5) (97.1%, 100.0%)
01/19 04:23:13 AM | Train: [97/100] Step 300/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:23:16 AM | Train: [96/100] Step 200/390 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:17 AM | Train: [97/100] Step 100/390 Loss 0.096 Prec@(1,5) (97.1%, 100.0%)
01/19 04:23:18 AM | Train: [96/100] Step 300/390 Loss 0.094 Prec@(1,5) (97.0%, 100.0%)
01/19 04:23:19 AM | Train: [97/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:19 AM | Train: [97/100] Final Prec@1 99.7560%
01/19 04:23:19 AM | Valid: [97/100] Step 000/078 Loss 0.118 Prec@(1,5) (94.5%, 100.0%)
01/19 04:23:21 AM | Valid: [97/100] Step 078/078 Loss 0.283 Prec@(1,5) (92.4%, 99.6%)
01/19 04:23:21 AM | Valid: [97/100] Final Prec@1 92.3800%
01/19 04:23:21 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:23:22 AM | Train: [96/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:22 AM | Train: [98/100] Step 000/390 Loss 0.019 Prec@(1,5) (99.2%, 100.0%)
01/19 04:23:23 AM | Train: [97/100] Step 200/390 Loss 0.093 Prec@(1,5) (97.1%, 100.0%)
01/19 04:23:23 AM | Train: [96/100] Step 390/390 Loss 0.092 Prec@(1,5) (97.0%, 100.0%)
01/19 04:23:24 AM | Train: [96/100] Final Prec@1 97.0440%
01/19 04:23:24 AM | Valid: [96/100] Step 000/078 Loss 0.229 Prec@(1,5) (93.0%, 100.0%)
01/19 04:23:26 AM | Valid: [96/100] Step 078/078 Loss 0.351 Prec@(1,5) (89.4%, 99.5%)
01/19 04:23:26 AM | Valid: [96/100] Final Prec@1 89.3900%
01/19 04:23:26 AM | Current best Prec@1 = 89.6600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:23:27 AM | Train: [97/100] Step 000/390 Loss 0.136 Prec@(1,5) (94.5%, 100.0%)
01/19 04:23:27 AM | Train: [96/100] Step 390/390 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:27 AM | Train: [96/100] Final Prec@1 99.7920%
01/19 04:23:28 AM | Train: [98/100] Step 100/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:23:28 AM | Valid: [96/100] Step 000/078 Loss 0.173 Prec@(1,5) (95.3%, 100.0%)
01/19 04:23:29 AM | Train: [97/100] Step 300/390 Loss 0.092 Prec@(1,5) (97.2%, 100.0%)
01/19 04:23:30 AM | Valid: [96/100] Step 078/078 Loss 0.283 Prec@(1,5) (92.5%, 99.6%)
01/19 04:23:30 AM | Valid: [96/100] Final Prec@1 92.4900%
01/19 04:23:30 AM | Current best Prec@1 = 92.6200%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:23:31 AM | Train: [97/100] Step 000/390 Loss 0.005 Prec@(1,5) (100.0%, 100.0%)
01/19 04:23:33 AM | Train: [97/100] Step 100/390 Loss 0.092 Prec@(1,5) (97.1%, 100.0%)
01/19 04:23:33 AM | Train: [98/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:34 AM | Train: [97/100] Step 390/390 Loss 0.094 Prec@(1,5) (97.1%, 100.0%)
01/19 04:23:34 AM | Train: [97/100] Final Prec@1 97.0920%
01/19 04:23:35 AM | Valid: [97/100] Step 000/078 Loss 0.240 Prec@(1,5) (93.0%, 100.0%)
01/19 04:23:36 AM | Valid: [97/100] Step 078/078 Loss 0.342 Prec@(1,5) (89.8%, 99.5%)
01/19 04:23:36 AM | Valid: [97/100] Final Prec@1 89.8500%
01/19 04:23:36 AM | Current best Prec@1 = 89.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:23:37 AM | Train: [97/100] Step 100/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:23:37 AM | Train: [98/100] Step 000/390 Loss 0.059 Prec@(1,5) (99.2%, 100.0%)
01/19 04:23:39 AM | Train: [97/100] Step 200/390 Loss 0.090 Prec@(1,5) (97.2%, 100.0%)
01/19 04:23:39 AM | Train: [98/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:43 AM | Train: [97/100] Step 200/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:23:43 AM | Train: [98/100] Step 100/390 Loss 0.093 Prec@(1,5) (97.2%, 100.0%)
01/19 04:23:45 AM | Train: [97/100] Step 300/390 Loss 0.089 Prec@(1,5) (97.3%, 100.0%)
01/19 04:23:45 AM | Train: [98/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:23:45 AM | Train: [98/100] Final Prec@1 99.7560%
01/19 04:23:46 AM | Valid: [98/100] Step 000/078 Loss 0.102 Prec@(1,5) (94.5%, 100.0%)
01/19 04:23:47 AM | Valid: [98/100] Step 078/078 Loss 0.279 Prec@(1,5) (92.5%, 99.7%)
01/19 04:23:47 AM | Valid: [98/100] Final Prec@1 92.5100%
01/19 04:23:47 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:23:48 AM | Train: [97/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:23:48 AM | Train: [99/100] Step 000/390 Loss 0.045 Prec@(1,5) (98.4%, 100.0%)
01/19 04:23:49 AM | Train: [98/100] Step 200/390 Loss 0.092 Prec@(1,5) (97.3%, 100.0%)
01/19 04:23:50 AM | Train: [97/100] Step 390/390 Loss 0.091 Prec@(1,5) (97.2%, 100.0%)
01/19 04:23:50 AM | Train: [97/100] Final Prec@1 97.1860%
01/19 04:23:51 AM | Valid: [97/100] Step 000/078 Loss 0.237 Prec@(1,5) (93.8%, 100.0%)
01/19 04:23:52 AM | Valid: [97/100] Step 078/078 Loss 0.350 Prec@(1,5) (89.5%, 99.6%)
01/19 04:23:53 AM | Valid: [97/100] Final Prec@1 89.4600%
01/19 04:23:53 AM | Current best Prec@1 = 89.6600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:23:53 AM | Train: [98/100] Step 000/390 Loss 0.054 Prec@(1,5) (97.7%, 100.0%)
01/19 04:23:53 AM | Train: [97/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:23:54 AM | Train: [97/100] Final Prec@1 99.7320%
01/19 04:23:54 AM | Train: [99/100] Step 100/390 Loss 0.015 Prec@(1,5) (99.6%, 100.0%)
01/19 04:23:54 AM | Valid: [97/100] Step 000/078 Loss 0.174 Prec@(1,5) (95.3%, 100.0%)
01/19 04:23:55 AM | Train: [98/100] Step 300/390 Loss 0.092 Prec@(1,5) (97.2%, 100.0%)
01/19 04:23:56 AM | Valid: [97/100] Step 078/078 Loss 0.281 Prec@(1,5) (92.6%, 99.6%)
01/19 04:23:56 AM | Valid: [97/100] Final Prec@1 92.6300%
01/19 04:23:56 AM | Current best Prec@1 = 92.6300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:23:57 AM | Train: [98/100] Step 000/390 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)
01/19 04:24:00 AM | Train: [98/100] Step 100/390 Loss 0.088 Prec@(1,5) (97.3%, 100.0%)
01/19 04:24:00 AM | Train: [99/100] Step 200/390 Loss 0.015 Prec@(1,5) (99.7%, 100.0%)
01/19 04:24:00 AM | Train: [98/100] Step 390/390 Loss 0.093 Prec@(1,5) (97.1%, 100.0%)
01/19 04:24:00 AM | Train: [98/100] Final Prec@1 97.1340%
01/19 04:24:01 AM | Valid: [98/100] Step 000/078 Loss 0.236 Prec@(1,5) (93.8%, 100.0%)
01/19 04:24:02 AM | Valid: [98/100] Step 078/078 Loss 0.342 Prec@(1,5) (89.9%, 99.6%)
01/19 04:24:02 AM | Valid: [98/100] Final Prec@1 89.8800%
01/19 04:24:03 AM | Current best Prec@1 = 89.9000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:24:03 AM | Train: [98/100] Step 100/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:03 AM | Train: [99/100] Step 000/390 Loss 0.098 Prec@(1,5) (96.1%, 100.0%)
01/19 04:24:05 AM | Train: [98/100] Step 200/390 Loss 0.088 Prec@(1,5) (97.3%, 100.0%)
01/19 04:24:06 AM | Train: [99/100] Step 300/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:24:09 AM | Train: [98/100] Step 200/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:09 AM | Train: [99/100] Step 100/390 Loss 0.094 Prec@(1,5) (97.1%, 100.0%)
01/19 04:24:11 AM | Train: [99/100] Step 390/390 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)
01/19 04:24:11 AM | Train: [99/100] Final Prec@1 99.7000%
01/19 04:24:11 AM | Train: [98/100] Step 300/390 Loss 0.087 Prec@(1,5) (97.3%, 100.0%)
01/19 04:24:12 AM | Valid: [99/100] Step 000/078 Loss 0.102 Prec@(1,5) (95.3%, 100.0%)
01/19 04:24:13 AM | Valid: [99/100] Step 078/078 Loss 0.278 Prec@(1,5) (92.6%, 99.7%)
01/19 04:24:13 AM | Valid: [99/100] Final Prec@1 92.5600%
01/19 04:24:14 AM | Current best Prec@1 = 92.6000%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44073.0, 0.6725006103515625]
['model.relu.alpha_aux_1_0', 16384, 7872.0, 0.48046875]
['model.relu.alpha_aux_2_0', 16384, 7839.0, 0.47845458984375]
['model.relu.alpha_aux_3_0', 16384, 8053.0, 0.49151611328125]
['model.relu.alpha_aux_4_0', 16384, 7417.0, 0.45269775390625]
['model.relu.alpha_aux_5_0', 8192, 3268.0, 0.39892578125]
['model.relu.alpha_aux_6_0', 8192, 3170.0, 0.386962890625]
['model.relu.alpha_aux_7_0', 8192, 3718.0, 0.453857421875]
['model.relu.alpha_aux_8_0', 8192, 3570.0, 0.435791015625]
['model.relu.alpha_aux_9_0', 4096, 1680.0, 0.41015625]
['model.relu.alpha_aux_10_0', 4096, 1607.0, 0.392333984375]
['model.relu.alpha_aux_11_0', 4096, 2086.0, 0.50927734375]
['model.relu.alpha_aux_12_0', 4096, 1613.0, 0.393798828125]
['model.relu.alpha_aux_13_0', 2048, 814.0, 0.3974609375]
['model.relu.alpha_aux_14_0', 2048, 1046.0, 0.5107421875]
['model.relu.alpha_aux_15_0', 2048, 1122.0, 0.5478515625]
['model.relu.alpha_aux_16_0', 2048, 965.0, 0.47119140625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99913.0, 0.5302787449048914]
########## End ###########
01/19 04:24:14 AM | Train: [100/100] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
01/19 04:24:14 AM | Train: [98/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:15 AM | Train: [99/100] Step 200/390 Loss 0.094 Prec@(1,5) (97.0%, 100.0%)
01/19 04:24:17 AM | Train: [98/100] Step 390/390 Loss 0.089 Prec@(1,5) (97.3%, 100.0%)
01/19 04:24:17 AM | Train: [98/100] Final Prec@1 97.2940%
01/19 04:24:18 AM | Valid: [98/100] Step 000/078 Loss 0.243 Prec@(1,5) (93.8%, 100.0%)
01/19 04:24:19 AM | Valid: [98/100] Step 078/078 Loss 0.350 Prec@(1,5) (89.6%, 99.5%)
01/19 04:24:19 AM | Valid: [98/100] Final Prec@1 89.5900%
01/19 04:24:19 AM | Current best Prec@1 = 89.6600%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:24:20 AM | Train: [98/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:20 AM | Train: [98/100] Final Prec@1 99.7620%
01/19 04:24:20 AM | Train: [99/100] Step 000/390 Loss 0.119 Prec@(1,5) (95.3%, 100.0%)
01/19 04:24:20 AM | Train: [100/100] Step 100/390 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:20 AM | Valid: [98/100] Step 000/078 Loss 0.172 Prec@(1,5) (95.3%, 100.0%)
01/19 04:24:21 AM | Train: [99/100] Step 300/390 Loss 0.096 Prec@(1,5) (97.0%, 100.0%)
01/19 04:24:22 AM | Valid: [98/100] Step 078/078 Loss 0.283 Prec@(1,5) (92.6%, 99.6%)
01/19 04:24:22 AM | Valid: [98/100] Final Prec@1 92.5800%
01/19 04:24:22 AM | Current best Prec@1 = 92.6300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:24:23 AM | Train: [99/100] Step 000/390 Loss 0.066 Prec@(1,5) (98.4%, 100.0%)
01/19 04:24:26 AM | Train: [100/100] Step 200/390 Loss 0.013 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:26 AM | Train: [99/100] Step 100/390 Loss 0.093 Prec@(1,5) (97.1%, 100.0%)
01/19 04:24:27 AM | Train: [99/100] Step 390/390 Loss 0.094 Prec@(1,5) (97.0%, 100.0%)
01/19 04:24:27 AM | Train: [99/100] Final Prec@1 97.0160%
01/19 04:24:27 AM | Valid: [99/100] Step 000/078 Loss 0.236 Prec@(1,5) (93.8%, 100.0%)
01/19 04:24:29 AM | Train: [99/100] Step 100/390 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)
01/19 04:24:29 AM | Valid: [99/100] Step 078/078 Loss 0.340 Prec@(1,5) (89.9%, 99.6%)
01/19 04:24:29 AM | Valid: [99/100] Final Prec@1 89.9400%
01/19 04:24:29 AM | Current best Prec@1 = 89.9400%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49215.0, 0.7509613037109375]
['model.relu.alpha_aux_1_0', 16384, 12282.0, 0.7496337890625]
['model.relu.alpha_aux_2_0', 16384, 11245.0, 0.68634033203125]
['model.relu.alpha_aux_3_0', 16384, 12468.0, 0.760986328125]
['model.relu.alpha_aux_4_0', 16384, 11605.0, 0.70831298828125]
['model.relu.alpha_aux_5_0', 8192, 6120.0, 0.7470703125]
['model.relu.alpha_aux_6_0', 8192, 5278.0, 0.644287109375]
['model.relu.alpha_aux_7_0', 8192, 5735.0, 0.7000732421875]
['model.relu.alpha_aux_8_0', 8192, 5654.0, 0.690185546875]
['model.relu.alpha_aux_9_0', 4096, 2451.0, 0.598388671875]
['model.relu.alpha_aux_10_0', 4096, 2078.0, 0.50732421875]
['model.relu.alpha_aux_11_0', 4096, 3338.0, 0.81494140625]
['model.relu.alpha_aux_12_0', 4096, 1949.0, 0.475830078125]
['model.relu.alpha_aux_13_0', 2048, 957.0, 0.46728515625]
['model.relu.alpha_aux_14_0', 2048, 1723.0, 0.84130859375]
['model.relu.alpha_aux_15_0', 2048, 2026.0, 0.9892578125]
['model.relu.alpha_aux_16_0', 2048, 1668.0, 0.814453125]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 135792.0, 0.720703125]
########## End ###########
01/19 04:24:30 AM | Train: [100/100] Step 000/390 Loss 0.036 Prec@(1,5) (100.0%, 100.0%)
01/19 04:24:32 AM | Train: [99/100] Step 200/390 Loss 0.091 Prec@(1,5) (97.2%, 100.0%)
01/19 04:24:32 AM | Train: [100/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:35 AM | Train: [99/100] Step 200/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:36 AM | Train: [100/100] Step 100/390 Loss 0.093 Prec@(1,5) (97.1%, 100.0%)
01/19 04:24:37 AM | Train: [100/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:38 AM | Train: [100/100] Final Prec@1 99.7840%
01/19 04:24:38 AM | Train: [99/100] Step 300/390 Loss 0.093 Prec@(1,5) (97.2%, 100.0%)
01/19 04:24:38 AM | Valid: [100/100] Step 000/078 Loss 0.116 Prec@(1,5) (95.3%, 100.0%)
01/19 04:24:40 AM | Valid: [100/100] Step 078/078 Loss 0.279 Prec@(1,5) (92.5%, 99.7%)
01/19 04:24:40 AM | Valid: [100/100] Final Prec@1 92.5200%
01/19 04:24:40 AM | Current best Prec@1 = 92.6000%
01/19 04:24:40 AM | Final best validation Prec@1 = 92.6000%
01/19 04:24:40 AM | Train: [99/100] Step 300/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:42 AM | Train: [100/100] Step 200/390 Loss 0.095 Prec@(1,5) (97.0%, 100.0%)
01/19 04:24:43 AM | Train: [99/100] Step 390/390 Loss 0.092 Prec@(1,5) (97.2%, 100.0%)
01/19 04:24:43 AM | Train: [99/100] Final Prec@1 97.1920%
01/19 04:24:44 AM | Valid: [99/100] Step 000/078 Loss 0.240 Prec@(1,5) (93.8%, 100.0%)
01/19 04:24:44 AM | Train: [99/100] Step 390/390 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:44 AM | Train: [99/100] Final Prec@1 99.7660%
01/19 04:24:45 AM | Valid: [99/100] Step 000/078 Loss 0.166 Prec@(1,5) (95.3%, 100.0%)
01/19 04:24:45 AM | Valid: [99/100] Step 078/078 Loss 0.355 Prec@(1,5) (89.7%, 99.6%)
01/19 04:24:45 AM | Valid: [99/100] Final Prec@1 89.7100%
01/19 04:24:46 AM | Current best Prec@1 = 89.7100%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 49627.0, 0.7572479248046875]
['model.relu.alpha_aux_1_0', 16384, 12370.0, 0.7550048828125]
['model.relu.alpha_aux_2_0', 16384, 11609.0, 0.70855712890625]
['model.relu.alpha_aux_3_0', 16384, 12528.0, 0.7646484375]
['model.relu.alpha_aux_4_0', 16384, 11880.0, 0.72509765625]
['model.relu.alpha_aux_5_0', 8192, 6249.0, 0.7628173828125]
['model.relu.alpha_aux_6_0', 8192, 5397.0, 0.6588134765625]
['model.relu.alpha_aux_7_0', 8192, 5973.0, 0.7291259765625]
['model.relu.alpha_aux_8_0', 8192, 5820.0, 0.71044921875]
['model.relu.alpha_aux_9_0', 4096, 2553.0, 0.623291015625]
['model.relu.alpha_aux_10_0', 4096, 2179.0, 0.531982421875]
['model.relu.alpha_aux_11_0', 4096, 3342.0, 0.81591796875]
['model.relu.alpha_aux_12_0', 4096, 1999.0, 0.488037109375]
['model.relu.alpha_aux_13_0', 2048, 964.0, 0.470703125]
['model.relu.alpha_aux_14_0', 2048, 1742.0, 0.8505859375]
['model.relu.alpha_aux_15_0', 2048, 2029.0, 0.99072265625]
['model.relu.alpha_aux_16_0', 2048, 1684.0, 0.822265625]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 137945.0, 0.7321299677309783]
########## End ###########
01/19 04:24:46 AM | Valid: [99/100] Step 078/078 Loss 0.280 Prec@(1,5) (92.6%, 99.6%)
01/19 04:24:46 AM | Valid: [99/100] Final Prec@1 92.5900%
01/19 04:24:46 AM | Current best Prec@1 = 92.6300%
####### ReLU Sparsity #######
# Layer wise neuron ReLU sparsity for the model
# Format: [layer name, Total original ReLU count, Pruned count, Pruned percentage]
['model.relu.alpha_aux_0_0', 65536, 44021.0, 0.6717071533203125]
['model.relu.alpha_aux_1_0', 16384, 7729.0, 0.47174072265625]
['model.relu.alpha_aux_2_0', 16384, 7924.0, 0.483642578125]
['model.relu.alpha_aux_3_0', 16384, 7975.0, 0.48675537109375]
['model.relu.alpha_aux_4_0', 16384, 7528.0, 0.45947265625]
['model.relu.alpha_aux_5_0', 8192, 3218.0, 0.392822265625]
['model.relu.alpha_aux_6_0', 8192, 3175.0, 0.3875732421875]
['model.relu.alpha_aux_7_0', 8192, 3665.0, 0.4473876953125]
['model.relu.alpha_aux_8_0', 8192, 3409.0, 0.4161376953125]
['model.relu.alpha_aux_9_0', 4096, 1625.0, 0.396728515625]
['model.relu.alpha_aux_10_0', 4096, 1636.0, 0.3994140625]
['model.relu.alpha_aux_11_0', 4096, 2093.0, 0.510986328125]
['model.relu.alpha_aux_12_0', 4096, 1673.0, 0.408447265625]
['model.relu.alpha_aux_13_0', 2048, 877.0, 0.42822265625]
['model.relu.alpha_aux_14_0', 2048, 1047.0, 0.51123046875]
['model.relu.alpha_aux_15_0', 2048, 1169.0, 0.57080078125]
['model.relu.alpha_aux_16_0', 2048, 967.0, 0.47216796875]
# Global ReLU neuron sparsity for the model
# Format: [Total original ReLU count, Pruned count, Pruned percentage]
[188416, 99731.0, 0.529312797214674]
########## End ###########
01/19 04:24:46 AM | Train: [100/100] Step 000/390 Loss 0.037 Prec@(1,5) (99.2%, 100.0%)
01/19 04:24:47 AM | Train: [100/100] Step 000/390 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)
01/19 04:24:47 AM | Train: [100/100] Step 300/390 Loss 0.093 Prec@(1,5) (97.1%, 100.0%)
01/19 04:24:51 AM | Train: [100/100] Step 100/390 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)
01/19 04:24:53 AM | Train: [100/100] Step 100/390 Loss 0.089 Prec@(1,5) (97.1%, 100.0%)
01/19 04:24:53 AM | Train: [100/100] Step 390/390 Loss 0.093 Prec@(1,5) (97.1%, 100.0%)
01/19 04:24:53 AM | Train: [100/100] Final Prec@1 97.1140%
01/19 04:24:54 AM | Valid: [100/100] Step 000/078 Loss 0.239 Prec@(1,5) (93.8%, 100.0%)
01/19 04:24:55 AM | Valid: [100/100] Step 078/078 Loss 0.339 Prec@(1,5) (90.0%, 99.5%)
01/19 04:24:55 AM | Valid: [100/100] Final Prec@1 89.9600%
01/19 04:24:55 AM | Current best Prec@1 = 89.9600%
01/19 04:24:55 AM | Final best validation Prec@1 = 89.9600%
01/19 04:24:56 AM | Train: [100/100] Step 200/390 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)
01/19 04:24:58 AM | Train: [100/100] Step 200/390 Loss 0.090 Prec@(1,5) (97.2%, 100.0%)
01/19 04:25:00 AM | Train: [100/100] Step 300/390 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)
01/19 04:25:02 AM | Train: [100/100] Step 300/390 Loss 0.090 Prec@(1,5) (97.1%, 100.0%)
01/19 04:25:04 AM | Train: [100/100] Step 390/390 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)
01/19 04:25:05 AM | Train: [100/100] Final Prec@1 99.7660%
01/19 04:25:05 AM | Valid: [100/100] Step 000/078 Loss 0.172 Prec@(1,5) (94.5%, 100.0%)
01/19 04:25:06 AM | Valid: [100/100] Step 078/078 Loss 0.279 Prec@(1,5) (92.6%, 99.6%)
01/19 04:25:06 AM | Valid: [100/100] Final Prec@1 92.6100%
01/19 04:25:06 AM | Train: [100/100] Step 390/390 Loss 0.091 Prec@(1,5) (97.1%, 100.0%)
01/19 04:25:07 AM | Train: [100/100] Final Prec@1 97.1200%
01/19 04:25:07 AM | Current best Prec@1 = 92.6300%
01/19 04:25:07 AM | Final best validation Prec@1 = 92.6300%
01/19 04:25:07 AM | Valid: [100/100] Step 000/078 Loss 0.241 Prec@(1,5) (93.0%, 100.0%)
01/19 04:25:08 AM | Valid: [100/100] Step 078/078 Loss 0.349 Prec@(1,5) (89.6%, 99.6%)
01/19 04:25:08 AM | Valid: [100/100] Final Prec@1 89.5600%
01/19 04:25:09 AM | Current best Prec@1 = 89.7100%
01/19 04:25:09 AM | Final best validation Prec@1 = 89.7100%
