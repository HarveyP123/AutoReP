import numpy as np
import torch
import math
import torchvision
from torch import nn
from torch.nn import functional as F
from torchvision import datasets, transforms
import warnings
from functools import partial


# define customized functions with customized gradients
class STEFunction(torch.autograd.Function):
    """ define straight through estimator with overrided gradient (gate) """
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return (input > 0).float()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        return torch.mul(F.softplus(input), grad_output)

# Build model and load/initialize weights
def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    """ 
        function to help weight initialization
        Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    """
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

### ReLU with run time initialization method
# mask1: bitmap, 1 means has ReLU, 0 means direct pass.
# mask2: bitmap, 1 means direct pass, 0 means have ReLU
# a*mask2: passed element
# a*mask1: element need to be ReLU
# ReLU(a*mask1) + a*mask2
class ReLU_masked(nn.Module):
    def __init__(self, Num_mask = 1, dropRate=0):
        super().__init__()
        self.Num_mask = Num_mask
        self.num_feature = 0
        self.current_feature = 0
        self.sel_mask = 0
        self.init = 1
        self.act = partial(F.relu, inplace = True)
        self.dropout = nn.Dropout2d(p=dropRate, inplace=True)
    def init_w_aux(self, size):
        for i in range(self.Num_mask):
            setattr(self, "alpha_aux_{}_{}".format(self.num_feature, i), nn.Parameter(torch.Tensor(*size)))
            nn.init.uniform_(getattr(self, "alpha_aux_{}_{}".format(self.num_feature, i)), a = 0, b = 1) # weight init for aux parameter, can be truncated normal
    
    def forward(self, x):
        ### Initialize the parameter at the beginning
        if self.init:
            x_size = list(x.size())[1:] ### Ignore batch size dimension
            self.init_w_aux(x_size)
            neuron_relu_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(self.num_feature, self.sel_mask))) ### Mask for element which applies ReLU
            self.num_feature += 1
        ### Conduct recurrently inference during normal inference and training
        else:
            # print("Current used: ", getattr(self, "alpha_aux_{}_{}".format(self.current_feature, self.sel_mask)))
            neuron_relu_mask = STEFunction.apply(getattr(self, "alpha_aux_{}_{}".format(self.current_feature, self.sel_mask))) ### Mask for element which applies ReLU
            self.current_feature = (self.current_feature + 1) % self.num_feature
        neuron_pass_mask = 1 - neuron_relu_mask  ### Mask for element which ignore ReLU
        out = self.act(torch.mul(x, neuron_relu_mask)) + torch.mul(x, neuron_pass_mask)
        out = self.dropout(out)
        return out