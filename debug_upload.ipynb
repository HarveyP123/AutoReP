{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# define customized functions with customized gradients\n",
    "class STEFunction_relay(torch.autograd.Function):\n",
    "    \"\"\" define straight through estimator with overrided gradient (gate) \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_x, mask, threshold = 0.005):\n",
    "        ctx.save_for_backward(input_x)\n",
    "        mask_higher = (input_x > threshold).float()\n",
    "        mask_lower = (input_x < (-1) * threshold).float()\n",
    "        mask > 0\n",
    "        return (input_x > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_x, = ctx.saved_tensors\n",
    "        return torch.mul(F.softplus(input_x), grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_state = 1\n",
    "in_val = -0.015\n",
    "threshold = 0.01\n",
    "def Hysteresis(now_state, in_val, threshold):\n",
    "    if now_state == 1:\n",
    "        if in_val < (-1) * threshold:\n",
    "            now_state = 0\n",
    "    else:\n",
    "        if in_val > threshold:\n",
    "            now_state = 1\n",
    "    return now_state\n",
    "now_state = Hysteresis(now_state, in_val, threshold) \n",
    "print(now_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_state = 1.0\n",
    "in_val_list = [-0.011, 0.005, 0.001, 0.011, -0.009, -0.011, 0.0, 0.011]\n",
    "threshold = 0.01\n",
    "def Hysteresis(now_state, in_val, threshold):\n",
    "    now_state = now_state * float(in_val > (-1) * threshold) + (1 - now_state) * float(in_val > threshold)\n",
    "    return now_state\n",
    "for in_val in in_val_list:\n",
    "    now_state = Hysteresis(now_state, in_val, threshold) \n",
    "    print(now_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_state = 0\n",
    "in_val = -0.0101\n",
    "threshold = 0.01\n",
    "now_state = now_state * float(in_val > (-1) * threshold) + (1 - now_state) * float(in_val > threshold)\n",
    "print(now_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "class MultiInputRelayFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input1, input2):\n",
    "        ctx.save_for_backward(input1, input2)\n",
    "        return input1, input2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output1, grad_output2):\n",
    "        input1, input2 = ctx.saved_tensors\n",
    "        if not input1.requires_grad:\n",
    "            grad_input1 = None\n",
    "        else:\n",
    "            grad_input1 = grad_output1.clone()\n",
    "\n",
    "        if not input2.requires_grad:\n",
    "            grad_input2 = None\n",
    "        else:\n",
    "            grad_input2 = grad_output2.clone()\n",
    "\n",
    "        return grad_input1, grad_input2\n",
    "\n",
    "x1 = torch.randn(10, requires_grad=True)\n",
    "x2 = torch.randn(10)\n",
    "print(x1)\n",
    "# Use the MultiInputRelayFunction to calculate the output\n",
    "output = MultiInputRelayFunction.apply(x1, x2)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = (output[0] ** 2).sum()\n",
    "\n",
    "# Backprop to calculate gradients\n",
    "loss.backward()\n",
    "\n",
    "# Update x1 using an optimizer\n",
    "optimizer = torch.optim.Adam([x1], lr=0.1)\n",
    "optimizer.step()\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# define customized functions with customized gradients\n",
    "class STEFunction_relay(torch.autograd.Function):\n",
    "    \"\"\" define straight through estimator with overrided gradient (gate) \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mask_old):\n",
    "        ctx.save_for_backward(input)\n",
    "        return mask_old\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return torch.mul(F.softplus(input), grad_output), None\n",
    "\n",
    "x1 = torch.randn(10, requires_grad=True)\n",
    "nn.init.uniform_(x1, a = 0, b = 1)\n",
    "mask_old = (x1 > 0).float()\n",
    "print(x1)\n",
    "print(mask_old)\n",
    "mask_old.requires_grad = False\n",
    "x1_out = STEFunction_relay.apply(x1, mask_old)\n",
    "loss = torch.sum(x1_out)\n",
    "threshold = 0.5\n",
    "\n",
    "loss.backward()\n",
    "optimizer = torch.optim.Adam([x1], lr=1)\n",
    "optimizer.step()\n",
    "print(x1)\n",
    "print(mask_old)\n",
    "mask_new = mask_old * (x1 > (-1) * threshold).float() + (1 - mask_old) * (x1 > threshold).float()\n",
    "print(mask_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with random values\n",
    "tensor = torch.rand(3, 3)\n",
    "\n",
    "# Define a model with a single linear layer\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(3, 3)\n",
    "\n",
    "# Initialize the model\n",
    "model = Model()\n",
    "\n",
    "# Copy the values from the tensor into the model's parameters\n",
    "print(model.linear.weight)\n",
    "print(tensor)\n",
    "model.linear.weight.data.copy_(tensor)\n",
    "print(model.linear.weight)\n",
    "\n",
    "### Other method to copy parameter from one to another\n",
    "param1 = torch.nn.Parameter(torch.rand(3, 3))\n",
    "param2 = torch.nn.Parameter(torch.rand(3, 3))\n",
    "param2.data.copy_(param1.data)\n",
    "print(param2.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self._attr1 = Attr1()\n",
    "    def set_attr1(self, value):\n",
    "        self._attr1.attr2 = value\n",
    "\n",
    "class Attr1:\n",
    "    def __init__(self):\n",
    "        self.attr2 = None\n",
    "\n",
    "obj = MyClass()\n",
    "obj.set_attr1(\"Value\")\n",
    "print(obj._attr1.attr2)\n",
    "# setattr(eval('obj._attr1'), 'attr2', 'a')\n",
    "print(getattr(obj, '_attr1').attr2)\n",
    "getattr(obj, '_attr1').attr2 = 'a'\n",
    "print(obj._attr1.attr2)\n",
    "obj._attr1.attr2 = 'a'\n",
    "print(obj._attr1.attr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w1 = nn.Parameter(torch.randn(1))\n",
    "        self.w2 = nn.Parameter(torch.randn(1))\n",
    "        self.b = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = nn.functional.relu(x)\n",
    "        # gx = self.w2 * (x ** 2) + self.w1 * x + self.b\n",
    "        gx = self.w1 * x + self.b\n",
    "        return fx, gx\n",
    "\n",
    "# Define your loss function\n",
    "def loss_fn(fx, gx):\n",
    "    return (fx - gx).pow(2).mean()\n",
    "\n",
    "net = Net()\n",
    "# Define your optimizer\n",
    "optimizer = optim.Adam([net.w1, net.w2, net.b], lr=0.001)\n",
    "epochs = 10000\n",
    "lr_scheduler_w = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max = epochs)\n",
    "# Train the model\n",
    "for i in range(10000):\n",
    "    # Generate a new random x with normal distribution\n",
    "    x = torch.randn(32, 32) #N(0, 1)\n",
    "    # x += torch.randn(32, 32) #N(0, 2)\n",
    "    fx, gx = net(x)\n",
    "    loss = loss_fn(fx, gx)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler_w.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.w2, net.w1, net.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(20)\n",
    "x = torch.tensor([3.0])\n",
    "# x += torch.randn(20)\n",
    "fx, gx = net(x)\n",
    "print(x)\n",
    "print(fx)\n",
    "print(gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.0])\n",
    "x.var_map = 1\n",
    "print(x.var_map)\n",
    "fx, gx = net(x)\n",
    "print(fx.var_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "x = torch.randn(3, 3)\n",
    "print(x)\n",
    "mask = (torch.randn(3, 3) > 0).float()\n",
    "print(mask)\n",
    "out1 = F.relu(x * mask)\n",
    "out2 =  mask* F.relu(x)\n",
    "print(out1)\n",
    "print(out2)\n",
    "print(out1 - out2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "with torch.no_grad():\n",
    "    x0 = torch.randn(4, 4)\n",
    "    x1 = torch.clone(x0)\n",
    "    mask0 = (torch.randn(4, 4) > 0).float()\n",
    "    mask1 = torch.clone(mask0)\n",
    "x0.requires_grad = True\n",
    "x1.requires_grad = True\n",
    "mask0.requires_grad = True\n",
    "mask1.requires_grad = True\n",
    "optimizer0 = optim.Adam([x0, mask0], lr=1)\n",
    "optimizer1 = optim.Adam([x1, mask1], lr=1)\n",
    "for i in range(1):\n",
    "    out0 = F.relu(x0 * mask0)\n",
    "    out1 =  mask1* F.relu(x1)\n",
    "    loss0 = torch.sum(out0)\n",
    "    loss1 = torch.sum(out1)\n",
    "    print(\"difference between out0 and out1:\", torch.sum(out0 - out1))\n",
    "    optimizer0.zero_grad()\n",
    "    optimizer1.zero_grad()\n",
    "    loss0.backward()\n",
    "    loss1.backward()\n",
    "    print(mask0.grad)\n",
    "    print(mask1.grad)\n",
    "    optimizer0.step()\n",
    "    optimizer1.step()\n",
    "    # print(x0)\n",
    "    # print(x1)\n",
    "    # print(mask0)\n",
    "    # print(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x.size()\n",
    "x.expand(4, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09ed53c31c4084b0592fb3928828ca5258593b9e37489b03328497df4e897e81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
